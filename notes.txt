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
git clone https://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@github.com/tejaswini29598/java_dsa
Minimum one branch is created when we create a repository
steps and commands:
place a document file in the java_dsa folder
here i placed Notes.txt
1. ask the git to list down all the files that nedd to be pushed(uploaded)

$git add path
$git add C:/learning/java_dsa
$git add .

2. push (upload)
$ git push origin main
$ git push (Always pushes to main branch)

3. Ask the git to create a secured object in which all the files to pushed are copied
$git commit -m "java dsa notes added"

To check the status of the repository
$ git status

now open the java_dsa in vs code

To create project in eclipse:
file -> new -> project -> java Project
in the dialogue box (new window)
de-select the module.java check box
enter the project name(practice) and click finish

now in explorer, expand the project folder
now right click  src folder -> create -> new -> package
give the package name "day1"

right click  "day1" in explorer (under src folder), create -> class
class name "HelloWorld" 

Primitives types in java
    Numeric:
	  number only
		byte
		short
		int 
		long
	  number with precision/ accuracy
	  float
	  double
	Char
    Boolean	
ARITHMETIC OPERATORS:
+ - * / %

// 11/06/25 //

All the operators take 2 operands/inputs
Hence, All are binary operators
The expression is written using "INFIX" notation
55 + 5
inputs are numbers
output is also a number
5+8/9 The division must be evaluated first because of the BODMAS rule
All arithmetic operators hava left to right associtivity
The "INFIX" expression is what we uses but it will be first converted to "POSTFIX" expression and only then it will get evaluated

int num=45;
00000000 00000000 00000000 00101101 //It is decimal to binary conversion how the 45 number is stored in memory

int num= -45;
00101101 // it is the binary value of the 45
11010010 // 1's compliment
11010011 // 2's compliment
now -1 * 2(7) + 1* 2(6) + 1 * 2(4) + 3
-128+64+16+3
-64+19
-45

float num = 5.5; // it is considered as double
therefore float num =5.5f; // considered as float

C++ inherited C

sum = 5 + 5.5;
when the expression has different types of values , then the lower type value will be changed to higher type
this is because the data must not be lost
-----------------------------------------------------------------------

Relational Operators:
> < >= <= != == 
input has numbers
output is Boolean
All are binary operators
used with infix notation
has lesser precedence than arithmetic operators
has higher precedence than logical operators

for i from 1 to n do:
for(int i=1;i<=n;i++)
  or for(int i=1;i<n+1;i++)

(10,40) //closed interval from 10 to 40
(20,30) //open interval from 21 to 29
(35,65) //right open interval from 35 to 64

//assembly language is stored in .esm

i=10;
j=5;
i++; j--;
a[i] =b[j];
b[j]--;

i=10;
j=5;
a[++i] =b[--j]--;
---------------------------------------------------------

logical operators
& &&  | || !
input has boolean 
output is Boolean
All are binary operators except !
used with infix notation
has lesser precedence than arithmetic operators and relational operators
has higher precedence than assignment operators

All the programming files are stored in secondary memory(disk)
files are loaded using "MACROS"(preprocessive directives (#include))
.(dot operator) //left to right associtivity
object.field 
--------------------------------------------------------
java program structure  SUN Microsystems (Standford University Network Microsystems)
oops: abstraction,polymorphism,inheritance,encapsulation
Disadvantages of C++:-
1.main() must be global function
2.global functions were allowed, thus the solutions(apps,software) created with neither procedural or object oriented
3.global variables are still allowed in C++
4.files are loaded using macros and stored till end of the program
5.pointers are always a mess to many programmers
6.The onus(responsibility) of creating and deleting objects in the heap is on programmer
7.the inheritance is by default private which makes the accessing the inherited properties is very difficult. In java it is public inheritance
8.Multiple inheritance creates a solution which is low cohensive
9.The operator overloading concept can be used to just Implementanything which is necessary
10.The compiler created object file which is environment(OS) restircted/dependent/specific.
11.Handling run time errors is difficult
12.objects can be created in stach area as well

In java:(The Disadvantages of C++ have the solution in Java)
1.No global functions, All are only methods (methods vs functions)
Strict object oriented solution.Therefore no concept of global variables
2.signed and unsigned removed thus the primitive datatypes are simplified.
3.libraries can be loaded and used dynamically(no pre process)
4.pointers are abstracted and users are given references.
5.All objects are created in heap only
6.array is an object in java
7.creating object is the only job the programmer does. object deletion is automated with the concept of GARBAGE COLLECTION.
8.the inheritance is by default public thus doesnot make implementation very easy
9.no multiple inheritance
10.all classes are always high cohensive
11.the only operator can be overloaded is + and only for string concatenation
12.The complier creates a byte code which is environment(OS) independent
13.handling runtime errors is easy via exception handling
14.friend concept is removed thus no more ambiguity and complexity in classes
-----------------------------------------------------------------------------------
Different stages of a program
C++ is fast than java during runtime and python is the slowest among all
The code area has the bytecode i.e .class files
static/class vars are stored in GDS
local vars and function parameters ,return address and method address(linkage) are stored in stack area
all objects(instance variables) are stored in heap(dynamically allocated memory)
The creation of objects are done only in runtime and the memory allocation is done also in runtime(dynamic runtime or memory allocation) and in compile time it thinks can i allocate the memory
----------------------------------------------------------------------------------
git add .
git status
git commit -m "assignments file added and notes updated"
git push
------------------------------------------------------------------------------------
1z0-829/1z0-830 java certificaitons 15000/-
study official documentations
NPTEL certificaitons
c-DAC govt of india
---------------------------------------------------------------------------
No function overloading in python
cases of writinig the code(pascal,snake,camel)

//12/6/25 LG Placement exam//

//13/6/25//
void ,static methods

When an object is created during the runtime , first the control goes to declarative statement, secondly it goes to instance initializer block(un-named block). If it is present , then it goes into the respective constructor.
All local variables in java are not initialized until it is initialized by the programmer. Thus the local variables will be empty if they are not initialized. the first assignment to the local variable is its initialization.
Unlike the local variables, the instance variables and the static(class) variables have default values.
byte,short,int and long instance variables hava 0 as the default values
float - 0.0f , double - 0.0
boolean instance or static variable - false
and if the instance or static variable is a reference type, then its default value will be null (ex:String)

The super() in the constructor will make call to immediate parent class constructor and this call happens right are PROLOG of the child class constructor.
We can optionally pass args to super(). However in such a case

without the input statements all the classes of java.lang are always automatically loaded..
The System.in,System.out,System.err are inside the java.lang package and here in,out and err are the static reference variables of the System class
public final class System{
public static final PrintStream out; ---It is the declaration of the out
static{
	out=new PrintStream();
}
}

final variable:
A variable once it is initialized(first assignment) cannot be muted.Thus final variables are read only variables. A final variable can be both static and non-static
A final class cannot be inherited/extended and a final method is one which cannot be overriden in the derived class


1.check if the given positive integer is a perfect square
2.exam result percentage
3.Tax calculation problem
i/p data :name of the employee,basic salary
HRA 15% of basic if employee lives in cosmopolitian city, urban city 10%

//14/6/25//
always use single-responsibility principle because using 2 problem solutions at a time ,it will consume more time 
JVM-Most important program in JRE,SM,MM,GC(Garbage Collector),BV,LL-all together work under JRE
1. public static void main(String[] args):-
In java main() called by OS(JVM) and it doesnot return any value thats why it is void 
Static - the main method belongs to the class itself, rather than to any specific object of that class.
public because it is called by the JVM to access the member of the class in some other package
java is the parent most package in java and Object is the parent most class in java

2. Why constructor has same name as class name and do we hava a return type of java and does not it contain void?
Constructors have the same name as the class for clear identification and implicit invocation during object creation.The constructor has no return type and the constructor is implicitly void always

The threads are nothing but process which run inside the process simultaneously and it is a light-weight process
A process is a program in execution, meaning it's an active entity running on a computer. It has its own memory space, resources, and can't directly share these with other processes without specific mechanisms

Data structure:-
Storing data and also arranging/orgranizing it in memory in so specific way to archive some efficiency(space or time or simplicity) is data structure

1.find sum of the series 1-n+n2-n3...n terms 0<=n<=9 ,1 <=m<=25
2.find sum of odd digits of a number
2345=8(3+5)
3.find sum of even placed digits of a number
9128735=12(1+8+3)
91827364=10(1+2+3+4)
4.second largest and second smallest
consider an array of 5 elements for example, 4 5 3 2 1
here the second largest will be 4 and second smallest will be 2
sort the elements by Arrays.sort(); now it will become 1 2 3 4 5
consider for loop to make iterations
in for loop do the iterations upto n elements or n-1

5. Find sum of Odd placed Even Digits in a number. 
Note: You can make other 3 combinations for the above program.
6. Find smallest/biggest digit in a number.
7. Find sum of 1st and last digits in a number.
8. Find nth placed digit in a number
9. Count number of Prime digits in a number
10. Count number of composite digits in a number.
-----------------------------------------------------------
1. Write a program to print Right angled TRiangle of N lines.
2. Write a program to print Square of N lines.
3. Write a program to print Hollow Square of N lines.
4. Write a program to print X shape of N lines.
5. Write a program to print X shape inside a hollow square of N lines.
6. Write a program to print Hollow Benzene Ring of N lines.
Arrays:-
An array is a DS in which the elements of the array are stored in continous memory locations(There is no gap between 2 consecutive elements in an array).
All elements of an array are of the same DATATYPE(same size).
Array in java is an object because the objects has the array itself and also the length variable in it.
Once the size of an array is fixed, it cannot be changed.
To delete an array ,just make the reference null
It is a memory inefficient datastructures ,time efficient and its lookup is O(1)
simple and primary data structure
Creating:
int array[10]; //error
int[] array = new int[size];
int[] array =new int[5]{1,2,3,4,5};//error
int[] array=new int[]{1,2,3,4,5};
int[] array ={1,2,3,4,5};

--------------------------------------------------------------------------------------------------------
WEEK-2
// 16/6/25 //

CONSTRUCTORS:
*It is called/invoked always implicitly (by the JVM) as soon as an object is created(Objects get memory allocation,that means objects reference values is created).
*The object must have same name as that of the class so that the compiler/JVM knows which of the non static methods in the class are the constructors.
*We can define more than one constructor for the class, thus all the constructors must have the same name and this onset is called as "function overloading".
*The job of the constructor is to initialize an object.
*A constructor cannot return a value ever. Thus its return type is always void. Now something which is always, in  programming must be implicit, because it is always well known. thus mentioning the returnn type for a constructor becomes redundant and hence there must be no return type to a constructor.
*A constructor cannot be static because a constructor is working for an object and it is always has "this". If it is static it should be work for class and not for the object.
* Constructor is usually(99% of the cases) is public.however,they can be private also.
*Constructor cannot be final because all constructors are always final by nature i.e suppose the constructor is overriden in the sub class,and we create an object of the base class, the overriden constructor in the subclass becomes always unreachable.
*The constructor cannot be overriden 
*Constructors cannot be abstract(becuase the abstract methods are not declared but defined) becuase first of all a class choose instance method is abstract its object cannot be created. Further, even if the constructor is defined in the derived class, we know it is unreachable.
Ex: Assume there is a class named "Flight"
Filght(){

}
Flight(String code){

}
main(){ //assume this is the main method
Flight flight = new Flight(); // In this it calls the zero argumented constructor is called, this is called as "STATIC BINDING/COMPILE TIME BIDING/EARLY TIME BINDING"(it decides which function should be called/invoked at the compile stage).
}

FUNCTION OVERLOADING:
*It is the compile time(static/early) bindng/polymorphism which of the overloaded methods must be called is decided by the compiler.
*The overloaded methods if have the same arguments list but different return types then it is an error, because the change must be present at prologue and if the change is only present at apilogue.
*The overloaded functions/methods must differ in their argument list either by number of arguments or datatypes of the arguments or if incase if both the number of arguments and their types are the same the the order must be different.

RELATIONS IN OOPS:
the diagrams that will show the relations called "UML Diagrams" and the diagrams which shows the entry and exit loops and conditions are called "Cyclomatic diagrams"
1.Generalization(inheritance)- "is-a" type of relationship
2.Association - "has-a" part of
--Aggregation (weak or optional association)
--Composition(Strong association)

*Inheritance: suv is a car is-a vehicle
*Aggregation: When the whole object existance is independent of the part object then it is aggregation. When the part object(containee) and the whole object(container) can exist independent of each other.
*Composition: when the whole object existence is dependent on the part of the object existance. When the containee and container cannot exist independent of eachother.
Real life example

why the main() method is static?
definition means the data defined between{ }

ABSTRACT CLASS:
* A class which has at least one abstract method must be marked as abstract
* A class having all methods defined can also be abstract.
* A class having all methods abstract(Except constructors) is said to be 100% abstract
* The derived class which inherits an abstract class can be a concrete class itself doesnot declares any abstract method.
* If the derived class fails to define anyone of the abstract methods of the parent class then it itself must be marked as abstract
* An abstract class can still be used via its static memebers
-----------------------------------------------------------
no super keyword in c++ instead we use ::
-----------------------------------------
create a class to which only one object can be created:
1.for that we have to make all the constructors private but the User can create only one object inside the function
2.The user can access the method when it is public static
3.final A1 a1 = new A1();

public class A1{

}
public class User{
	A1 a1= new A1();
	A1 a2 =new A1();
}
Ex2:
 
//Create a class to which only one onject can be created:

public class A1 {

}
public class User {
	A a1 = new A();
	A a2 = new A();
}


import java.util.Scanner;

class Person {
    private int id;
    private String name;
    private char gender;
    private String location;
    private static Person person; //person is the reference to the Person
    
    private Person() {
        System.out.println("Person object is created");
    }
    
    public static Person createPerson() {
        if (person == null) {
            person = new Person();
        }
        return person;
    }
    
    public void setPerson(int id, String name, char gender, String location) {
        this.id = id;
        this.name = name;
        this.gender = gender;
        this.location = location;
    }
    
    @Override
    public String toString() {
        return "Person Details = Id:" + id + ", Name:" + name +
         ", Gender:" + gender + ", Location:" + location;
    }
}

public class Singleton {
    public static void main(String[] args) {
        Scanner scanner = new Scanner(System.in);
        Person person1 = Person.createPerson();
        person1.setPerson(101, "nithin", 'm', "mysuru");
        System.out.println(person1); // System.out.print(person.toString());
        
        Person person2 = Person.createPerson();
        System.out.println(person2); // System.out.print(person.toString());
        person2.setPerson(102, "aadya", 'f', "mysuru");
        System.out.println(person1); // System.out.print(person.toString());
    }
}


//17/6/25//
*which string objects in java are not stored in string pool(String pool is a storage space in the Java heap memory where string literals are stored)
...The string literals which stored with new keyword it wont be stored in string pool

//Orange partition
n=9 [18,36,5,79,50,65,30,15,33]
i=0 
j=0 
pivot =33
n=7 [15,23,5,30,40,50,55]
n=6 [50,40,35,30,20,15]


The packages only have code but in the folders we have different formats of files
The public members of the class are accessible in any other package

Function Call STACK:
...The intermediate results are stored in accumulator

IR: Instruction Register --- holds address of the current instruction that is running
PC: Program Counter ----holds address of next instruction to be executed
SP: Stack Pointer --- holds address of the top frame in the stack
FP: Frame Pointer --- Holds address of the frame of the funciton which currently running
Frame:----Memory allocated to a function during runtime
Contents of the Frame:
local variables: The default value of the local variable is NULL until it is initialized and it be known at compiletime,the default values are only available for static and instance variables
function parameter:The value will be known at the runtime
Function addresses: Called function addresses
Return address
when stack  Pointer and frame pointer is null that is the end of the program . This mechanism is designed by Dennis Ritchie
main(String... args){  //main frame is created first
   //args is the reference to the String array
    String s= new String("Hello"); // s is the reference to the string
    print("I am at home");
    A();//function call statement
    print("I am back home");
}
A(){
    int num=10;
     print("I am at home");
    B(45);
    print("I am back home");
}
B(int num){
 print("Hello world");
}


//18/6/25//
Searching,sorting algorithms:
CRUD Opearations(5):
1.File i/p
2.JDBC(Mysql or MongoDB)
3.Hibernate(ORM)
4.SpringBoot(Postman-to test the backend)
Frontend
5.HTML/CSS/JS
6.Reactjs
7.Bootstrap
8.Authenticaiton
9.Multiple entities (More than one tabels in the database)

0! =1 (factorial of zero is 1)
n!=n×(n−1)!
1!=1×0!
⇒1=1×0!
⇒0!=1

Linear search(sequential): O(n)
Given a list/array,we have to search for an element.
Count the frequency of an element (number of occurances).
Find the biggest element in an array
Find the 2nd smallest element in an array
Fing biggest and smallest elements in an array
Replace the every occurance of x with y in an array
Remove the spaces or the value -1 in an array
O(n):
1.Usually used to find the WCE of an algorithms
2.It finds the efficiency in terms of N, where N is the size of the i/p list.Thus it depends on N .
3.We can also find BCE and ACE as well
4.Its job is not to give the exact efficiency but to reveal the order in which the efficiency is.Thus we must remove the smaller chucks of value and also the constants.

Best case efficiency(BCE)  O(1)
ACE O(n)
WCE O(n)

Take the i/p zise from the user.
create the array with user given size.
Read the i/p data for the array.
Now search an element
Print the o/p - position where the element is found (1st occurance)

Problem:
ArrayList javacollection
Project Game 
     Game
     Player
Player
  id,name,points,playerCount (static)
  
define 2 classes:
Player -pojo class
PlayerOperation - CRUD 
 Menu:create/add player , modifyPlayer, deletePlayer, listAllPlayers,
Main()-- Menu based (do-while switch case)



OLTP(Online Transactional Processing)
OLAP(Online Analytical Processing)

inside instance methods we can access static methods

Interface: Type independent and abstract

Binary Search:
*Auto boxing(the int value is converted into INTEGER Object in the INTEGER wrapper class and all wrapper classes are placed in java.lang) and auto unboxing is the automatic conversion of wrapper class to primitive
The fastest algorithm
Pre-requiste: The i/p list must be sorted and we must also know the order of sorting
first we go to exact mid element 
if it is even number of elements it is easy,but it is odd number of elements it is not easy(we reduce the search area by 50% percent).
Example:
number of searches:
linear search       binary search
10,00,000            10,00,000
9,99,999            5,00,000
9,99,,998            2,50,000
9,99,997            1,25,000
9,99,996            62,500
9,99,995            31,250
9,99,994...          15,525.... 1000,500,250,125,62,31,15,7,4,3,2,1
why the iterations is not visible in binary search?
The amount of area is decreased and that decrease we can predict but cannot define it 
Best case efficiency --O(1) (at mid position)
Finding Worst case efficiency in binary search:
Assume it has  X  number of iterations
number of elements in iteration:
N      N/2 N/4 N/8  .......................... 8    4     2     1
2**(x-1)                                         2**3 2**2 2**1  2**0
2**(x-1) =N  ,N is the number of elements in the iterations
2**(x) = N 
x= log(N)
x=y-z
x=y+z 
x/y =z 
y=x/z
in loop low<=high---when low becomes equal to high it means we became at the last element of the iteration 
low<high---it prints that There is a chance of the very first element and last element is not found

for all sorting techniques:
  for i from 0 to n-1:
      for j from 0 to n-i-1 do:
         compare consecutive elements
            swap
//assume array elements 20
public void bubbleSort(int[] array){
    for(int i=0;i<array.length-1;i++){ //here 20-1=19 ,therefore i<18
        for(int j=0;j<array.length-i;j++){
          if(array[j]<arr[j+1])
             swap(array[j],array[j+1]);
        }
    }
}
int n=arr.length;                               int n =arr.length; boolean swaped;
                                                 do{ 
while(true){                                    swaped=false;
    boolean swaped=false;                         for(int j=0;j<n-1;j++){
    int i=0;                                       if(arr[j]>arr[j+1]){
    while(i<n-1){                                     int temp =arr[j];
        if(arr[i]>arr[i+1]){                           arr[j]=arr[j+1];
            int temp = arr[i];                           arr[j+1] = temp;
            arr[i] =arr[i+1];                         }
            arr[i+1] =temp;                       } 
        }                                        n--;
    }                                            } while(swapped);
    if(!swaped) break; 
}
Best case scenario for almost all sorting techniques are the input array is already sorted(almost sorted)
Best case efficiency : O(n2)
Worst :O(n2) 
worst case scenario for almost all sorting techniques is the input array is already sorted(almost sorted) but we are sorting in reverse order

Bubble Sort: (swaping)
Unsorted array and we will get the sorted array at the end
Pesimistic sorting technique
O(n)
number of swaps is unpredictable
public void bubbleSort(int[] array){
     boolean sorted =true; //assume the input array is sorted
    for(int i=0;i<array.length-1;i++){ //here 20-1=19 ,therefore i<18
        for(int j=0;j<array.length-i;j++){
          if(array[j]<arr[j+1]){
             swap(array[j],array[j+1]);
             sorted=false;
          }
          if(sorted)
             break;
        }
    }
}
for optimized bubble sort
BCE O(n)
WCE(n2)

5 4 3 2 1
4 3 2 1 5  4 swaps
3 2 1 4 5  3
2 1 3 4 5  2
1 2 3 4 5  1
if the i/p size is 5 then the number of swaps is 10
n*(n-1)/2= 5*(5-1)/2 = 5*4/2=10
(n-1)(n-1+1)/2  = (5-1)(5-1+1)/2 = (4)(5)/2=10

for all sorting techniques:
  for i from 0 to n-1:
      for j from 0 to n-i-1 do:
         compare consecutive elements
            swap

//assume array elements 20
public void bubbleSort(int[] array){
    for(int i=0;i<array.length-1;i++){ //here 20-1=19 ,therefore i<18
        for(int j=0;j<array.length-i;j++){
          if(array[j]<arr[j+1])
             swap(array[j],array[j+1]);
        }
    }
}

Best case scenario for almost all sorting techniques are the input array is already sorted(almost sorted)
Best case efficiency : O(n2)
Worst :O(n2) 
worst case scenario for almost all sorting techniques is the input array is already sorted(almost sorted) but we are sorting in reverse order
//assume elements are 20
public void bubbleSort(int[] array){
     boolean sorted =true; //assume the input array is sorted
    for(int i=0;i<array.length-1;i++){ //here 20-1=19 ,therefore i<18
        for(int j=0;j<array.length-i;j++){
          if(array[j]<arr[j+1]){
             swap(array[j],array[j+1]);
             sorted=false;
          }
          if(sorted)
             break;
        }
    }
}
for optimized bubble sort
BCE O(n)
WCE O(n2)





Selection Sort:(comparision and swaping) repeatedly finds the minimum the element from the unsorted portion and places it at the beginning
         4 6 4 1 2 9 10 6
  index: 0 1 2 3 4 5 6  7
small =4
index =0
1.find the minimum element in unsorted array
2.swap it with the first element
3.move the boundary of unsorted array by one position
4.repeat until the entire array is sorted

public static void selection(int[] unsorted){
    n=unsorted.length;
    for(int i=0;i<n-1;i++){
        int min=i;
        for(int j=i+1;j<n;j++){
            if(arr[j] <arr[min]){
                min=j;
            }
        }
            int temp = arr[min];
            arr[min]=arr[i];
            arr[i]=temp;
        
    }
}

Insertion Sort (Decrease and Conquer/shifting):
Optimistic sorting technique
why Insertion sort has better efficiency in the worst case when compared to bubble sort?
..
23  11  3 13  7  5  29  17  23 //length=9
final element :23
here we shifted no swaped
now
11 23
3 11 23
3 11 13 23 
3 7 11 13 23
3 5 7 11 13 23 
3 5 7 11 13 23 29 - here zero swap so increment
3 5 7 11 13 17 23 29 
3 5 7 11 13 17 23 29 23
3 5 7 11 13 17 23  23 29 

1.starts with second element (assume first is sorted)
2.compare with elements in sorted portion
3.insert current element at correct position
4.repeat for all elements

public void insertionSort(int[] array){
    for(int i=1;i<array.length;i++){ //run through the elements unsorted array
        element=array[i];
        int j=i-1; //j is the index of the element in the sorted array in which we have to compare the element in the unsorted array
        while(j>=0 && element <array[j]){ //shifting the element in sorted
            array[j+1]=array[j];
            j--;
        }
        array[j+1] =element;
    }
}

int n=arr.length;
for(int i=1;i<n;i++){ //1..n-1,2..n-1,......,n-1..n-1 //unsorted part
    int key =arr[i]; //take the first element of unsorted part
    int keyIndex = i;
    for(int j=i-1;j>=0;j--){//0...0,1..0,2...0,.......,n-1..0 //sorted part
        if(arr[j]>key){
            arr[j+1]=arr[j]; //shift right
            keyIndex--;//find new key index
            continue;
        }
        break;
    }
    if(i!=keyIndex)  arr[keyIndex]=key; //insert
}
Best case scenario : already sorted array
BCE -O(n)
WCE -O(n2)
average -O(n2)

Quick Sort(divide and conquer):
1.choose a pivot element
2.partition away 
3.
4.
private static int partition(int[] arr,int low,int high){
    int pivot = arr[high]; // choose last element
    int i=low-1; //index of smaller element
    for(int j=low,j<high;j++){
        if(arr[j]<=pivot){
            i++;
            int temp = arr[i];
            arr[i]=arr[j];
            arr[j]=temp;
        }
    }
    int temp =arr[i+1];
    arr[i+1]=arr[high];
    arr[high]=temp;
}

Merge sort:
1.divide array into two halves
2.recursively sort both halves
3.merge the sorted halves

*Depth/height of the tree ,we can insert 2**(h)-1 ,if the actual number of nodes are less than 2(h)-1 it is imbalanced tree
AVL TREE:


//19/6/25//
Problem solving in Guvi HCl portal 
Bitwise Operators:
& | ^ >> << ~
byte num1 = 29;//00011101
byte num2 =18;//00010010
int result = num1 & num2;
System.out.println("Num1 and Num2" +result);
int result1 = num1 & num2;
System.out.println("Num1 and Num2" +result1);
int result2 = num1 | num2;
System.out.println("Num1 and Num2" +result2);
int result3 = num1 ^ num2;
System.out.println("Num1 and Num2" +result3);
int result4 = num1 >> num2; //right shift
System.out.println("Num1 and Num2" +result4);
int result5 = num1 << num2; //lef shift
System.out.println("Num1 and Num2" +result5);
int result6 = ~ num2;
System.out.println("Num2 " +result6);
Problems on Bitwise operators
1.Find the nth bit of an int variable
2.flip all the bits of a variable and print its value.
3.Masking
-------------------------------------------------------------------
INTERVIEW SKILLS:
MCQ TEST:
MCSR(Multiple choice single response):
1. Direct method
2.Indirect method
MCMR:
1.Negative marking

Problem: //permutation
input num =8476273
output num =the next biggest number (8476327)
//use this problem using recursion and also can iteration
//write a four digit number in that any digit should repeat twice only . it is Karprekar constant problem
i/p: 3421
step1 : 4321 //decreasing
1234 //increasing
4321-1234 =3087 //difference between biggest and smallest
step2:
i/p: 3087
8730
0378
8730-0378=8352
i/p:8352
int num =scanner.nextInt();
String inputNum =String.valueOf(nums);
char[] numArray =input.toCharArray();
Arrays.sort(numArray);
inputNum = new String(numArray);
int small = integer.ParseInt(inputNum);
StringBuilder sb = new StringBuilder(inputNum);
sb.reverse();
inputNum = sb.toString();
int bigNum = Integer.ParseInt(inputNum);
int difference = bigNum - small;


---------------------------------------------------------------
DAY9 FRIDAY 20-06-2025 (absent)

STEPS TO DOWNLOAD MYSQL:

Google Search: Download MySQL
Click on Link:   mysql.com/downloads
-> MySQL Community GPL Downloads 
-> MySQL Installer for Windows 
-> No thanks start my download
Note: Download the Latest version (8.0.3) and the Bigger Sized File (330 MB or so)

Steps to Install MySQL:
Choose Setup type as Full -> Click Next
MySql Server + Mysql WorkBench + MySql Shell (Drag all these to right)
Do not select the check box -> Click Next
Click on Execute (Make sure that all 3 Apps are visible in "Installation") -> Click Next
Type and Networking -> Click Next
Product Config -> click Next -> Port Number is shown -> Click Next
Use Strong Password -> Click Next
Set the Password (Remember it) -> Click Next
Windows Service -> Click Next
Server File Permissions -> Click Next
Apply Config -> Click Execute
Successful Message -> Click Finish 
Product Config -> Click Next
Installation Complete -> Click Finish
Workbench Runs
Go to MysQl Folder -> server folder -> bin (Add path to Environment Vars)

cmd -> mysql --version

mysql -u root -p   (To force the mysql to prompt for password)

show databases;  // run this command
create database db1;
--------------------------------------------------------------
//21/6/25//
if we cant get the version of mysql in command prompt:
Steps:
1. c:/programfiles/mysql/mysql server/bin (copy the path)
2. go to environment variables
3. path->new -> paste the path
now go the command prompt and enter mysql --version (it gives the version of the mysql)

C:\Users\tejas>mysql --version
mysql  Ver 8.0.42 for Win64 on x86_64 (MySQL Community Server - GPL)

C:\Users\tejas>mysql -u root -p
Enter password: *********  (Teju@2903)
Welcome to the MySQL monitor.  Commands end with ; or \g.
Your MySQL connection id is 22
Server version: 8.0.42 MySQL Community Server - GPL

Copyright (c) 2000, 2025, Oracle and/or its affiliates.

Oracle is a registered trademark of Oracle Corporation and/or its
affiliates. Other names may be trademarks of their respective
owners.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

mysql>  show databases
    -> ;
+--------------------+
| Database           |
+--------------------+
| information_schema |
| mysql              |
| performance_schema |
| sakila             |
| sys                |
| world              |
+--------------------+
6 rows in set (0.01 sec)

mysql> create databse training
    -> ;
ERROR 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'databse training' at line 1
mysql> create databse trainingdb;
ERROR 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'databse trainingdb' at line 1
mysql> create databse trainingdb
     ->;
ERROR 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'databse trainingdb' at line 1
mysql> CREATE DATABASE trainingdb;
Query OK, 1 row affected (0.01 sec)

mysql> show databases;
+--------------------+
| Database           |
+--------------------+
| information_schema |
| mysql              |
| performance_schema |
| sakila             |
| sys                |
| trainingdb         |
| world              |
+--------------------+
7 rows in set (0.00 sec)

mysql> USE trainingdb;
Database changed

mysql> create table people(id int primary key auto_increment, name varchar(50) not null, location varchar(50), gender varchar(2), age smallint default(0));
Query OK, 0 rows affected (0.04 sec)

mysql> insert into people(john,kakinada,M,18);
ERROR 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near '18)' at line 1
mysql> insert into people(john,kakinada,'M',18);
ERROR 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near ''M',18)' at line 1
mysql> insert into people('john','kakinada','M',18);
ERROR 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near ''john','kakinada','M',18)' at line 1
mysql> insert into people(name,location,gender) values ('john','kakinada','M',18);
ERROR 1136 (21S01): Column count doesn't match value count at row 1

mysql> insert into people(name,location,gender,age) values ('john','kakinada','M',18);
Query OK, 1 row affected (0.01 sec)

mysql> insert into people(name,location,gender,age) values ('Tom','Rajamundry','M',17);
Query OK, 1 row affected (0.01 sec)

mysql> insert into people(name,location,gender,age) values ('Lila','kakinada','F',18);
Query OK, 1 row affected (0.01 sec)

mysql> select * from people;
+----+------+------------+--------+------+
| id | name | location   | gender | age  |
+----+------+------------+--------+------+
|  1 | john | kakinada   | M      |   18 |
|  2 | Tom  | Rajamundry | M      |   17 |
|  3 | Lila | kakinada   | F      |   18 |
+----+------+------------+--------+------+
3 rows in set (0.00 sec)

mysql> insert into people(name,gender,age) values ('David','M',18);
Query OK, 1 row affected (0.01 sec)mysql> select * from people;
+----+-------+------------+--------+------+
| id | name  | location   | gender | age  |
+----+-------+------------+--------+------+
|  1 | john  | kakinada   | M      |   18 |
|  2 | Tom   | Rajamundry | M      |   17 |
|  3 | Lila  | kakinada   | F      |   18 |
|  4 | David | NULL       | M      |   18 |
+----+-------+------------+--------+------+
4 rows in set (0.01 sec)


-----------------------23/6/25 to 29/6/25 (Project space holidays)-------------------------------------------

// 30/6/25 //
Recursion and Backtracking
>>> Recursion algorithms(Towers of hanoi, fibonnaci series, factorials)
A recursion function is the function that calls itself either directly or indirectly
Structure of a recursive function:
1.Base case:
  The condition under which the function will not call itself again
2.Recursive case:
    The condition under which the function will call itself again with a simpler or smaller input

Types of Recursion:
1.Direct Recursion:
   The function calls itself directly
   Example: Factorial function
   factorial(n) {
       if (n == 0) return 1; // Base case
       return n * factorial(n - 1); // Recursive case
   }
2.Indirect Recursion:
    The function calls another function, which in turn calls the first function
    Example: Two functions calling each other
    functionA() {
         functionB();
    }
    functionB() {
         functionA();
    }
3.Tail Recursion:
    The recursive call is the last operation in the function, allowing for optimization by the compiler or interpreter
    Example: Tail-recursive factorial function
    tailFactorial(n, accumulator = 1) {
         if (n == 0) return accumulator; // Base case
         return tailFactorial(n - 1, n * accumulator); // Recursive case
    }

Advantages:
..Simplifies code for the Problems that have a recursive structure 
..can make code more reliable and easier to understand
Disadvantages:
..Can lead to stack overflow if the recursion depth is too high

Usage:
..Mathematical problems (factorial, Fibonacci)
..Data structures (trees, graphs)
..Algorithms(quick sort, merge sort,binary search)

>>> Backtracking:
It is systematically searches for a solution to a problem among all available options
It is a problem-solving technique that builds candidates for solutions incrementally and abandons candidates as soon as it determines that they cannot lead to a valid solution

Structure of a Backtracking Algorithm:
1.Choose:
   Select an option from the available choices
2. Explore:
   Recursively explore the next level of the search space
3. Evaluate:
   Check if the current solution is valid
4. Backtrack:
   If the solution is not valid, undo the last step and try the next option

How backtracking works:

** Radix Sort **
consider 23,87,11,113,56,48,9,103
now see the digits of the numbers as take 23 in one's place it is having 3 and in ten's place it is having 2
Therefore in 10^0 (1)=3 and 10^1(10)=2
now sort the elements according to 1's place
{011} {023,113,103} {056} {087} {048} {009}
sorting for ten's place
{103,009} {011,113} {023} {048} {056} {087}
sorting at 100's place
{009,011,023,048,056,087} {103,113} therefore the elements are sorted

Pseudo code:
function radixSort(array,N,place) //pass {
   dict = parse of each pair is of key:digit, value:array
   for 0 in array:
    digit = 0/place % 10
    if the key 'digit' not in dict:
      dict[digit] = new arr
    dict[digit].add(0)
   //copy numbers in the dictionary back to array
   i =0
   for group in dictValues:
     for 0 in group:
        arr[i]=0
        i++
}
 function sort(array,N){
    //find the max number in the array
    max = maximum(array,N)  
    iterate base/radix/place = 1,10,100 < max
     radixSort(array,N,place)
    print(array,N)
 }

** Bucket Sort **
the bucket sort technique is used to store the elements in the given range in the buckets 
in other words,  distributing elements of an array into a number of "buckets." Each bucket is then sorted individually(we can use any sorting technique), and finally, the elements are gathered back into the original array in sorted order.


//1/7/25//
Detailed Notes on Recursion:
    1. Definition:
        - Recursion is a method 
          where the solution to a problem 
          depends on solutions to smaller instances of the same problem.
        - A recursive function is 
          a function that calls itself, 
          either directly or indirectly.

    2. Structure of a Recursive Function:
        - Base Case: 
          The condition under which the recursion ends. 
          Prevents infinite recursion.
        - Recursive Case: 
          The part where the function calls itself 
          with a simpler or smaller input.

    3. How Recursion Works:
        - Each recursive call 
          adds a new frame to the call stack.
        - When the base case is reached, 
          the stack unwinds as each function call returns.

    4. Types of Recursion:
        - Direct Recursion: 
          A function calls itself directly.
        - Indirect Recursion: 
          A function calls another function, 
          which eventually calls the first function.
        - Tail Recursion: 
          The recursive call is the last operation in the function. 
          Some languages optimize tail recursion 
          to avoid stack overflow.

    5. Advantages:
        - Simplifies code for problems 
          that have a recursive structure 
          (e.g., tree/graph traversal, divide and conquer algorithms).
        - Can make code more readable and 
          easier to maintain.

    6. Disadvantages:
        - Can lead to high memory usage 
          due to call stack growth.
        - May be less efficient 
          than iterative solutions 
          for some problems.
        - Risk of stack overflow 
          if the base case is not reached or 
          recursion is too deep.

    7. Common Uses:
        - Mathematical computations (factorial, Fibonacci numbers).
        - Data structures (traversing trees, graphs).
        - Algorithms (quick sort, merge sort, binary search).

    8. Example (Factorial):

        ```python
        def factorial(n):
            if n == 0 or n == 1:  # Base case
                return 1
            else:
                return n * factorial(n - 1)  # Recursive case
        ```

    9. Tips for Writing Recursive Functions:
        - Always define a clear base case.
        - Ensure each recursive call progresses toward the base case.
        - Consider the maximum recursion depth and stack limitations.
        - Test with small inputs first to verify correctness.

    10. Recursion vs Iteration:
        - Recursion uses function calls and 
          the call stack; 
          iteration uses loops.
        - Some problems are naturally recursive, 
          while others are more efficiently solved with iteration.

Detailed Notes on Backtracking:
    1. Definition:
        - Backtracking is a general algorithmic technique 
          for solving problems incrementally, 
          by trying partial solutions and 
          then abandoning them 
          if they do not lead to a valid solution.
        - It systematically searches 
          for a solution to a problem 
          among all available options.

    2. Structure of a Backtracking Algorithm:
        - Choose: Make a choice from available options.
        - Constraint: Check if the current choice 
          leads to a valid solution 
          (satisfies constraints).
        - Explore: Recursively explore further choices.
        - Unchoose (Backtrack): Undo the last choice and 
          try another option.

    3. How Backtracking Works:
        - The algorithm builds candidates 
          for the solution step by step.
        - If a candidate fails to satisfy the constraints, 
          the algorithm backtracks to the previous step and 
          tries a different option.
        - This process continues 
          until all possibilities are explored or 
          a solution is found.

    4. Applications:
        - Solving puzzles 
          (e.g., Sudoku, N-Queens problem).
        - Combinatorial problems 
          (e.g., permutations, combinations, subset sum).
        - Constraint satisfaction problems 
          (e.g., crosswords, map coloring).

    5. Advantages:
        - Finds all possible solutions 
          (if required).
        - Can be more efficient than brute-force search 
          by pruning invalid paths early.

    6. Disadvantages:
        - Can be slow for large problem spaces 
          due to exponential time complexity.
        - May require significant memory 
          for deep recursion or large state spaces.

    7. Example (N-Queens Problem):
        ```python
        def solve_n_queens(n):
            def is_safe(board, row, col):
                for i in range(row):
                    if board[i] == col or \
                        board[i] - i == col - row or \
                        board[i] + i == col + row:
                        return False
                return True

            def solve(row, board, solutions):
                if row == n:
                    solutions.append(board[:])
                    return
                for col in range(n):
                    if is_safe(board, row, col):
                        board[row] = col
                        solve(row + 1, board, solutions)
                        board[row] = -1  # Backtrack

            solutions = []
            solve(0, [-1] * n, solutions)
            return solutions
        ```

    8. Tips for Writing Backtracking Algorithms:
        - Clearly define the constraints and base case.
        - Use pruning to eliminate invalid candidates early.
        - Keep track of the current state and 
          undo changes when backtracking.
        - Test with small inputs to verify correctness.

    9. Backtracking vs Recursion:
        - Backtracking is often implemented using recursion, 
          but not all recursive algorithms use backtracking.
        - Backtracking specifically involves undoing choices and 
          exploring alternative paths 
          when a constraint is violated.
N-Queens:
Detailed step-by-step explanation of the provided N-Queens code:
    1. Imports and Class Declaration
        ```java
        import java.util.*;

        public class NQueens {
        ```
        - `import java.util.*;`  
           imports all utility classes 
           (like `Scanner`, `List`, `ArrayList`, etc.).
        - The class `NQueens` contains the main logic.

    2. Main Method
        ```java
        public static void main(String[] args) {
            Scanner sc = new Scanner(System.in);
            int n = sc.nextInt();
            List<List<String>> solutions = solveNQueens(n);
            if (solutions.isEmpty()) {
                System.out.println("No solution");
            } else {
                for (List<String> sol : solutions) {
                    for (String row : sol) {
                        System.out.println(row);
                    }
                    System.out.println();
                }
            }
        }
        ```
        - Reads an integer `n` 
          (the size of the chessboard and number of queens).
        - Calls `solveNQueens(n)` 
          to get all valid solutions.
        - If no solution exists, 
          prints "No solution".
        - Otherwise, 
          prints each solution (each as a list of strings, one per row).

    3. Solving N-Queens
        ```java
        private static List<List<String>> solveNQueens(int n) {
            List<List<String>> res = new ArrayList<>();
            char[][] board = new char[n][n];
            for (char[] row : board) Arrays.fill(row, '.');
            backtrack(res, board, 0, n, new boolean[n], new boolean[2 * n], new boolean[2 * n]);
            return res;
        }
        ```
        - Initializes the result list `res`.
        - Creates an `n x n` board filled with `'.'` (empty cells).
        - Calls `backtrack` to fill the board row by row.
        - Uses three boolean arrays to track columns and diagonals:
        - `cols`: columns with queens.
        - `d1`: "main" diagonals (top-left to bottom-right).
        - `d2`: "anti" diagonals (top-right to bottom-left).

    4. Backtracking Function
        ```java
        private static void backtrack(List<List<String>> res, char[][] board, int row, int n, boolean[] cols, boolean[] d1, boolean[] d2) {
            if (row == n) {
                List<String> sol = new ArrayList<>();
                for (char[] r : board) sol.add(new String(r));
                res.add(sol);
                return;
            }
            for (int col = 0; col < n; col++) {
                int id1 = col - row + n, id2 = col + row;
                if (cols[col] || d1[id1] || d2[id2]) continue;
                board[row][col] = 'Q';
                cols[col] = d1[id1] = d2[id2] = true;
                backtrack(res, board, row + 1, n, cols, d1, d2);
                board[row][col] = '.';
                cols[col] = d1[id1] = d2[id2] = false;
            }
        }
        ```
        - Base Case: If `row == n`, a valid solution is found. 
          Convert the board to a list of strings and add to results.
        - For Each Column: Try placing a queen 
          in each column of the current row.
        - Diagonal Indexing:
            - `id1 = col - row + n` for main diagonals 
            (offset by `n` to avoid negative indices).
            - `id2 = col + row` for anti-diagonals.
        - Safety Check: If the column or 
          either diagonal is already occupied, 
          skip.
        - Place Queen: 
          Mark the board and tracking arrays.
        - Recursive Call: Move to the next row.
        - Backtrack: Remove the queen and 
          reset tracking arrays (undo the move).
        - Continue until all columns are tried.

** Sudoko **
It is a 9x9 grid puzzle where the objective is to fill the grid with digits from 1 to 9 such that no row,column and subgrid contains the same digit more than once.
Step-by-step explanation of your Sudoku solver code:
    1. Imports and Class Declaration
        ```java
        import java.util.*;

        public class SudokuSolver {
        ```
        - Imports all classes from `java.util` (only `Scanner` is used).
        - Declares the `SudokuSolver` class.

    2. Main Method: Input and Output
        ```java
        public static void main(String[] args) {
            Scanner sc = new Scanner(System.in);
            char[][] board = new char[9][9];
            for (int i = 0; i < 9; i++) {
                String[] line = sc.nextLine().trim().split("\\s+");
                for (int j = 0; j < 9; j++) {
                    board[i][j] = line[j].charAt(0);
                }
            }
            solveSudoku(board);
            for (int i = 0; i < 9; i++) {
                for (int j = 0; j < 9; j++) {
                    System.out.print(board[i][j]);
                    if (j < 8) System.out.print(" ");
                }
                System.out.println();
            }
        }
        ```
        - Reads a 9x9 Sudoku board from standard input, one line at a time.
        - Each line is split by whitespace, and 
          each cell is stored as a `char` 
          (e.g., `'5'`, `'.'` for empty).
        - Calls `solveSudoku(board)` to solve the puzzle.
        - Prints the solved board, formatting with spaces between numbers.

        Gotcha: Input must be 9 lines, 
                each with 9 space-separated characters.

    3. Sudoku Solver (Backtracking)
        ```java
        public static boolean solveSudoku(char[][] board) {
            for (int row = 0; row < 9; row++) {
                for (int col = 0; col < 9; col++) {
                    if (board[row][col] == '.') {
                        for (char c = '1'; c <= '9'; c++) {
                            if (isValid(board, row, col, c)) {
                                board[row][col] = c;
                                if (solveSudoku(board)) return true;
                                board[row][col] = '.';
                            }
                        }
                        return false;
                    }
                }
            }
            return true;
        }
        ```
        - Loops through each cell.
        - If an empty cell (`'.'`) is found:
        - Tries all digits `'1'` to `'9'`.
        - If `isValid` returns true, 
          places the digit and recursively tries to solve the rest.
        - If recursion fails, resets the cell to `'.'` (backtracks).
        - If all cells are filled, returns `true` (solved).

        Gotcha: Returns `false` if no valid digit can be placed, 
                triggering backtracking.

    4. Validation Function
        ```java
        private static boolean isValid(char[][] board, int row, int col, char c) {
            for (int i = 0; i < 9; i++) {
                if (board[row][i] == c) return false; // Row check
                if (board[i][col] == c) return false; // Column check
                if (board[3*(row/3)+i/3][3*(col/3)+i%3] == c) return false; // 3x3 box check
            }
            return true;
        }
        ```
        - Checks if placing `c` at `(row, col)` is valid:
        - Not already in the same row.
        - Not already in the same column.
        - Not already in the same 3x3 subgrid.

        Gotcha: The 3x3 box calculation is a common source of confusion:
        - `3*(row/3)` and `3*(col/3)` find the top-left corner of the box (sub-grid).
        - `i/3` and `i%3` iterate over the 3x3 box.

** Permutation of a string **
A permutation of a string is a "rearrangement" of its characters to form a new string or sequence. It essentially means changing the order of the characters while keeping all the original characters present
n!--- for the permutation for not repeated characters

// done codekata problems from 51 - 60 problems


// 2/7/25 //
-  A graph is a non-linear data structure 
   made up of vertices (nodes) and edges (connections). 
   It is used to represent networks 
   like social connections, maps, the internet, 
   dependency structures, and more.
    🔹 Types of Graphs:
        * Directed vs Undirected: Edges have direction or not.
        * Weighted vs Unweighted: Edges may carry weights (e.g., distances, costs).
        * Cyclic vs Acyclic: May or may not contain cycles.
        * Connected vs Disconnected: Whether all nodes are reachable from each other.
        * Simple vs Multigraph: No multiple edges between the same pair vs allowing them.

    🔹 Representation:
        * Adjacency Matrix: 2D array showing connections.
        * Adjacency List: List of nodes with their neighbors (space-efficient).

    🔹 Applications:
        * Shortest path finding (Dijkstra, Bellman-Ford)
        * Topological sort (DAG)
        * Network routing
        * Cycle detection
        * Social network analysis
        * Game AI and puzzles

Notes on graph    
    🔹 Properties of Graphs:
        * Order: Number of vertices (nodes) in the graph.
        * Size: Number of edges in the graph.
        * Degree: Number of edges connected to a vertex.
        * Path: Sequence of vertices connected by edges.
        * Cycle: Path that starts and ends at the same vertex.
        * Connectedness: Whether there is a path between every pair of vertices.
        * Components: Subgraphs in which any two vertices are connected.

    🔹 More Types of Graphs:
        * Complete Graph: Every pair of vertices is connected.
        * Bipartite Graph: Vertices can be divided into two sets, with edges only between sets.
        * Tree: A connected acyclic undirected graph.
        * DAG (Directed Acyclic Graph): Directed graph with no cycles.

    🔹 Graph Representations (Examples):

        * Adjacency Matrix (for 4 nodes):
            0 1 2 3
          0 0 1 0 1
          1 1 0 1 0
          2 0 1 0 1
          3 1 0 1 0

        * Adjacency List:
            0: 1, 3
            1: 0, 2
            2: 1, 3
            3: 0, 2

    🔹 Graph Traversal Algorithms:

        * Breadth-First Search (BFS):
            - Explores neighbors level by level.
            - Uses a queue.
            - Finds shortest path in unweighted graphs.

            Example (Adjacency List):
                Graph: 0: [1,2], 1: [0,3], 2: [0,3], 3: [1,2]
                BFS from 0: Visit 0 → 1 → 2 → 3

            Python Example:
                from collections import deque
                def bfs(graph, start):
                    visited = set()
                    queue = deque([start])
                    while queue:
                        node = queue.popleft()
                        if node not in visited:
                            print(node)
                            visited.add(node)
                            queue.extend(n for n in graph[node] if n not in visited)

        * Depth-First Search (DFS):
            - Explores as far as possible along each branch before backtracking.
            - Uses a stack (can be implemented recursively).
            - The recursive function in graph is Dfs
            Example (Adjacency List):
                Graph: 0: [1,2], 1: [0,3], 2: [0,3], 3: [1,2]
                DFS from 0: Visit 0 → 1 → 3 → 2

            Python Example:
                def dfs(graph, node, visited=None):
                    if visited is None:
                        visited = set()
                    if node not in visited:
                        print(node)
                        visited.add(node)
                        for neighbor in graph[node]:
                            dfs(graph, neighbor, visited)
                            
>>>DFS and BFS 
why scanner.nextLine(); is added before declaration of string like this  Scanner scanner = new Scanner(System.in);
        int n = scanner.nextInt();
        List<Integer> arr = new ArrayList<>();
        scanner.nextLine();
        String secondLine =scanner.nextLine();
// This is done to consume the newline character left in the input buffer after reading an integer with nextInt().
// If we don't do this, the next call to nextLine() will read an empty string
// because it encounters the leftover newline character from the previous input.
int n = scanner.nextInt();   // reads number, leaves \n
scanner.nextLine();          // consumes the leftover newline
String secondLine = scanner.nextLine();  // now reads the actual line

input: 
5
10 20 30 40 50
output:
line = ""   // oops! it's just the leftover \n
10 20 30 40 50

int n = scanner.nextInt();
scanner.nextLine();  // flush the newline
String line = scanner.nextLine();  // correctly reads "10 20 30 40 50"
// This ensures that the nextLine() call works as expected and captures the intended input.
>>>Heap techniques 
32 29 3 4 1 5 6 18 7 20  n=10

// 3/7/25  afternoon:AMCAT exam //

Queue : offer() ,poll(), peek() 
>>peek():
 Returns the smallest element (for a min-heap) or the largest element (for a max-heap) without removing it from the heap.
>> poll()
 Returns and removes the smallest (min-heap) or largest (max-heap) element from the heap.
... peek() → look at the top element
...poll() → remove and return the top element
 minHeap.offer(5);
        minHeap.offer(3);
        minHeap.offer(8);  
minheap:[3, 5, 8]
Index:   0   1   2
Value:   3   5   8

        3        ← index 0
       / \
      5   8      ← indices 1 and 2
heap[i] <= heap[2*i + 1]  // left child (if exists)
heap[i] <= heap[2*i + 2]  // right child (if exists)
For i = 0 (value = 3):
Left child: heap[1] = 5 → ✅ 3 <= 5
Right child: heap[2] = 8 → ✅ 3 <= 8
Heap condition is satisfied.

For i = 1 (value = 5):
2*i + 1 = 3 and 2*i + 2 = 4 → indices out of bounds → no children → ✅

For i = 2 (value = 8):
Same: no children → ✅

SOLID principles:
>> open-close principle(O) , single rule principle(S), liskov substitution principle(L), dependency inversion principle(D), interface segregation principle(I)
>> Liskov Substitution Principle (LSP):
    - Objects of a superclass should be replaceable with objects of a subclass without affecting the correctness of the program.
    - Example: If `Bird` is a superclass and `Penguin` is a subclass, then `Penguin` should be able to replace `Bird` without causing issues in the code that uses `Bird`.
>> Dependency Inversion Principle (DIP):
    - High-level modules should not depend on low-level modules. Both should depend on abstractions (interfaces).
    - Abstractions should not depend on details; details should depend on abstractions.
    - Example: Instead of a class directly instantiating another class, it should use an interface or abstract class to allow for flexibility and easier testing.
>>open-close principle:
open for extension, closed for modification
    - Classes should be open for extension but closed for modification.
    - This means you can add new functionality by extending existing classes without changing their source code.
    - Example: If you have a class `Shape`, you can extend it to create `Circle` and `Square` without modifying the `Shape` class itself.
>> Single Responsibility Principle (SRP):
    - A class should have only one reason to change, meaning it should have only one job or responsibility.
    - Example: A class that handles user authentication should not also handle user notifications. These should be separate classes.
>> Interface Segregation Principle (ISP):
    - Clients should not be forced to depend on interfaces they do not use.
    - This means that interfaces should be small and specific rather than large and general.
    - Example: Instead of having a single `Animal` interface with methods for flying, swimming, and walking, you could have separate interfaces like `Flyable`, `Swimmable`, and `Walkable` so that classes only implement what they need.
Heap:
    - A heap is a specialized tree-based data structure that satisfies the heap property.
    - In a heap, for any given node, 
      the value of the node is either 
      greater than or equal to (in max-heap) or 
      less than or equal to (in min-heap) 
      the values of its children.
    - Heaps are commonly implemented as binary heaps, 
      where each parent has at most two children.
    - Heaps are complete binary trees, 
      meaning all levels are fully filled except possibly the last, 
      which is filled from left to right.

Min-Heap:
    - In a min-heap, 
      the value of each parent node is 
      less than or equal to the values of its children.
    - The smallest element is always at the root.
    - Common operations: 
        insert, extract-min (remove the smallest element), decrease-key.
    - Used in algorithms like Dijkstra's shortest path and Prim's minimum spanning tree.

Max-Heap:
    - In a max-heap, 
      the value of each parent node is 
      greater than or equal to the values of its children.
    - The largest element is always at the root.
    - Common operations: 
        insert, extract-max (remove the largest element), increase-key.
    - Used in algorithms like heap sort and for implementing priority queues.

Priority Queue:
    - A priority queue is an abstract data type where each element has a priority.
    - Elements are served based on priority (highest or lowest), not just the order they arrive.
    - Heaps (min-heap or max-heap) are commonly used to implement priority queues efficiently.
    - Operations: insert (add an element), 
      peek (view the highest/lowest priority element), 
      extract (remove the highest/lowest priority element), 
      change priority.

Summary:
    - Heap is a tree-based structure with heap property.
    - Min-heap: root is the minimum; max-heap: root is the maximum.
    - Priority queue uses heaps to manage elements by priority.

Examples:
.
    Min-Heap Example:
        Consider the following numbers inserted into a min-heap: 5, 3, 8, 1, 2
        The resulting min-heap (as a binary tree) would look like:
                1
              /   \
             2     8
            / \
           5   3
        - The smallest element (1) is at the root.

    Max-Heap Example:
        Insert the numbers 5, 3, 8, 1, 2 into a max-heap:
                8
              /   \
             5     3
            / \
           1   2
        - The largest element (8) is at the root.

    Priority Queue Example (using min-heap):
        Suppose we have tasks with priorities: (Task A, 4), (Task B, 2), (Task C, 5)
        After inserting into a min-heap-based priority queue:
            - The task with the lowest priority value (Task B, 2) will be served first.
        Operations:
            - Insert (Task D, 1): Task D becomes the new root.
            - Extract-min: Removes Task D (priority 1).

    Heap as Array Representation:
        For a heap:        10
                         /    \
                        15     30
                       /  \
                      40  50
        Array: [10, 15, 30, 40, 50]
        - For node at index i:
            - Left child: 2i + 1
            - Right child: 2i + 2
            - Parent: floor((i - 1) / 2)
//4/7/25//
>>>Hash table:
...it is a datastructure that stores key-value pairs.
...uses hash function to compute an index into an array of buckets or slots, from which the desired value can be found.
operations:
...insert,delete,search(all average O(1) time complexity)
...collisions (when two keys hash to the same index) are handled using techniques like chaining(linked list at each bucket) or open addressing (probing for next available slot)

>>>HashMap:
A hash map is a specific implementation of a hash table that maps keys to values in Java.
Hash Table:
    - A hash table is a data structure that stores key-value pairs.
    - It uses a hash function 
      to compute an index (hash code) into an array of buckets or slots, 
      from which the desired value can be found.
    - Operations: Insert, Delete, Search (all average O(1) time complexity).
    - Collisions (when two keys hash to the same index) are 
      handled using techniques like chaining 
      (linked lists at each bucket) or 
      open addressing (probing for next available slot).
    - Hash tables are widely used 
      for implementing associative arrays, 
      database indexing, and caches.

Hash Map:
    - A hash map is a specific implementation of a hash table that maps keys to values.
    - In many programming languages (e.g., Java, C++), "HashMap" is a built-in class.
    - Keys must be unique; values can be duplicated.
    - Not thread-safe by default (e.g., Java's HashMap), but thread-safe variants exist (e.g., ConcurrentHashMap).
    - Provides fast lookups, insertions, and deletions.

Hash Set:
    - A hash set is a data structure that stores unique elements, using a hash table internally.
    - Only stores keys (no associated values).
    - Used to test membership (whether an element exists) efficiently.
    - Operations: Add, Remove, Contains (all average O(1) time complexity).
    - In many languages, "HashSet" is a built-in class (e.g., Java, C#).

Summary:
    - Hash Table: General concept for key-value storage using hashing.
    - Hash Map: Key-value implementation of a hash table.
    - Hash Set: Stores unique elements using a hash table.

Collision Resolution Techniques in Hashing:
    - Collisions occur when two keys hash to the same index in a hash table.
    - Common techniques to handle collisions:
        1. Chaining:
            - Each bucket contains a linked list (or another data structure) of entries.
            - All elements that hash to the same index are stored in the list.
            - Simple to implement; performance degrades if many collisions occur.
        2. Open Addressing:
            - All elements are stored within the hash table array itself.
            - When a collision occurs, the algorithm searches for the next available slot.
            - Methods include:
                a. Linear Probing: Check the next slot sequentially.
                b. Quadratic Probing: Check slots at increasing quadratic intervals.
                c. Double Hashing: Use a second hash function to determine the step size.
        3. Other Techniques:
            - Cuckoo Hashing: Uses multiple hash functions and relocates existing keys.
            - Robin Hood Hashing: Balances probe sequence lengths to minimize variance.
    - The choice of technique affects performance, memory usage, and implementation complexity.

Java Example: Implementing a Simple Hash Table
    Below is a basic implementation of a hash table in Java using chaining for collision resolution:

    ```java
    import java.util.LinkedList;

    class HashTable<K, V> {
        private static class Entry<K, V> {
            K key;
            V value;
            Entry(K key, V value) {
                this.key = key;
                this.value = value;
            }
        }

        private final int SIZE = 16;
        private LinkedList<Entry<K, V>>[] table;

        @SuppressWarnings("unchecked")
        public HashTable() {
            table = new LinkedList[SIZE];
            for (int i = 0; i < SIZE; i++) {
                table[i] = new LinkedList<>();
            }
        }

        private int hash(K key) {
            return Math.abs(key.hashCode()) % SIZE;
        }

        public void put(K key, V value) {
            int index = hash(key);
            for (Entry<K, V> entry : table[index]) {
                if (entry.key.equals(key)) {
                    entry.value = value;
                    return;
                }
            }
            table[index].add(new Entry<>(key, value));
        }

        public V get(K key) {
            int index = hash(key);
            for (Entry<K, V> entry : table[index]) {
                if (entry.key.equals(key)) {
                    return entry.value;
                }
            }
            return null;
        }

        public void remove(K key) {
            int index = hash(key);
            table[index].removeIf(entry -> entry.key.equals(key));
        }
    }

    // Example usage:
    public class Main {
        public static void main(String[] args) {
            HashTable<String, Integer> hashTable = new HashTable<>();
            hashTable.put("apple", 1);
            hashTable.put("banana", 2);
            hashTable.put("orange", 3);

            System.out.println(hashTable.get("banana")); // Output: 2

            hashTable.remove("banana");
            System.out.println(hashTable.get("banana")); // Output: null
        }
    }
    ```

    Explanation:
    - The `HashTable` class uses an array of linked lists to handle collisions (chaining).
    - The `put` method adds or updates key-value pairs.
    - The `get` method retrieves the value for a given key.
    - The `remove` method deletes a key-value pair.
    - The example in `Main` demonstrates basic usage.

In Java, a hash function converts an object (like a String or Integer) 
into an integer value called a hash code. 
This hash code is used to determine 
where to store or find the object 
in hash-based data structures 
(like `HashMap` or your `HashTable`).

How it works:
    - Every Java object has a `hashCode()` method (inherited from `Object`).
    - For built-in types (like `String`, `Integer`), 
    Java provides efficient `hashCode()` implementations.
    - The hash code is usually further processed 
    (e.g., using modulo with the array size) 
    to find the correct index in the underlying array.

Example:
    ```java
    String key = "apple";
    int hash = key.hashCode(); // Generates a hash code for "apple"
    int index = Math.abs(hash) % array.length; // Maps hash to a valid array index
    ```

Key points:
    - Good hash functions distribute keys evenly to minimize collisions.

===



//5/7/25//
Dynamic programming:
It is a problem solving technique used to solve complex problems by breaking them down into simpler subproblems and storing the results of these subproblems to avoid redundant calculations.
key concepts:
1.overlapping subproblems:
  The problem can be broken down into smaller subproblems that are reused multiple times.
2.optimal substructure:
  The optimal solution to the problem can be constructed from the optimal solutions of its subproblems.

How Dynamic programming:
-Memorization (top-down approach):
  in this we start solving from 'n';
  To use the memoization (top-down) approach for the Climbing Stairs problem, you use recursion and store results of subproblems in an array (or map) to avoid recomputation
  Store the results of exprensive function calls and return the cached result ,when the same inputs occur again, usually implemented with recursion and a cache (like a dictionary).
-Tabulation (bottom-up approach):
  in this we start solving from '0'th term of n
  In tabulation (bottom-up dynamic programming), you do NOT use recursion.
Instead, you use loops to fill up a table (usually an array) from the base cases up to the desired value.
  Solve all subproblems first, typically using iteration, and store their result in a table(like an array). Build up the solution from the smallest subproblem
  Example: Fibonacci Sequence
    #Naive Recursive Approach (Inefficient)
        ```python
        def fib(n):
            if n <= 1:
                return n
            return fib(n-1) + fib(n-2)
        ```
        This recalculates the same values many times.

    #DP with Memoization
        ```python
        def fib(n, memo={}):
            if n in memo:
                return memo[n]
            if n <= 1:
                return n
            memo[n] = fib(n-1, memo) + fib(n-2, memo)
            return memo[n]
        ```

    #DP with Tabulation
        ```python
        def fib(n):
            if n <= 1:
                return n
            dp = [0, 1]
            for i in range(2, n+1):
                dp.append(dp[i-1] + dp[i-2])
            return dp[n]
        ```

When to Use Dynamic Programming
    - When a problem can be broken into overlapping subproblems.
    - When the problem has optimal substructure.
    - Common in optimization problems (e.g., shortest path, knapsack, coin change).

Gotchas
    - Not all recursive problems benefit from DP—only those with overlapping subproblems.
    - Space complexity can be high if you store all subproblem results; 
    sometimes you can optimize by only keeping necessary states.

Summary
    Dynamic Programming is a powerful technique 
    for optimizing recursive algorithms 
    by storing and reusing subproblem solutions, 
    making it possible to solve complex problems efficiently.

// 6/7/25 sunday //

//7/7/25 //
Trees:
Preorder , postorder and inorder
Traversal:
    - Preorder: Visit root, then left subtree, then right subtree.
    - Inorder: Visit left subtree, then root, then right subtree.
    - Postorder: Visit left subtree, then right subtree, then root.
    - Level Order: Visit nodes level by level, from left to right.
    - Reverse Level Order: Visit nodes level by level, from right to left.
 Example:
 Take a tree and done traversal:
    1                 preorder:1 2 4 5 3 6 7 (Root, Left, Right)
   / \                Inorder:4 2 5 1 3 6 7  (Left, Root, Right)
   2  3               postorder:4 5 2 7 6 3 1 (Left, Right, Root)
  / \  \              levelorder:1 2 3 4 5 6 7  (Top to Bottom, Left to Right)
  4  5  6  
         \
          7 


//8/7/25//
General Tree - Detailed Notes
    Definition:
    - A general tree is a hierarchical data structure in which each node can have zero or more child nodes.
    - Unlike binary trees, there is no restriction on the number of children a node can have.

    Basic Terminology:
    - Node: Fundamental part of the tree containing data.
    - Root: The topmost node in the tree.
    - Parent: A node that has one or more child nodes.
    - Child: A node that descends from another node (its parent).
    - Leaf: A node with no children.
    - Sibling: Nodes that share the same parent.
    - Subtree: A tree formed by a node and its descendants.
    - Level: The distance from the root node (root is at level 0).
    - Height: The length of the longest path from the root to a leaf.

    Properties:
    - There is exactly one root node.
    - Every node (except the root) has exactly one parent.
    - Nodes can have any number of children.
    - The tree is connected and acyclic.

    Representation:
    1. Parent-Child List:
        - Each node stores a list of its children.
    2. First Child/Next Sibling (Left-Child Right-Sibling):
        - Each node has two pointers: one to its first child and one to its next sibling.
    3. Adjacency List:
        - Useful for representing trees as graphs.

    Operations:
    - Traversal:
    - Preorder: Visit node, then recursively visit children.
    - Postorder: Recursively visit children, then visit node.
    - Level-order: Visit nodes level by level.
    - Insertion: Add a child to a node.
    - Deletion: Remove a node and its subtree.
    - Searching: Find a node with a specific value.

    Applications:
    - File system hierarchies (folders and files)
    - Organization charts
    - XML/HTML document object models (DOM)
    - Game trees (AI decision making)
    - Expression trees in compilers

    Advantages:
    - Flexible structure for representing hierarchical relationships.
    - No restriction on the number of children per node.

    Disadvantages:
    - More complex to implement than binary trees.
    - Traversal and manipulation can be less efficient due to variable number of children.

    Example (Parent-Child List):

    Root
    ├── Child1
    │   ├── Grandchild1
    │   └── Grandchild2
    └── Child2
        └── Grandchild3

    References:
    - Data Structures and Algorithms textbooks
    - Wikipedia: https://en.wikipedia.org/wiki/Tree_(data_structure)

Binary Tree - Detailed Notes
    Definition:
    - A binary tree is a hierarchical data structure in which each node has at most two children, referred to as the left child and the right child.

    Basic Terminology:
    - Node: Fundamental part of the tree containing data.
    - Root: The topmost node in the tree.
    - Parent: A node that has one or more child nodes.
    - Child: A node that descends from another node (its parent).
    - Leaf: A node with no children.
    - Sibling: Nodes that share the same parent.
    - Subtree: A tree formed by a node and its descendants.
    - Level: The distance from the root node (root is at level 0).
    - Height: The length of the longest path from the root to a leaf.
    - Depth: The length of the path from the root to a node.

    Properties:
    - Each node has at most two children (left and right).
    - There is exactly one root node.
    - Every node (except the root) has exactly one parent.
    - The maximum number of nodes at level l is 2^l.
    - The maximum number of nodes in a binary tree of height h is 2^(h+1) - 1.

    Types of Binary Trees:
    - Full Binary Tree: Every node has 0 or 2 children.
    - Complete Binary Tree: All levels are completely filled except possibly the last, which is filled from left to right.
    - Perfect Binary Tree: All internal nodes have two children and all leaves are at the same level.
    - Skewed Binary Tree: All nodes have only left or only right child (left-skewed or right-skewed).
    - Balanced Binary Tree: The height of the left and right subtrees of every node differ by at most one.

    Representation:
    1. Linked Representation:
        - Each node contains data, a pointer to the left child, and a pointer to the right child.
    2. Array Representation:
        - Useful for complete binary trees; for node at index i:
            - Left child at 2i + 1
            - Right child at 2i + 2
            - Parent at floor((i - 1) / 2)

    Operations:
    - Traversal:
        - Preorder: Visit node, then left subtree, then right subtree.
        - Inorder: Visit left subtree, then node, then right subtree.
        - Postorder: Visit left subtree, then right subtree, then node.
        - Level-order: Visit nodes level by level.
    - Insertion: Add a node at the appropriate position.
    - Deletion: Remove a node and restructure the tree if necessary.
    - Searching: Find a node with a specific value.

    Applications:
    - Expression trees (arithmetic expressions)
    - Binary search trees (efficient searching and sorting)
    - Heaps (priority queues)
    - Huffman coding trees (data compression)
    - Syntax trees in compilers

    Advantages:
    - Efficient searching, insertion, and deletion (especially in balanced trees).
    - Simple recursive algorithms for traversal and manipulation.

    Disadvantages:
    - Can become unbalanced, leading to degraded performance (O(n) time complexity).
    - Requires careful implementation to maintain balance.

    Example (Linked Representation):

    Root
    ├── LeftChild
    │   ├── LeftLeftGrandchild
    │   └── LeftRightGrandchild
    └── RightChild
        └── RightRightGrandchild

    References:
    - Data Structures and Algorithms textbooks
    - Wikipedia: https://en.wikipedia.org/wiki/Binary_tree

Binary Search Tree (BST) - Detailed Notes
    Definition:
    - A binary search tree is a type of binary tree in which each node contains a key, and satisfies the following properties:
        - The key in each node is greater than all keys in its left subtree.
        - The key in each node is less than all keys in its right subtree.

    Properties:
    - Each node has at most two children (left and right).
    - All keys are unique (no duplicates).
    - Inorder traversal of a BST yields keys in sorted (ascending) order.
    - The left subtree contains only nodes with keys less than the node’s key.
    - The right subtree contains only nodes with keys greater than the node’s key.

    Representation:
    - Typically implemented using linked nodes, where each node contains:
        - Data (key)
        - Pointer to left child
        - Pointer to right child

    Operations:
    - Search:
        - Start at the root and recursively or iteratively move left or right depending on the key.
        - Time complexity: O(h), where h is the height of the tree.
    - Insertion:
        - Insert a new key by traversing the tree and placing it in the correct position to maintain BST property.
        - Time complexity: O(h).
    - Deletion:
        - Remove a node and restructure the tree to maintain BST property.
        - Three cases:
            1. Node is a leaf: Remove it directly.
            2. Node has one child: Replace node with its child.
            3. Node has two children: Replace node with its inorder successor or predecessor.
        - Time complexity: O(h).
    - Traversal:
        - Inorder: Yields sorted order.
        - Preorder, Postorder, Level-order: As in binary trees.

    Applications:
    - Efficient searching, insertion, and deletion of data.
    - Implementing dynamic sets and lookup tables.
    - Used in databases and file systems for indexing.
    - Building associative arrays and symbol tables.

    Advantages:
    - Average-case time complexity for search, insert, and delete is O(log n) if the tree is balanced.
    - Simple implementation and easy to understand.

    Disadvantages:
    - Can become unbalanced (degenerate to a linked list), leading to O(n) time complexity.
    - Requires additional logic or self-balancing variants (e.g., AVL tree, Red-Black tree) to maintain efficiency.

    Example (BST Structure):

    8
    ├── 3
    │   ├── 1
    │   └── 6
    │       ├── 4
    │       └── 7
    └── 10
        └── 14
            └── 13

    References:
    - Data Structures and Algorithms textbooks
    - Wikipedia: https://en.wikipedia.org/wiki/Binary_search_tree

The recursive function in graph is Dfs

//9/7/25//
AVL Tree Notes (Detailed)
  The height starts from '1'
    1. Definition:
        - An AVL tree is a self-balancing binary search tree (BST).
        - Named after inventors Adelson-Velsky and Landis.
        - For every node, the heights of the left and right subtrees differ by at most 1.

    2. Properties:
        - Balance Factor (BF) = height(left subtree) - height(right subtree)
        - For every node: BF ∈ {-1, 0, 1}
        - Ensures O(log n) time complexity for search, insert, and delete.

    3. Rotations:
        - Used to restore balance after insertions or deletions.
        - Four types:
        a) Left Rotation (LL)
        b) Right Rotation (RR)
        c) Left-Right Rotation (LR)
        d) Right-Left Rotation (RL)

    4. Insertion:
        - Insert as in BST.
        - Update heights and balance factors.
        - If imbalance occurs, perform appropriate rotation(s).

    5. Deletion:
        - Delete as in BST.
        - Update heights and balance factors.
        - If imbalance occurs, perform appropriate rotation(s).

    6. Rotations Explained:
        - LL Rotation: Right rotation on unbalanced node.
        - RR Rotation: Left rotation on unbalanced node.
        - LR Rotation: Left rotation on left child, then right rotation on unbalanced node.
        - RL Rotation: Right rotation on right child, then left rotation on unbalanced node.

    7. Complexity:
        - Search: O(log n)
        - Insert: O(log n)
        - Delete: O(log n)

    8. Applications:
        - Databases, file systems, memory management, and any application requiring ordered data with fast lookups.

    9. Advantages:
        - Maintains strict balance, guaranteeing logarithmic height.
        - Faster lookups compared to unbalanced BSTs.

    10. Disadvantages:
        - More rotations and bookkeeping compared to other BSTs (e.g., Red-Black Trees).
        - Slightly more complex implementation.

    References:
    - "Introduction to Algorithms" by Cormen et al.
    - Wikipedia: https://en.wikipedia.org/wiki/AVL_tree

AVL Tree - Operations
    AVL - Left Rotate Steps:
        1. Identify the node (x) to perform left rotation on.
        2. Let y = x.right (the right child of x).
        3. Set x.right = y.left.
        4. Set y.left = x.
        5. Update heights of x and y.
        6. Return y (the new root of the rotated subtree).

        Example (Pseudocode):

        leftRotate(x):
            y = x.right
            x.right = y.left
            y.left = x
            // Update heights
            x.height = 1 + max(height(x.left), height(x.right))
            y.height = 1 + max(height(y.left), height(y.right))
            return y

    AVL - Right Rotate Steps:
        1. Identify the node (y) to perform right rotation on.
        2. Let x = y.left (the left child of y).
        3. Set y.left = x.right.
        4. Set x.right = y.
        5. Update heights of y and x.
        6. Return x (the new root of the rotated subtree).

        Example (Pseudocode):

        rightRotate(y):
            x = y.left
            y.left = x.right
            x.right = y
            // Update heights
            y.height = 1 + max(height(y.left), height(y.right))
            x.height = 1 + max(height(x.left), height(x.right))
            return x

        
    AVL Tree - Insert Steps:
        1. Start at the root node.
        2. Insert the new key as in a standard BST.
        3. Update the height of each ancestor node.
        4. Calculate the balance factor for each ancestor node.
        5. If the balance factor becomes unbalanced (not in {-1, 0, 1}):
            a) Identify the case (LL, RR, LR, RL) based on the structure.
            b) Perform the appropriate rotation(s) to restore balance.
        6. Repeat the process up to the root if necessary.


        Step 5: Handling Unbalanced Nodes in AVL Tree Insertion

        When the balance factor of a node is not in {-1, 0, 1}, the tree is unbalanced. You need to:

        1. Identify the imbalance type:  
        - LL (Left-Left): Insertion happened 
        in the left subtree of the left child.
        - RR (Right-Right): Insertion happened 
        in the right subtree of the right child.
        - LR (Left-Right): Insertion happened 
        in the right subtree of the left child.
        - RL (Right-Left): Insertion happened 
        in the left subtree of the right child.

        2. Apply the appropriate rotation(s):
        - LL Case: Perform a single right rotation.
        - RR Case: Perform a single left rotation.
        - LR Case: Perform a left rotation on the left child, 
        then a right rotation on the unbalanced node.
        - RL Case: Perform a right rotation on the right child, 
        then a left rotation on the unbalanced node.

            Example (Pseudocode):

            ````plaintext
            if balance > 1 and key < node.left.key:
                // LL Case
                rightRotate(node)
            elif balance < -1 and key > node.right.key:
                // RR Case
                leftRotate(node)
            elif balance > 1 and key > node.left.key:
                // LR Case
                leftRotate(node.left)
                rightRotate(node)
            elif balance < -1 and key < node.right.key:
                // RL Case
                rightRotate(node.right)
                leftRotate(node)
            ````

            Summary:  
            - Detect the imbalance type by comparing the inserted key and the structure.
            - Apply the correct rotation(s) to restore AVL balance.

    Additional Explanation: LR Case (Left Triangle)
        The LR case is a left triangle that needs to be straightened:
            1. Left Triangle Pattern:
            ```
                A (BF = +2)
               /
              B (BF = -1)  
               \
                C
            ```
            - This creates a "zigzag" or triangle pattern going left-right
            
            2. Step 1: Left Rotate B (Straighten the triangle)
            ```
                    A
                   /
                  C
                 /
                B
            ```
            - Now we have a straight line going left-left
        
        3. Step 2: Right Rotate A (Balance the line)
            ```
                  C
                 / \
                B   A
            ```
            - Final balanced tree with C as new root

        Key Insight: LR = Left Triangle → Straighten → Balance
            - Triangle: Must be straightened first (left rotate the left child)
            - Line: Then balanced (right rotate the unbalanced node)

    AVL Tree - Delete Steps:
        1. Start at the root node.
        2. Delete the target key as in a standard BST:
            - If the node has no children, simply remove it.
            - If the node has one child, replace it with its child.
            - If the node has two children, 
            replace it with its in-order successor or predecessor, 
            then delete that node.
        3. Update the height of each ancestor node.
        4. Calculate the balance factor for each ancestor node.
        5. If the balance factor becomes unbalanced (not in {-1, 0, 1}):
            a) Identify the case (LL, RR, LR, RL) based on the structure.
            b) Perform the appropriate rotation(s) to restore balance.
        6. Repeat the process up to the root if necessary.

        Step 5: Handling Unbalanced Nodes in AVL Tree Deletion

        When the balance factor of a node is not in {-1, 0, 1}, 
        the tree is unbalanced. You need to:

        1. Identify the imbalance type:
            - LL (Left-Left): Left child’s left subtree is taller.
            - LR (Left-Right): Left child’s right subtree is taller.
            - RR (Right-Right): Right child’s right subtree is taller.
            - RL (Right-Left): Right child’s left subtree is taller.

        2. Apply the appropriate rotation(s):
            - LL Case: Perform a single right rotation.
            - LR Case: Perform a left rotation on the left child, then a right rotation on the unbalanced node.
            - RR Case: Perform a single left rotation.
            - RL Case: Perform a right rotation on the right child, then a left rotation on the unbalanced node.

            Example (Pseudocode):

            ````plaintext
            if balance > 1 and getBalance(node.left) >= 0:
                // LL Case
                rightRotate(node)
            elif balance > 1 and getBalance(node.left) < 0:
                // LR Case
                leftRotate(node.left)
                rightRotate(node)
            elif balance < -1 and getBalance(node.right) <= 0:
                // RR Case
                leftRotate(node)
            elif balance < -1 and getBalance(node.right) > 0:
                // RL Case
                rightRotate(node.right)
                leftRotate(node)
            ````

            Summary:
            - After deletion, update heights and balance factors up the tree.
            - Detect the imbalance type by checking the balance factor of the node and its children.
            - Apply the correct rotation(s) to restore AVL balance.
            
Red-Black Tree Notes (Detailed)
    1. Definition:
        - A Red-Black Tree (RBT) is a self-balancing binary search tree (BST).
        - Each node contains an extra bit for color (red or black).
        - Ensures the tree remains approximately balanced.

    2. Properties (Red-Black Properties):
        - Every node is either red or black.
        - The root is always black.
        - All leaves (NIL nodes) are black.
        - If a node is red, then both its children are black (no two reds in a row).
        - Every path from a node to its descendant NIL nodes contains the same number of black nodes (black-height).

    3. Rotations:
        - Used to maintain balance after insertions and deletions.
        - Two types:
            a) Left Rotation
            b) Right Rotation

    4. Insertion:
        - Insert as in BST, color the new node red.
        - Fix any violations of red-black properties using recoloring and rotations.
        - May require multiple adjustments up the tree.

    5. Deletion:
        - Delete as in BST.
        - If deleting a black node, fix violations using recoloring and rotations.
        - May require multiple adjustments up the tree.

    6. Balancing Explained:
        - Balancing is achieved by enforcing red-black properties after every insert and delete.
        - Rotations and recoloring are used to restore properties.

    7. Complexity:
        - Search: O(log n)
        - Insert: O(log n)
        - Delete: O(log n)

    8. Applications:
        - Used in many libraries and systems (e.g., C++ STL map/set, Java TreeMap/TreeSet).
        - Databases, memory management, and associative containers.

    9. Advantages:
        - Guarantees logarithmic height.
        - Fewer rotations on average compared to AVL trees.
        - Efficient for insertion and deletion-heavy workloads.

    10. Disadvantages:
        - Slightly slower lookups compared to AVL trees due to less strict balancing.
        - More complex than simple BSTs.

    References:
    - "Introduction to Algorithms" by Cormen et al.
    - Wikipedia: https://en.wikipedia.org/wiki/Red%E2%80%93black_tree

Red-Black Tree - Operations
    RBT - Left Rotate Steps:
        1. Identify the node (x) to perform left rotation on.
        2. Let y = x.right (the right child of x).
        3. Set x.right = y.left.
        4. If y.left is not null, set y.left.parent = x.
        5. Set y.parent = x.parent.
        6. If x is the root, set root = y.
        Else if x is a left child, set x.parent.left = y.
        Else set x.parent.right = y.
        7. Set y.left = x.
        8. Set x.parent = y.

        Example (Pseudocode):

        ````plaintext
        leftRotate(x):
            y = x.right
            x.right = y.left
            if y.left != null:
                y.left.parent = x
            y.parent = x.parent
            if x.parent == null:
                root = y
            elif x == x.parent.left:
                x.parent.left = y
            else:
                x.parent.right = y
            y.left = x
            x.parent = y
        ````

    RBT - Right Rotate Steps: 
        1. Identify the node (y) to perform right rotation on.
        2. Let x = y.left (the left child of y).
        3. Set y.left = x.right.
        4. If x.right is not null, set x.right.parent = y.
        5. Set x.parent = y.parent.
        6. If y is the root, set root = x.
        Else if y is a right child, set y.parent.right = x.
        Else set y.parent.left = x.
        7. Set x.right = y.
        8. Set y.parent = x.

        Example (Pseudocode):

        ````plaintext
        rightRotate(y):
            x = y.left
            y.left = x.right
            if x.right != null:
                x.right.parent = y
            x.parent = y.parent
            if y.parent == null:
                root = x
            elif y == y.parent.right:
                y.parent.right = x
            else:
                y.parent.left = x
            x.right = y
            y.parent = x
        ````

    RBT - Insert Steps
        1. Start at the root node.
        2. Insert the new key as in a standard BST.
        3. Color the new node red.
        4. Fix any violations of Red-Black properties:
            a) If the parent is black, insertion is done.
            b) If the parent is red, there is a violation:
                i. If the uncle is red:
                    - Recolor parent and uncle to black.
                    - Recolor grandparent to red.
                    - Move up to the grandparent and repeat.
                ii. If the uncle is black or null:
                    - If the new node is on the "inside" 
                    (left-right or right-left), 
                    rotate to convert to "outside" 
                    (left-left or right-right).
                    - Perform a rotation on the grandparent (right or left, as appropriate).
                    - Swap colors of parent and grandparent.
        5. Ensure the root is always black.

        Example (Pseudocode):

        ````plaintext
        insert(node):
            standard BST insert, color new node red
            while node != root and node.parent.color == RED:
                if node.parent is left child of grandparent:
                    uncle = grandparent.right
                    if uncle and uncle.color == RED:
                        // Case 1: recolor
                        node.parent.color = BLACK
                        uncle.color = BLACK
                        grandparent.color = RED
                        node = grandparent
                    else:
                        if node == node.parent.right:
                            // Case 2: left-rotate parent
                            node = node.parent
                            leftRotate(node)
                        // Case 3: right-rotate grandparent
                        node.parent.color = BLACK
                        grandparent.color = RED
                        rightRotate(grandparent)
                else:
                    // mirror image of above
                    uncle = grandparent.left
                    if uncle and uncle.color == RED:
                        node.parent.color = BLACK
                        uncle.color = BLACK
                        grandparent.color = RED
                        node = grandparent
                    else:
                        if node == node.parent.left:
                            node = node.parent
                            rightRotate(node)
                        node.parent.color = BLACK
                        grandparent.color = RED
                        leftRotate(grandparent)
            root.color = BLACK
        ``` 

    RBT - Delete Steps:
        1. Start at the root node.
        2. Delete the target key as in a standard BST.
        3. If the deleted node or the node that replaces it is red, simply remove it (no violation).
        4. If a black node is deleted or replaced, fix violations of Red-Black properties:
            a) If the replacement node is red, color it black.
            b) If the replacement node is black (or null), perform fix-up:
                i. While the node is not the root and is black:
                    - If the node is a left child:
                        * Let sibling = parent.right
                        * If sibling is red:
                            - Recolor sibling and parent.
                            - Left-rotate parent.
                            - Update sibling.
                        * If both sibling's children are black:
                            - Recolor sibling red.
                            - Move up to parent.
                        * If sibling's right child is black and left child is red:
                            - Recolor sibling and its left child.
                            - Right-rotate sibling.
                            - Update sibling.
                        * If sibling's right child is red:
                            - Recolor sibling with parent's color.
                            - Color parent and sibling's right child black.
                            - Left-rotate parent.
                            - Set node to root.
                    - If the node is a right child: (mirror above)
                ii. Color node black.
        5. Ensure the root is always black.

        Example (Pseudocode):

        ````plaintext
        delete(node):
            standard BST delete
            if deleted node was red or replacement is red:
                // No fix needed
                return
            while node != root and node.color == BLACK:
                if node == node.parent.left:
                    sibling = node.parent.right
                    if sibling.color == RED:
                        sibling.color = BLACK
                        node.parent.color = RED
                        leftRotate(node.parent)
                        sibling = node.parent.right
                    if sibling.left.color == BLACK and sibling.right.color == BLACK:
                        sibling.color = RED
                        node = node.parent
                    else:
                        if sibling.right.color == BLACK:
                            sibling.left.color = BLACK
                            sibling.color = RED
                            rightRotate(sibling)
                            sibling = node.parent.right
                        sibling.color = node.parent.color
                        node.parent.color = BLACK
                        sibling.right.color = BLACK
                        leftRotate(node.parent)
                        node = root
                else:
                    // mirror image of above
                    sibling = node.parent.left
                    if sibling.color == RED:
                        sibling.color = BLACK
                        node.parent.color = RED
                        rightRotate(node.parent)
                        sibling = node.parent.left
                    if sibling.left.color == BLACK and sibling.right.color == BLACK:
                        sibling.color = RED
                        node = node.parent
                    else:
                        if sibling.left.color == BLACK:
                            sibling.right.color = BLACK
                            sibling.color = RED
                            leftRotate(sibling)
                            sibling = node.parent.left
                        sibling.color = node.parent.color
                        node.parent.color = BLACK
                        sibling.left.color = BLACK
                        rightRotate(node.parent)
                        node = root
            node.color = BLACK
        ````
RBT - functions 
       x
      / \
     α   y
        / \
       β   γ
    ==>
       y
      / \
     x   γ
    / \
   α   β
    private void leftRotate(Node x) {
        y = x.right                         
        gama = y.right 
        beta = y.left **
        alpha = x.left 
        parent = x.parent 
        isLeft = (x == parent.left)
        //rotation
        y.left = x
        x.parent = y

        x.right = beta 
        beta.parent = x

        y.parent = parent         
        
        if parent == null: root = y
        else if isLeft: x.parent.left = y
        else: x.parent.right = y 
    }
       y
      / \
     x   γ
    / \
   α   β
   ==>
       x
      / \
     α   y
        / \
       β   γ   
    private void rightRotate(Node y) {
        x = y.left 
        alpha = x.left 
        beta = x.right **
        gama = y.right 
        parent = y.parent 
        isLeft = (y == parent.left)
        //rotaion 
        x.right = y 
        y.parent = x 

        y.left = beta 
        beta.parent = y 

        x.parent = parent 
        if isLeft: parent.left = x 
        else parent.right = x 
    }
        AVL Tree Notes (Detailed)

Trie Notes:
Trie Notes (Detailed)
    1. Definition:
        - A Trie (pronounced "try"), also known as a prefix tree or digital tree, is 
        a tree-like data structure 
        used to store a dynamic set of strings, 
        typically for retrieval by prefix.
        - Each node represents a single character of a string.

    2. Properties:
        - The root node is usually empty and 
        does not correspond to any character.
        - Each edge represents a character.
        - Each path from the root to a node represents 
        a prefix of the stored strings.
        - Nodes may store a flag indicating 
        if they mark the end of a valid word.

    3. Structure:
        - Each node contains:
            * A map or array of child pointers 
            (one for each possible character).
            * A boolean flag (isEndOfWord) 
            to indicate if the node represents the end of a word.

    4. Search Operation:
        - Start at the root.
        - For each character in the search string, 
        follow the corresponding child pointer.
        - If a pointer is missing, 
        the string is not present.
        - If all characters are found and 
        the last node is marked as end of word, 
        the string exists.

        Example (Pseudocode):

        search(root, word):
            node = root
            for char in word:
                if char not in node.children:
                    return False
                node = node.children[char]
            return node.isEndOfWord

    5. Insertion:
        - Start at the root.
        - For each character in the word, create a child node if it does not exist.
        - After the last character, mark the node as end of word.

        Example (Pseudocode):

        insert(root, word):
            node = root
            for char in word:
                if char not in node.children:
                    node.children[char] = new TrieNode()
                node = node.children[char]
            node.isEndOfWord = True

    6. Deletion:
        - Recursively traverse the trie to the end of the word.
        - Unmark the end of word flag.
        - Optionally, remove nodes that are no longer needed (i.e., nodes that are not prefixes of other words).

        Example (Pseudocode):

        delete(node, word, depth=0):
            if depth == len(word):
                if not node.isEndOfWord:
                    return False
                node.isEndOfWord = False
                return len(node.children) == 0
            char = word[depth]
            if char not in node.children:
                return False
            shouldDelete = delete(node.children[char], word, depth+1)
            if shouldDelete:
                del node.children[char]
                return not node.isEndOfWord and len(node.children) == 0
            return False

    7. Complexity:
        - Search: O(L), where L is the length of the word.
        - Insert: O(L)
        - Delete: O(L)
        - Space: O(N * L * A), where N is the number of words, L is average word length, and A is the alphabet size.

    8. Applications:
        - Autocomplete and spell-checking.
        - IP routing (longest prefix matching).
        - Dictionary implementations.
        - Word games and search engines.

    9. Advantages:
        - Fast prefix-based lookups.
        - Efficient for storing large sets of strings with shared prefixes.

    10. Disadvantages:
        - Can use more memory than other data structures (e.g., hash tables) for sparse datasets.
        - Not suitable for non-string or non-prefix-based queries.

    References:
    - "Introduction to Algorithms" by Cormen et al.
    - Wikipedia: https://en.wikipedia.org/wiki/Trie

Example with text diagram:

    Trie Example: Dictionary Words

    Let's insert these words: "CAT", "CAR", "CARD", "CARE", "CAREFUL", "CATS", "DOG"

    Step-by-Step Insertion:

    #1. Insert "CAT":
    ```
        ROOT
        |
        C
        |
        A
        |
        T*
    ```

    #2. Insert "CAR":
    ```
        ROOT
        |
        C
        |
        A
       / \
      T*  R*
    ```

    #3. Insert "CARD":
    ```
        ROOT
        |
        C
        |
        A
       / \
      T*  R*
          |
          D*
    ```

    #4. Insert "CARE":
    ```
        ROOT
        |
        C
        |
        A
       / \
      T*  R*
         / \
        D*  E*
    ```

    #5. Insert "CAREFUL":
    ```
        ROOT
        |
        C
        |
        A
       / \
      T*  R*
         / \
        D*  E*
            |
            F
            |
            U
            |
            L*
    ```

    #6. Insert "CATS":
    ```
        ROOT
         |
         C
         |
         A
        / \
       T*  R*
       |  / \
      S* D* E*
            |
            F
            |
            U
            |
            L*
    ```

    #7. Insert "DOG":
    ```
        ROOT
       /   \
      C     D
      |     |
      A     O
     / \    |
    T*  R*  G*
    |  / \
    S* D* E*
          |
          F
          |
          U
          |
          L*
    ```

    Final Complete Trie Structure:

    ```
        ROOT
       /   \
      C     D
      |     |
      A     O
     / \    |
    T*  R*  G*
    |  / \
    S* D* E*
          |
          F
          |
          U
          |
          L*
    ```

    Legend: `*` = End of word marker (isEndOfWord = true)

    Trie Properties Demonstrated:

    1. Shared Prefixes: "CAR", "CARD", "CARE", "CAREFUL" all share "CAR"
    2. Root is Empty: ROOT node contains no character
    3. Path = Word: Each path from root to `*` represents a complete word
    4. Efficient Storage: Common prefixes stored only once

    Search Examples:

    Search "CAR":
    - ROOT → C → A → R* ✅ (Found, ends with *)

    Search "CARD":
    - ROOT → C → A → R → D* ✅ (Found, ends with *)

    Search "CA":
    - ROOT → C → A ❌ (Not found, no * marker)

    Search "CAMERA":
    - ROOT → C → A → ? ❌ (Path doesn't exist)

    Time Complexity:
    - Search/Insert/Delete: O(L) where L = length of word
    - Space: O(N × L × A) where N = number of words, A = alphabet size

    This Trie efficiently stores 7 words using shared prefixes, demonstrating why it's perfect for autocomplete, spell-checkers, and dictionary implementations!

Trie - Operations
    Trie - Insert Steps:
        1. Start at the root node.
        2. For each character in the word:
            a) If the character does not exist as a child, create a new node.
            b) Move to the child node.
        3. After the last character, mark the node as end of word.

    Trie - Search Steps:
        1. Start at the root node.
        2. For each character in the word:
            a) If the character does not exist as a child, return False.
            b) Move to the child node.
        3. After the last character, return True if the node is marked as end of word.

    Trie - Prefix Search Steps:
        1. Start at the root node.
        2. For each character in the prefix:
            a) If the character does not exist as a child, return False.
            b) Move to the child node.
        3. After the last character, return True (prefix exists).

    Trie - Delete Steps:
        1. Recursively traverse to the end of the word.
        2. Unmark the end of word flag.
        3. Remove nodes that are no longer needed (i.e., not prefixes for other words).

    Summary:
        - Tries are efficient for prefix-based operations and storing dictionaries of words.
        - They support fast insert, search, and prefix queries.
        - Widely used in text processing, autocomplete, and search applications.



B-Tree notes:
B-Tree Notes (Detailed)
    1. Definition:
        - A B-Tree is a self-balancing search tree designed 
        for systems that read and write large blocks of data 
        (e.g., databases, filesystems).
        - Generalizes binary search trees by allowing nodes 
        to have more than two children.
        - Optimized for minimizing disk I/O.

    2. Properties:
        - Each node contains a certain number of keys and children (except leaves).
        - All leaves appear at the same level (tree is balanced).
        - A B-Tree of order t (minimum degree) has:
            * Every node (except root) has at least t-1 keys and at most 2t-1 keys.
            * The root has at least 1 key.
            * Every internal node has at least t children and at most 2t children.
            * Keys within a node are sorted.
            * Children pointers separate the keys into intervals.

    3. Structure:
        - Each node contains:
            * n: number of keys
            * keys[1..n]: sorted array of keys
            * children[0..n]: pointers to child nodes 
            (if not a leaf)
            * leaf: boolean indicating if node is a leaf

    4. Search Operation:
        - Similar to binary search within a node.
        - At each node, perform binary search 
        to find the key or the child to descend into.
        - Time complexity: O(log n)

    5. Insertion:
        - Always insert into a leaf node.
        - If the leaf is full (2t-1 keys), split it:
            * Median key moves up to the parent.
            * Node splits into two nodes with t-1 keys each.
        - If the parent is also full, recursively split up to the root.
        - If the root splits, the tree height increases by 1.

        Example (Pseudocode):

        insert(key):
            if root is full:
                s = new node
                s.leaf = False
                s.children[0] = root
                splitChild(s, 0)
                root = s
            insertNonFull(root, key)

        splitChild(parent, i):
            y = parent.children[i]
            z = new node
            z.leaf = y.leaf
            z.keys = y.keys[t:2t-1]
            if not y.leaf:
                z.children = y.children[t:2t]
            y.keys = y.keys[0:t-1]
            parent.children.insert(i+1, z)
            parent.keys.insert(i, y.keys[t-1])

    6. Deletion:
        - More complex than insertion.
        - If the key is in a leaf, simply remove it.
        - If the key is in an internal node:
            * If the child before or after the key has at least t keys, 
            replace the key with its predecessor or successor and recursively delete.
            * If both children have t-1 keys, merge them and recursively delete.
        - If a child has only t-1 keys, ensure it has at least t keys 
        before descending (by borrowing from siblings or merging).

    7. Complexity:
        - Search: O(log n)
        - Insert: O(log n)
        - Delete: O(log n)
        - Height of B-Tree: O(log_t n), where t is the minimum degree.

    8. Applications:
        - Widely used in databases and filesystems (e.g., MySQL, SQLite, NTFS, ext4).
        - Suitable for storage systems with large blocks/pages.

    9. Advantages:
        - Minimizes disk reads/writes by maximizing branching factor.
        - Maintains balance with minimal restructuring.
        - Efficient for large datasets and external storage.

    10. Disadvantages:
        - More complex implementation than binary search trees.
        - Not as efficient for in-memory data structures with small datasets.

    References:
    - "Introduction to Algorithms" by Cormen et al.
    - Wikipedia: https://en.wikipedia.org/wiki/B-tree

B-Tree - Operations
    B-Tree - Search Steps:
        1. Start at the root node.
        2. For the current node:
            a) Perform binary search among the keys.
            b) If the key is found, return it.
            c) If the node is a leaf and key not found, return null.
            d) Otherwise, descend to the appropriate child and repeat.

        Example (Pseudocode):

        search(node, key):
            i = 0
            while i < node.n and key > node.keys[i]:
                i += 1
            if i < node.n and key == node.keys[i]:
                return (node, i)
            elif node.leaf:
                return null
            else:
                return search(node.children[i], key)

    B-Tree - Insert Steps:
        1. If the root is full, split it and increase the tree height.
        2. Descend to the appropriate child recursively, splitting any full child encountered on the way down.
        3. Insert the key into a non-full leaf node.

        Example (Pseudocode):

        insertNonFull(node, key):
            i = node.n - 1
            if node.leaf:
                node.keys.append(0)
                while i >= 0 and key < node.keys[i]:
                    node.keys[i+1] = node.keys[i]
                    i -= 1
                node.keys[i+1] = key
                node.n += 1
            else:
                while i >= 0 and key < node.keys[i]:
                    i -= 1
                i += 1
                if node.children[i].n == 2*t - 1:
                    splitChild(node, i)
                    if key > node.keys[i]:
                        i += 1
                insertNonFull(node.children[i], key)

    B-Tree - Delete Steps:
        1. Find the key to be deleted.
        2. If the key is in a leaf, remove it.
        3. If the key is in an internal node:
            a) If the child before or after the key has at least t keys, 
            replace the key with its predecessor or successor and recursively delete.
            b) If both children have t-1 keys, merge them and recursively delete.
        4. If descending into a child with t-1 keys, 
        ensure it has at least t keys by borrowing or merging.

        Example (Pseudocode):

        delete(node, key):
            // See Cormen et al. for full details; deletion is complex and involves multiple cases.

    Summary:
        - B-Trees are balanced, multi-way search trees optimized for disk and large block storage.
        - They minimize disk I/O by maximizing the number of keys per node.
        - Used extensively in database and filesystem implementations.

//11/7/25 (Day-17 github)//
# Dijkstra's Algorithm - Complete Study Notes

## Table of Contents
1. [Introduction](#introduction)
2. [Problem Statement](#problem-statement)
3. [Algorithm Overview](#algorithm-overview)
4. [Step-by-Step Process](#step-by-step-process)
5. [Implementation](#implementation)
6. [Time & Space Complexity](#time--space-complexity)
7. [Examples](#examples)
8. [Applications](#applications)
9. [Advantages & Disadvantages](#advantages--disadvantages)
10. [Variations](#variations)
11. [Practice Problems](#practice-problems)

## Introduction
I
**Dijkstra's Algorithm** is a graph search algorithm that finds the shortest path between nodes in a weighted graph. It was conceived by computer scientist Edsger W. Dijkstra in 1956 and published three years later.

### Key Characteristics:
- **Greedy Algorithm**: Makes locally optimal choices at each step
- **Single-Source Shortest Path**: Finds shortest paths from one source to all other vertices
- **Works with**: Non-negative weighted graphs only
- **Graph Type**: Works with both directed and undirected graphs

## Problem Statement

Given a weighted graph and a source vertex, find the shortest distance from the source to all other vertices in the graph.

### Constraints:
- All edge weights must be **non-negative**
- Graph can be directed or undirected
- Graph must be connected (for meaningful results)

## Algorithm Overview

### Core Idea:
1. Start from the source vertex
2. Maintain distances to all vertices (initially infinity except source = 0)
3. Always select the unvisited vertex with minimum distance
4. Update distances of its neighbors if a shorter path is found
5. Mark the vertex as visited
6. Repeat until all vertices are processed

### Data Structures Used:
- **Distance Array**: Stores shortest distance from source to each vertex
- **Visited Array**: Tracks which vertices have been processed
- **Priority Queue/Min-Heap**: Efficiently gets the vertex with minimum distance

## Step-by-Step Process

### Algorithm Steps:

1. **Initialize**:
   - Set distance to source = 0
   - Set distance to all other vertices = ∞
   - Mark all vertices as unvisited
   - Create a priority queue with all vertices

2. **Main Loop**:
   ```
   While there are unvisited vertices:
       a. Select unvisited vertex 'u' with minimum distance
       b. Mark 'u' as visited
       c. For each unvisited neighbor 'v' of 'u':
          - Calculate new_distance = distance[u] + weight(u,v)
          - If new_distance < distance[v]:
              distance[v] = new_distance
              parent[v] = u  (for path reconstruction)
   ```

3. **Result**: Distance array contains shortest distances from source to all vertices

## Implementation

### Python Implementation (Using Priority Queue):

```python
import heapq
from collections import defaultdict

class Graph:
    def __init__(self):
        self.graph = defaultdict(list)
        self.vertices = 0
    
    def add_edge(self, u, v, weight):
        self.graph[u].append((v, weight))
        self.vertices = max(self.vertices, max(u, v) + 1)
    
    def dijkstra(self, source):
        # Initialize distances and visited array
        distances = [float('inf')] * self.vertices
        distances[source] = 0
        visited = [False] * self.vertices
        parent = [-1] * self.vertices
        
        # Priority queue: (distance, vertex)
        pq = [(0, source)]
        
        while pq:
            current_dist, u = heapq.heappop(pq)
            
            # Skip if already visited
            if visited[u]:
                continue
                
            visited[u] = True
            
            # Check all neighbors
            for neighbor, weight in self.graph[u]:
                if not visited[neighbor]:
                    new_distance = current_dist + weight
                    
                    if new_distance < distances[neighbor]:
                        distances[neighbor] = new_distance
                        parent[neighbor] = u
                        heapq.heappush(pq, (new_distance, neighbor))
        
        return distances, parent
    
    def get_shortest_path(self, source, target, parent):
        path = []
        current = target
        
        while current != -1:
            path.append(current)
            current = parent[current]
        
        path.reverse()
        return path if path[0] == source else []

# Example Usage
g = Graph()
g.add_edge(0, 1, 4)
g.add_edge(0, 2, 1)
g.add_edge(2, 1, 2)
g.add_edge(1, 3, 1)
g.add_edge(2, 3, 5)

distances, parent = g.dijkstra(0)
print("Shortest distances from vertex 0:", distances)
print("Shortest path from 0 to 3:", g.get_shortest_path(0, 3, parent))
```

### C++ Implementation:

```cpp
#include <iostream>
#include <vector>
#include <queue>
#include <climits>
using namespace std;

class Graph {
    int V;
    vector<vector<pair<int, int>>> adj;

public:
    Graph(int vertices) : V(vertices) {
        adj.resize(V);
    }
    
    void addEdge(int u, int v, int weight) {
        adj[u].push_back({v, weight});
    }
    
    vector<int> dijkstra(int source) {
        vector<int> dist(V, INT_MAX);
        priority_queue<pair<int, int>, vector<pair<int, int>>, greater<pair<int, int>>> pq;
        
        dist[source] = 0;
        pq.push({0, source});
        
        while (!pq.empty()) {
            int u = pq.top().second;
            pq.pop();
            
            for (auto& edge : adj[u]) {
                int v = edge.first;
                int weight = edge.second;
                
                if (dist[u] + weight < dist[v]) {
                    dist[v] = dist[u] + weight;
                    pq.push({dist[v], v});
                }
            }
        }
        
        return dist;
    }
};
```

## Time & Space Complexity

### Time Complexity:
- **Using Array**: O(V²) where V = number of vertices
- **Using Binary Heap**: O((V + E) log V) where E = number of edges
- **Using Fibonacci Heap**: O(E + V log V)

### Space Complexity:
- **O(V)** for distance array, visited array, and priority queue

### When to use which implementation:
- **Dense graphs** (E ≈ V²): Array implementation is better
- **Sparse graphs** (E << V²): Heap implementation is better

## Examples

### Example 1: Simple Graph

```
Graph:
    0 ----4---- 1
    |          /|
    1        2  |
    |      /    1
    2 ---5----- 3

Source: 0
```

**Step-by-step execution:**

| Step | Current | Distance Array | Visited | Action |
|------|---------|----------------|---------|--------|
| 0    | -       | [0, ∞, ∞, ∞]  | [F,F,F,F] | Initialize |
| 1    | 0       | [0, 4, 1, ∞]  | [T,F,F,F] | Process vertex 0 |
| 2    | 2       | [0, 3, 1, 6]  | [T,F,T,F] | Process vertex 2 |
| 3    | 1       | [0, 3, 1, 4]  | [T,T,T,F] | Process vertex 1 |
| 4    | 3       | [0, 3, 1, 4]  | [T,T,T,T] | Process vertex 3 |

**Final Result:**
- Distance from 0 to 0: 0
- Distance from 0 to 1: 3 (path: 0→2→1)
- Distance from 0 to 2: 1 (path: 0→2)
- Distance from 0 to 3: 4 (path: 0→2→1→3)

### Example 2: Larger Graph

```
       2
   0 -----> 1
   |      /   \
   |     /     \ 4
   6   1        \
   |  /          v
   v/            3
   2 ----3-----> 4
```

**Adjacency representation:**
- 0 → [(1,2), (2,6)]
- 1 → [(2,1), (3,4)]
- 2 → [(4,3)]
- 3 → [(4,1)]

## Applications

### 1. **Network Routing Protocols**
- **OSPF (Open Shortest Path First)**: Used in IP networks
- **IS-IS**: Intermediate System to Intermediate System protocol

### 2. **GPS Navigation Systems**
- Finding shortest route between locations
- Traffic-aware routing

### 3. **Social Networks**
- Finding degrees of separation
- Shortest connection path between users

### 4. **Game Development**
- AI pathfinding for NPCs
- Optimal movement planning

### 5. **Transportation Networks**
- Flight connections with minimum cost
- Public transport route planning

### 6. **Supply Chain Optimization**
- Shortest delivery routes
- Cost minimization in logistics

### 7. **Network Analysis**
- Finding critical nodes
- Analyzing network reliability

## Advantages & Disadvantages

### ✅ Advantages:
1. **Optimal Solution**: Always finds the shortest path
2. **Versatile**: Works with various graph types
3. **Well-studied**: Mature algorithm with many optimizations
4. **Practical**: Widely applicable in real-world scenarios

### ❌ Disadvantages:
1. **Non-negative weights only**: Cannot handle negative edge weights
2. **Single-source**: Needs to run multiple times for all-pairs shortest paths
3. **Memory intensive**: Requires storing all distances
4. **Not suitable for dynamic graphs**: Needs recomputation when graph changes

## Variations

### 1. **Bidirectional Dijkstra**
- Run Dijkstra from both source and target
- Stop when the searches meet
- Often faster for single source-target queries

### 2. **A* Algorithm**
- Uses heuristic function to guide search
- More efficient for specific target finding
- Guarantees optimal path if heuristic is admissible

### 3. **Dijkstra with Path Compression**
- Optimized for repeated queries
- Preprocesses graph for faster subsequent searches

### 4. **Multi-source Dijkstra**
- Starts from multiple sources simultaneously
- Useful for facility location problems

## Important Notes & Tips

### 🚨 Common Pitfalls:
1. **Negative weights**: Dijkstra fails with negative edge weights (use Bellman-Ford instead)
2. **Infinite loops**: Can occur if implementation doesn't properly handle visited vertices
3. **Integer overflow**: Use appropriate data types for large weights
4. **Zero-weight cycles**: While allowed, need careful handling

### 💡 Optimization Tips:
1. **Early termination**: Stop when target vertex is reached (for single target)
2. **Preprocessing**: Sort adjacency lists by weight for better cache performance
3. **Data structure choice**: Use Fibonacci heap for very sparse graphs
4. **Memory optimization**: Use bit arrays for visited flags

### 🔍 Debugging Tips:
1. Print distance array at each step
2. Verify graph representation
3. Check for negative weights
4. Ensure proper priority queue usage

## Practice Problems

### Beginner Level:
1. **Shortest Path in Binary Maze** (LeetCode 1091)
2. **Network Delay Time** (LeetCode 743)
3. **Cheapest Flights Within K Stops** (LeetCode 787)

### Intermediate Level:
1. **Path with Maximum Probability** (LeetCode 1514)
2. **Swim in Rising Water** (LeetCode 778)
3. **Minimum Cost to Make Array Non-decreasing** (Custom)

### Advanced Level:
1. **Shortest Path Visiting All Nodes** (LeetCode 847)
2. **Minimum Cost to Reach Destination in Time** (LeetCode 1928)
3. **Find the City With the Smallest Number of Neighbors** (LeetCode 1334)

## Related Algorithms

### Comparison with other shortest path algorithms:

| Algorithm | Use Case | Time Complexity | Handles Negative Weights |
|-----------|----------|----------------|-------------------------|
| Dijkstra | Single-source, non-negative weights | O(E + V log V) | ❌ |
| Bellman-Ford | Single-source, any weights | O(VE) | ✅ |
| Floyd-Warshall | All-pairs shortest paths | O(V³) | ✅ |
| A* | Single target with heuristic | O(E) | ❌ |

## Summary

Dijkstra's algorithm is a fundamental and highly practical algorithm for finding shortest paths in weighted graphs. Its greedy approach and optimal substructure make it both efficient and reliable for graphs with non-negative weights. Understanding this algorithm is crucial for:

- **Competitive Programming**: Many problems involve shortest path calculations
- **System Design**: Network routing and optimization problems
- **Interview Preparation**: Commonly asked in technical interviews
- **Real-world Applications**: GPS, networking, game development

Master the implementation, understand the complexity trade-offs, and practice with various problem types to become proficient with this essential algorithm.

---

*Remember: Practice implementing Dijkstra's algorithm multiple times until you can write it from memory. Understanding the intuition behind the greedy choice is as important as memorizing the code.*

# Bellman-Ford Algorithm - Complete Study Notes

## Table of Contents
1. [Introduction](#introduction)
2. [Problem Statement](#problem-statement)
3. [Algorithm Overview](#algorithm-overview)
4. [Step-by-Step Process](#step-by-step-process)
5. [Implementation](#implementation)
6. [Time & Space Complexity](#time--space-complexity)
7. [Examples](#examples)
8. [Negative Cycle Detection](#negative-cycle-detection)
9. [Applications](#applications)
10. [Advantages & Disadvantages](#advantages--disadvantages)
11. [Comparison with Dijkstra](#comparison-with-dijkstra)
12. [Variations](#variations)
13. [Practice Problems](#practice-problems)

## Introduction

**Bellman-Ford Algorithm** is a single-source shortest path algorithm that can handle graphs with negative edge weights. It was developed by Richard Bellman and Lester Ford Jr. in the 1950s.

### Key Characteristics:
- **Dynamic Programming**: Uses optimal substructure and overlapping subproblems
- **Single-Source Shortest Path**: Finds shortest paths from one source to all other vertices
- **Handles Negative Weights**: Works with negative edge weights (unlike Dijkstra)
- **Negative Cycle Detection**: Can detect negative cycles in the graph
- **Graph Type**: Works with both directed and undirected graphs

## Problem Statement

Given a weighted graph and a source vertex, find the shortest distance from the source to all other vertices. Additionally, detect if there are any negative cycles reachable from the source.

### Constraints:
- Graph can have **negative edge weights**
- Graph can be directed or undirected
- Must detect negative cycles (paths where total weight < 0)

## Algorithm Overview

### Core Idea:
1. Initialize distances to all vertices as infinity (except source = 0)
2. **Relax all edges** repeatedly for (V-1) iterations
3. Check for negative cycles by doing one more iteration
4. If any distance can still be improved, there's a negative cycle

### Key Concept - Edge Relaxation:
For edge (u, v) with weight w:
```
if distance[u] + w < distance[v]:
    distance[v] = distance[u] + w
    parent[v] = u
```

### Why (V-1) iterations?
- Shortest path can have at most (V-1) edges
- Each iteration can extend the shortest path by one edge
- After (V-1) iterations, all shortest paths are found

## Step-by-Step Process

### Algorithm Steps:

1. **Initialize**:
   - Set distance to source = 0
   - Set distance to all other vertices = ∞
   - Set parent of all vertices = -1

2. **Main Loop** (Repeat V-1 times):
   ```
   For i = 1 to V-1:
       For each edge (u, v) with weight w:
           if distance[u] ≠ ∞ AND distance[u] + w < distance[v]:
               distance[v] = distance[u] + w
               parent[v] = u
   ```

3. **Negative Cycle Check**:
   ```
   For each edge (u, v) with weight w:
       if distance[u] ≠ ∞ AND distance[u] + w < distance[v]:
           return "Negative cycle detected"
   ```

4. **Result**: Distance array contains shortest distances (or negative cycle detected)

## Implementation

### Python Implementation:

```python
class Graph:
    def __init__(self, vertices):
        self.V = vertices
        self.edges = []
    
    def add_edge(self, u, v, weight):
        self.edges.append((u, v, weight))
    
    def bellman_ford(self, source):
        # Step 1: Initialize distances
        distances = [float('inf')] * self.V
        distances[source] = 0
        parent = [-1] * self.V
        
        # Step 2: Relax all edges V-1 times
        for i in range(self.V - 1):
            # Flag to check if any distance was updated
            updated = False
            
            for u, v, weight in self.edges:
                if distances[u] != float('inf') and distances[u] + weight < distances[v]:
                    distances[v] = distances[u] + weight
                    parent[v] = u
                    updated = True
            
            # Early termination optimization
            if not updated:
                break
        
        # Step 3: Check for negative cycles
        for u, v, weight in self.edges:
            if distances[u] != float('inf') and distances[u] + weight < distances[v]:
                return None, None, "Negative cycle detected"
        
        return distances, parent, "No negative cycle"
    
    def get_shortest_path(self, source, target, parent):
        if parent[target] == -1 and target != source:
            return []
        
        path = []
        current = target
        
        while current != -1:
            path.append(current)
            current = parent[current]
        
        path.reverse()
        return path
    
    def print_solution(self, distances, parent, source):
        print(f"Shortest distances from vertex {source}:")
        for i in range(self.V):
            if distances[i] == float('inf'):
                print(f"Vertex {i}: No path")
            else:
                path = self.get_shortest_path(source, i, parent)
                print(f"Vertex {i}: Distance = {distances[i]}, Path = {' -> '.join(map(str, path))}")

# Example Usage
g = Graph(5)
g.add_edge(0, 1, -1)
g.add_edge(0, 2, 4)
g.add_edge(1, 2, 3)
g.add_edge(1, 3, 2)
g.add_edge(1, 4, 2)
g.add_edge(3, 2, 5)
g.add_edge(3, 1, 1)
g.add_edge(4, 3, -3)

distances, parent, status = g.bellman_ford(0)
if distances:
    g.print_solution(distances, parent, 0)
else:
    print(status)
```

### C++ Implementation:

```cpp
#include <iostream>
#include <vector>
#include <climits>
using namespace std;

struct Edge {
    int u, v, weight;
    Edge(int u, int v, int weight) : u(u), v(v), weight(weight) {}
};

class Graph {
    int V;
    vector<Edge> edges;

public:
    Graph(int vertices) : V(vertices) {}
    
    void addEdge(int u, int v, int weight) {
        edges.push_back(Edge(u, v, weight));
    }
    
    pair<vector<int>, bool> bellmanFord(int source) {
        vector<int> dist(V, INT_MAX);
        dist[source] = 0;
        
        // Relax all edges V-1 times
        for (int i = 0; i < V - 1; i++) {
            for (auto& edge : edges) {
                if (dist[edge.u] != INT_MAX && 
                    dist[edge.u] + edge.weight < dist[edge.v]) {
                    dist[edge.v] = dist[edge.u] + edge.weight;
                }
            }
        }
        
        // Check for negative cycles
        for (auto& edge : edges) {
            if (dist[edge.u] != INT_MAX && 
                dist[edge.u] + edge.weight < dist[edge.v]) {
                return {dist, false}; // Negative cycle detected
            }
        }
        
        return {dist, true}; // No negative cycle
    }
};
```

### Optimized Version with Early Termination:

```python
def bellman_ford_optimized(self, source):
    distances = [float('inf')] * self.V
    distances[source] = 0
    
    for iteration in range(self.V - 1):
        updated = False
        
        for u, v, weight in self.edges:
            if distances[u] != float('inf') and distances[u] + weight < distances[v]:
                distances[v] = distances[u] + weight
                updated = True
        
        # If no updates in this iteration, we're done
        if not updated:
            print(f"Converged after {iteration + 1} iterations")
            break
    
    # Check for negative cycles
    for u, v, weight in self.edges:
        if distances[u] != float('inf') and distances[u] + weight < distances[v]:
            return None, "Negative cycle detected"
    
    return distances, "No negative cycle"
```

## Time & Space Complexity

### Time Complexity:
- **Worst Case**: O(V × E) where V = vertices, E = edges
- **Best Case**: O(E) with early termination optimization
- **Average Case**: O(V × E)

### Space Complexity:
- **O(V)** for distance and parent arrays
- **O(E)** for storing edges

### Comparison:
- **Dense graphs** (E ≈ V²): O(V³)
- **Sparse graphs** (E ≈ V): O(V²)

## Examples

### Example 1: Graph with Negative Edges

```
Graph:
    0 ----(-1)---- 1
    |              |
    4             2
    |           /   |
    2 ---(3)--- 2   2
           |        |
           (5)      4
           |      /
           3 --(-3)

Edges: (0,1,-1), (0,2,4), (1,2,3), (1,3,2), (1,4,2), (3,2,5), (3,1,1), (4,3,-3)
Source: 0
```

**Step-by-step execution:**

| Iteration | Edge Relaxed | Distance Array | Updated |
|-----------|--------------|----------------|---------|
| 0         | -            | [0, ∞, ∞, ∞, ∞] | - |
| 1         | (0,1,-1)     | [0, -1, ∞, ∞, ∞] | Yes |
| 1         | (0,2,4)      | [0, -1, 4, ∞, ∞] | Yes |
| 1         | (1,2,3)      | [0, -1, 2, ∞, ∞] | Yes |
| 1         | (1,3,2)      | [0, -1, 2, 1, ∞] | Yes |
| 1         | (1,4,2)      | [0, -1, 2, 1, 1] | Yes |
| 2         | (4,3,-3)     | [0, -1, 2, -2, 1] | Yes |
| 2         | (3,1,1)      | [0, -1, 2, -2, 1] | No |
| 3         | No updates   | [0, -1, 2, -2, 1] | No |

**Final Result:**
- Distance from 0 to 0: 0
- Distance from 0 to 1: -1 (path: 0→1)
- Distance from 0 to 2: 2 (path: 0→1→2)
- Distance from 0 to 3: -2 (path: 0→1→4→3)
- Distance from 0 to 4: 1 (path: 0→1→4)

### Example 2: Graph with Negative Cycle

```
Graph with negative cycle:
    0 ----1---- 1
              / |
            -3  2
           /    |
          2 ----1---- 3

Edges: (0,1,1), (1,2,2), (2,1,-3), (1,3,1), (2,3,1)
```

The cycle 1→2→1 has weight: 2 + (-3) = -1 (negative cycle)

## Negative Cycle Detection

### Why is Negative Cycle Detection Important?

1. **Shortest Path Undefined**: In presence of negative cycles, shortest path is not well-defined
2. **Infinite Improvement**: Distances can be improved indefinitely
3. **Real-world Applications**: Arbitrage detection in finance, finding inconsistencies

### Detection Method:

After V-1 iterations, if we can still relax any edge, there's a negative cycle:

```python
def detect_negative_cycle(self, source):
    distances, _, status = self.bellman_ford(source)
    
    if distances is None:
        return True  # Negative cycle detected
    
    return False

def find_negative_cycle_vertices(self, source):
    distances = [float('inf')] * self.V
    distances[source] = 0
    
    # Run V-1 iterations
    for _ in range(self.V - 1):
        for u, v, weight in self.edges:
            if distances[u] != float('inf') and distances[u] + weight < distances[v]:
                distances[v] = distances[u] + weight
    
    # Find vertices affected by negative cycle
    cycle_vertices = set()
    for u, v, weight in self.edges:
        if distances[u] != float('inf') and distances[u] + weight < distances[v]:
            cycle_vertices.add(v)
    
    return list(cycle_vertices)
```

## Applications

### 1. **Finance - Arbitrage Detection**
- Currency exchange rates
- Finding profitable trading cycles
- Risk assessment in financial markets

### 2. **Network Routing**
- Handling negative costs (discounts, incentives)
- Network optimization with penalties

### 3. **Game Theory**
- Finding optimal strategies with penalties
- Economic modeling

### 4. **Transportation Networks**
- Routes with tolls and subsidies
- Multi-modal transportation optimization

### 5. **Supply Chain Management**
- Cost optimization with rebates
- Inventory management with penalties

### 6. **Social Networks**
- Influence propagation with negative effects
- Trust and reputation systems

## Advantages & Disadvantages

### ✅ Advantages:
1. **Handles Negative Weights**: Can process graphs with negative edge weights
2. **Negative Cycle Detection**: Can identify and report negative cycles
3. **Simplicity**: Easy to understand and implement
4. **Versatility**: Works with any graph structure
5. **Guaranteed Convergence**: Always terminates with correct answer

### ❌ Disadvantages:
1. **Slower than Dijkstra**: O(VE) vs O((V+E)logV) for non-negative weights
2. **Not suitable for large graphs**: Cubic time complexity for dense graphs
3. **Single-source only**: Need multiple runs for all-pairs shortest paths
4. **No early termination guarantee**: May need full V-1 iterations

## Comparison with Dijkstra

| Aspect | Dijkstra | Bellman-Ford |
|--------|----------|--------------|
| **Time Complexity** | O((V+E)logV) | O(VE) |
| **Negative Weights** | ❌ No | ✅ Yes |
| **Negative Cycle Detection** | ❌ No | ✅ Yes |
| **Graph Type** | Weighted, non-negative | Weighted, any weights |
| **Algorithm Type** | Greedy | Dynamic Programming |
| **Memory Usage** | O(V) | O(V) |
| **Use Case** | Fast shortest path | Negative weights, cycle detection |

### When to Use Which:

**Use Dijkstra when:**
- All edge weights are non-negative
- Performance is critical
- Large graphs with many vertices

**Use Bellman-Ford when:**
- Graph has negative edge weights
- Need to detect negative cycles
- Graph is relatively small
- Robustness is more important than speed

## Variations

### 1. **SPFA (Shortest Path Faster Algorithm)**
- Optimization of Bellman-Ford using queue
- Average case: O(E), Worst case: O(VE)
- More efficient in practice

```python
def spfa(self, source):
    distances = [float('inf')] * self.V
    distances[source] = 0
    in_queue = [False] * self.V
    queue = [source]
    in_queue[source] = True
    
    while queue:
        u = queue.pop(0)
        in_queue[u] = False
        
        for v, weight in self.adj[u]:
            if distances[u] + weight < distances[v]:
                distances[v] = distances[u] + weight
                if not in_queue[v]:
                    queue.append(v)
                    in_queue[v] = True
    
    return distances
```

### 2. **Yen's Algorithm**
- For finding K shortest paths
- Combines Dijkstra and path deviation

### 3. **Eppstein's Algorithm**
- K shortest paths in O(E + V log V + K)
- More efficient for multiple path queries

## Important Notes & Tips

### 🚨 Common Pitfalls:
1. **Integer Overflow**: Use appropriate data types for large weights
2. **Unreachable Vertices**: Check for infinity distances
3. **Negative Cycle Handling**: Decide how to handle negative cycles
4. **Graph Representation**: Ensure all edges are properly stored

### 💡 Optimization Tips:
1. **Early Termination**: Stop if no updates in an iteration
2. **Queue-based Approach**: Use SPFA for better average performance
3. **Parallel Processing**: Relax edges in parallel
4. **Memory Optimization**: Use efficient data structures

### 🔍 Debugging Tips:
1. Print distances after each iteration
2. Verify edge list is complete
3. Check for proper initialization
4. Test with known examples

## Practice Problems

### Beginner Level:
1. **Single Source Shortest Path** (Basic implementation)
2. **Detect Negative Cycle** (GeeksforGeeks)
3. **Distance from Source** (HackerRank)

### Intermediate Level:
1. **Cheapest Flights Within K Stops** (LeetCode 787) - Modified Bellman-Ford
2. **Network Delay Time with Negative Edges** (Custom)
3. **Currency Exchange** (UVa 10557)

### Advanced Level:
1. **Time Travel** (Codeforces)
2. **Wormholes** (USACO)
3. **Arbitrage** (Programming Challenges)

## Summary

Bellman-Ford algorithm is essential for handling graphs with negative edge weights and detecting negative cycles. While slower than Dijkstra's algorithm, it provides crucial functionality that Dijkstra cannot handle.

### Key Takeaways:
- **Complementary to Dijkstra**: Use when negative weights are present
- **Reliable**: Always finds correct shortest paths or detects negative cycles
- **Practical Applications**: Finance, economics, network optimization
- **Implementation**: Straightforward dynamic programming approach

### Master Bellman-Ford by:
1. Understanding the relaxation concept
2. Practicing implementation from scratch
3. Solving problems with negative weights
4. Learning to detect and handle negative cycles

---

*Remember: Bellman-Ford is about patience over speed - it methodically explores all possibilities to guarantee correctness even with negative weights.*

# Topological Sorting - Complete Study Notes

## Table of Contents
1. [Introduction](#introduction)
2. [Problem Statement](#problem-statement)
3. [Prerequisites & Concepts](#prerequisites--concepts)
4. [Algorithm Approaches](#algorithm-approaches)
5. [DFS-Based Implementation](#dfs-based-implementation)
6. [Kahn's Algorithm (BFS-Based)](#kahns-algorithm-bfs-based)
7. [Time & Space Complexity](#time--space-complexity)
8. [Examples](#examples)
9. [Applications](#applications)
10. [Cycle Detection](#cycle-detection)
11. [Variations](#variations)
12. [Common Problems](#common-problems)
13. [Practice Problems](#practice-problems)

## Introduction

**Topological Sorting** is a linear ordering of vertices in a Directed Acyclic Graph (DAG) such that for every directed edge (u, v), vertex u comes before vertex v in the ordering.

### Key Characteristics:
- **Only for DAGs**: Works only on Directed Acyclic Graphs
- **Linear Ordering**: Produces a sequence of vertices
- **Dependency Resolution**: Respects all dependency relationships
- **Multiple Solutions**: A DAG can have multiple valid topological sorts
- **Cycle Detection**: Can detect cycles in directed graphs

### Real-World Analogy:
Think of it like **task scheduling** where some tasks must be completed before others:
- Getting dressed: underwear → pants → shoes
- Course prerequisites: Math 101 → Math 201 → Math 301
- Build dependencies: compile libraries → link → execute

## Problem Statement

Given a directed acyclic graph (DAG), find a linear ordering of its vertices such that for every directed edge (u, v), vertex u appears before vertex v in the ordering.

### Constraints:
- Graph must be **directed**
- Graph must be **acyclic** (no cycles)
- If cycles exist, topological sorting is not possible

## Prerequisites & Concepts

### 1. **Directed Acyclic Graph (DAG)**
```
Valid DAG:
A → B → D
↓   ↓
C → E

Invalid (has cycle):
A → B
↑   ↓
D ← C
```

### 2. **In-degree and Out-degree**
- **In-degree**: Number of incoming edges to a vertex
- **Out-degree**: Number of outgoing edges from a vertex

### 3. **Dependency Relationship**
- If there's an edge u → v, then u must come before v
- u is a **prerequisite** for v
- v **depends on** u

## Algorithm Approaches

There are two main approaches for topological sorting:

### 1. **DFS-Based Approach**
- Use Depth-First Search
- Finish times determine the order
- Vertices are added to result in reverse finish order

### 2. **Kahn's Algorithm (BFS-Based)**
- Use in-degree counting
- Process vertices with zero in-degree first
- Remove processed vertices and update in-degrees

## DFS-Based Implementation

### Algorithm Steps:
1. Perform DFS on all unvisited vertices
2. When finishing a vertex (all neighbors processed), add it to the front of result
3. The result will be in topologically sorted order

### Python Implementation:

```python
from collections import defaultdict

class Graph:
    def __init__(self, vertices):
        self.V = vertices
        self.graph = defaultdict(list)
    
    def add_edge(self, u, v):
        self.graph[u].append(v)
    
    def topological_sort_dfs(self):
        visited = [False] * self.V
        stack = []
        
        # Call DFS for all unvisited vertices
        for vertex in range(self.V):
            if not visited[vertex]:
                self._dfs_util(vertex, visited, stack)
        
        return stack[::-1]  # Reverse to get correct order
    
    def _dfs_util(self, vertex, visited, stack):
        visited[vertex] = True
        
        # Visit all neighbors
        for neighbor in self.graph[vertex]:
            if not visited[neighbor]:
                self._dfs_util(neighbor, visited, stack)
        
        # Add to stack after visiting all neighbors
        stack.append(vertex)
    
    def topological_sort_with_cycle_detection(self):
        WHITE, GRAY, BLACK = 0, 1, 2
        color = [WHITE] * self.V
        result = []
        
        def dfs(vertex):
            if color[vertex] == GRAY:
                return False  # Cycle detected
            if color[vertex] == BLACK:
                return True   # Already processed
            
            color[vertex] = GRAY  # Mark as being processed
            
            for neighbor in self.graph[vertex]:
                if not dfs(neighbor):
                    return False
            
            color[vertex] = BLACK  # Mark as completely processed
            result.append(vertex)
            return True
        
        # Process all vertices
        for vertex in range(self.V):
            if color[vertex] == WHITE:
                if not dfs(vertex):
                    return None, "Cycle detected"
        
        return result[::-1], "No cycle"

# Example Usage
g = Graph(6)
g.add_edge(5, 2)
g.add_edge(5, 0)
g.add_edge(4, 0)
g.add_edge(4, 1)
g.add_edge(2, 3)
g.add_edge(3, 1)

print("DFS-based Topological Sort:", g.topological_sort_dfs())
```

### C++ Implementation:

```cpp
#include <iostream>
#include <vector>
#include <stack>
#include <algorithm>
using namespace std;

class Graph {
    int V;
    vector<vector<int>> adj;

public:
    Graph(int vertices) : V(vertices) {
        adj.resize(V);
    }
    
    void addEdge(int u, int v) {
        adj[u].push_back(v);
    }
    
    void dfsUtil(int vertex, vector<bool>& visited, stack<int>& result) {
        visited[vertex] = true;
        
        for (int neighbor : adj[vertex]) {
            if (!visited[neighbor]) {
                dfsUtil(neighbor, visited, result);
            }
        }
        
        result.push(vertex);
    }
    
    vector<int> topologicalSort() {
        stack<int> result;
        vector<bool> visited(V, false);
        
        for (int i = 0; i < V; i++) {
            if (!visited[i]) {
                dfsUtil(i, visited, result);
            }
        }
        
        vector<int> topOrder;
        while (!result.empty()) {
            topOrder.push_back(result.top());
            result.pop();
        }
        
        return topOrder;
    }
};
```

## Kahn's Algorithm (BFS-Based)

### Algorithm Steps:
1. Calculate in-degree for all vertices
2. Add all vertices with in-degree 0 to a queue
3. While queue is not empty:
   - Remove a vertex from queue and add to result
   - For each neighbor, decrease its in-degree by 1
   - If neighbor's in-degree becomes 0, add it to queue
4. If result contains all vertices, return it; otherwise, cycle exists

### Python Implementation:

```python
from collections import deque, defaultdict

class Graph:
    def __init__(self, vertices):
        self.V = vertices
        self.graph = defaultdict(list)
    
    def add_edge(self, u, v):
        self.graph[u].append(v)
    
    def topological_sort_kahn(self):
        # Calculate in-degrees
        in_degree = [0] * self.V
        for vertex in range(self.V):
            for neighbor in self.graph[vertex]:
                in_degree[neighbor] += 1
        
        # Initialize queue with vertices having 0 in-degree
        queue = deque()
        for vertex in range(self.V):
            if in_degree[vertex] == 0:
                queue.append(vertex)
        
        result = []
        
        while queue:
            vertex = queue.popleft()
            result.append(vertex)
            
            # Update in-degrees of neighbors
            for neighbor in self.graph[vertex]:
                in_degree[neighbor] -= 1
                if in_degree[neighbor] == 0:
                    queue.append(neighbor)
        
        # Check for cycles
        if len(result) != self.V:
            return None, "Cycle detected"
        
        return result, "No cycle"
    
    def all_topological_sorts(self):
        in_degree = [0] * self.V
        for vertex in range(self.V):
            for neighbor in self.graph[vertex]:
                in_degree[neighbor] += 1
        
        result = []
        all_sorts = []
        
        def backtrack():
            # Find vertices with 0 in-degree
            available = []
            for vertex in range(self.V):
                if in_degree[vertex] == 0 and vertex not in result:
                    available.append(vertex)
            
            if not available:
                if len(result) == self.V:
                    all_sorts.append(result[:])
                return
            
            for vertex in available:
                result.append(vertex)
                
                # Temporarily reduce in-degrees
                temp_reduced = []
                for neighbor in self.graph[vertex]:
                    in_degree[neighbor] -= 1
                    temp_reduced.append(neighbor)
                
                backtrack()
                
                # Restore in-degrees
                for neighbor in temp_reduced:
                    in_degree[neighbor] += 1
                
                result.pop()
        
        backtrack()
        return all_sorts

# Example Usage
g = Graph(4)
g.add_edge(0, 1)
g.add_edge(0, 2)
g.add_edge(1, 3)
g.add_edge(2, 3)

result, status = g.topological_sort_kahn()
print(f"Kahn's Algorithm: {result}")
print(f"Status: {status}")

all_sorts = g.all_topological_sorts()
print(f"All possible topological sorts: {all_sorts}")
```

## Time & Space Complexity

### DFS-Based Approach:
- **Time Complexity**: O(V + E)
- **Space Complexity**: O(V) for recursion stack and visited array

### Kahn's Algorithm:
- **Time Complexity**: O(V + E)
- **Space Complexity**: O(V) for in-degree array and queue

### All Topological Sorts:
- **Time Complexity**: O(V! × (V + E)) in worst case
- **Space Complexity**: O(V)

## Examples

### Example 1: Course Prerequisites

```
Courses and Prerequisites:
- Course 0: No prerequisites
- Course 1: Requires Course 0
- Course 2: Requires Course 0
- Course 3: Requires Courses 1 and 2

Graph representation:
0 → 1 → 3
↓     ↗
2 ----
```

**Step-by-step Kahn's Algorithm:**

| Step | Queue | Result | In-degrees | Action |
|------|-------|--------|------------|--------|
| 0    | [0]   | []     | [0,1,1,2]  | Initialize |
| 1    | [1,2] | [0]    | [0,0,0,2]  | Process 0 |
| 2    | [2]   | [0,1]  | [0,0,0,1]  | Process 1 |
| 3    | [3]   | [0,1,2]| [0,0,0,0]  | Process 2 |
| 4    | []    | [0,1,2,3]| [0,0,0,0] | Process 3 |

**Valid topological orders:**
- [0, 1, 2, 3]
- [0, 2, 1, 3]

### Example 2: Build Dependencies

```
Build system dependencies:
A.cpp → A.o → program
B.cpp → B.o ↗
C.cpp → C.o ↗

Graph:
A.cpp → A.o ↘
              program
B.cpp → B.o ↗
C.cpp → C.o ↗
```

**Possible build orders:**
- [A.cpp, A.o, B.cpp, B.o, C.cpp, C.o, program]
- [B.cpp, B.o, A.cpp, A.o, C.cpp, C.o, program]
- And many other valid orderings...

### Example 3: Cycle Detection

```
Graph with cycle:
A → B → C
↑       ↓
D ←---- 

This forms cycle: A → B → C → D → A
Topological sorting is NOT possible.
```

## Applications

### 1. **Task Scheduling**
```python
def schedule_tasks(tasks, dependencies):
    g = Graph(len(tasks))
    task_to_id = {task: i for i, task in enumerate(tasks)}
    
    for prereq, task in dependencies:
        g.add_edge(task_to_id[prereq], task_to_id[task])
    
    order, status = g.topological_sort_kahn()
    if order is None:
        return "Cannot schedule - circular dependency"
    
    return [tasks[i] for i in order]

# Example
tasks = ["wake_up", "brush_teeth", "eat_breakfast", "go_to_work"]
deps = [("wake_up", "brush_teeth"), ("brush_teeth", "eat_breakfast"), 
        ("eat_breakfast", "go_to_work")]
print(schedule_tasks(tasks, deps))
```

### 2. **Build Systems**
- **Make**: Determines compilation order
- **Maven/Gradle**: Dependency resolution
- **npm/pip**: Package installation order

### 3. **Course Planning**
```python
def plan_courses(courses, prerequisites):
    # Implementation similar to task scheduling
    # Returns optimal course taking order
    pass
```

### 4. **Database Query Optimization**
- Join order optimization
- Dependency-based query execution

### 5. **Spreadsheet Formula Evaluation**
- Cell dependency resolution
- Circular reference detection

### 6. **Version Control Systems**
- Commit ordering
- Branch merge strategies

## Cycle Detection

### Why Cycles Matter:
- **Deadlock**: Circular dependencies can cause deadlocks
- **Infinite Loops**: Impossible to find a valid ordering
- **Data Consistency**: Ensures acyclic dependencies

### Detection Methods:

#### 1. **Using DFS (3-Color Method)**
```python
def has_cycle_dfs(self):
    WHITE, GRAY, BLACK = 0, 1, 2
    color = [WHITE] * self.V
    
    def dfs(vertex):
        if color[vertex] == GRAY:
            return True  # Back edge found - cycle!
        if color[vertex] == BLACK:
            return False
        
        color[vertex] = GRAY
        for neighbor in self.graph[vertex]:
            if dfs(neighbor):
                return True
        color[vertex] = BLACK
        return False
    
    for vertex in range(self.V):
        if color[vertex] == WHITE:
            if dfs(vertex):
                return True
    return False
```

#### 2. **Using Kahn's Algorithm**
```python
def has_cycle_kahn(self):
    result, _ = self.topological_sort_kahn()
    return result is None  # If None, cycle exists
```

## Variations

### 1. **Lexicographically Smallest Topological Sort**
```python
def lexicographically_smallest_topological_sort(self):
    import heapq
    
    in_degree = [0] * self.V
    for vertex in range(self.V):
        for neighbor in self.graph[vertex]:
            in_degree[neighbor] += 1
    
    # Use min-heap instead of regular queue
    heap = []
    for vertex in range(self.V):
        if in_degree[vertex] == 0:
            heapq.heappush(heap, vertex)
    
    result = []
    while heap:
        vertex = heapq.heappop(heap)
        result.append(vertex)
        
        for neighbor in self.graph[vertex]:
            in_degree[neighbor] -= 1
            if in_degree[neighbor] == 0:
                heapq.heappush(heap, neighbor)
    
    return result if len(result) == self.V else None
```

### 2. **Topological Sort with Parallel Processing**
```python
def parallel_topological_sort(self):
    in_degree = [0] * self.V
    for vertex in range(self.V):
        for neighbor in self.graph[vertex]:
            in_degree[neighbor] += 1
    
    levels = []
    current_level = [v for v in range(self.V) if in_degree[v] == 0]
    
    while current_level:
        levels.append(current_level[:])
        next_level = []
        
        for vertex in current_level:
            for neighbor in self.graph[vertex]:
                in_degree[neighbor] -= 1
                if in_degree[neighbor] == 0:
                    next_level.append(neighbor)
        
        current_level = next_level
    
    return levels
```

## Common Problems

### 1. **Course Schedule (LeetCode 207)**
```python
def can_finish(num_courses, prerequisites):
    g = Graph(num_courses)
    for course, prereq in prerequisites:
        g.add_edge(prereq, course)
    
    _, status = g.topological_sort_kahn()
    return status == "No cycle"
```

### 2. **Course Schedule II (LeetCode 210)**
```python
def find_order(num_courses, prerequisites):
    g = Graph(num_courses)
    for course, prereq in prerequisites:
        g.add_edge(prereq, course)
    
    order, status = g.topological_sort_kahn()
    return order if status == "No cycle" else []
```

### 3. **Alien Dictionary (LeetCode 269)**
```python
def alien_order(words):
    # Build graph from word ordering
    # Use topological sort to find character order
    pass
```

## Practice Problems

### Beginner Level:
1. **Course Schedule** (LeetCode 207)
2. **Course Schedule II** (LeetCode 210)
3. **Minimum Height Trees** (LeetCode 310)

### Intermediate Level:
1. **Alien Dictionary** (LeetCode 269)
2. **Sequence Reconstruction** (LeetCode 444)
3. **Sort Items by Groups Respecting Dependencies** (LeetCode 1203)

### Advanced Level:
1. **Build Binary Expression Tree From Infix Expression** (LeetCode 1597)
2. **Parallel Courses** (LeetCode 1136)
3. **Find All Possible Recipes** (LeetCode 2115)

## Important Notes & Tips

### 🚨 Common Pitfalls:
1. **Cycles in Input**: Always check for cycles before sorting
2. **Multiple Components**: Handle disconnected components
3. **Self-loops**: Consider vertices with edges to themselves
4. **Empty Graph**: Handle edge case of empty graph

### 💡 Optimization Tips:
1. **Early Termination**: Stop when cycle is detected
2. **Batch Processing**: Group independent vertices
3. **Memory Efficiency**: Use bit arrays for large graphs
4. **Parallel Execution**: Process vertices with same level in parallel

### 🔍 Debugging Tips:
1. Print in-degrees at each step
2. Visualize the graph structure
3. Check for missing edges
4. Verify cycle detection logic

## Summary

Topological sorting is a fundamental algorithm for ordering vertices in directed acyclic graphs while respecting dependency relationships.

### Key Takeaways:
- **Two Main Approaches**: DFS-based and Kahn's algorithm (BFS-based)
- **Cycle Detection**: Essential feature for validating DAGs
- **Multiple Solutions**: DAGs can have multiple valid topological orders
- **Practical Applications**: Scheduling, build systems, course planning

### Master Topological Sorting by:
1. Understanding the dependency concept
2. Implementing both DFS and BFS approaches
3. Practicing cycle detection
4. Solving real-world scheduling problems

**Choose the right approach:**
- **DFS**: When you need to detect cycles and get one valid ordering
- **Kahn's**: When you want level-by-level processing or all possible orderings
- **Lexicographic**: When you need the smallest valid ordering

---

*Remember: Topological sorting is all about respecting dependencies - think of it as organizing tasks where some must be done before others.*               

//11/7/25 (day18)//
KMP String Matching Algorithm - Complete Study Notes
Table of Contents
Introduction
Problem Statement
Naive Approach & Its Problems
KMP Algorithm Overview
Failure Function (LPS Array)
Step-by-Step Process
Implementation
Time & Space Complexity
Examples
Applications
Advantages & Disadvantages
Variations
Practice Problems

Introduction
KMP (Knuth-Morris-Pratt) Algorithm is an efficient string matching algorithm that finds occurrences of a pattern string within a text string. It was developed by Donald Knuth, James Morris, and Vaughan Pratt in 1977.

Key Characteristics:
Linear Time Complexity: O(n + m) where n = text length, m = pattern length
Preprocessing: Uses a failure function to avoid redundant comparisons
No Backtracking: Never moves backwards in the text
Optimal: Achieves the theoretical lower bound for string matching
Deterministic: Always produces the same result for given input
Why KMP is Important:
Efficiency: Much faster than naive approach for large texts
Foundation: Basis for many other string algorithms
Practical: Used in text editors, search engines, and compilers
Educational: Teaches important concepts of preprocessing and pattern analysis
Problem Statement
String Matching Problem: Given a text string T of length n and a pattern string P of length m, find all occurrences of P in T.

Input:
Text: T = "ABABDABACDABABCABCABCABCABC"
Pattern: P = "ABABCABCABCABC"
Output:
All starting positions where pattern occurs in text
Example: Pattern found at position 10
Constraints:
Both strings can contain any characters
Pattern length ≤ Text length
Case-sensitive matching (unless specified otherwise)
Naive Approach & Its Problems
Naive Algorithm:
def naive_search(text, pattern):
    n = len(text)
    m = len(pattern)
    positions = []
    
    for i in range(n - m + 1):
        j = 0
        while j < m and text[i + j] == pattern[j]:
            j += 1
        if j == m:
            positions.append(i)
    
    return positions
Problems with Naive Approach:
1. Redundant Comparisons
Text:    ABABDABABCAB
Pattern: ABABCAB
         ↑
         Mismatch at position 4

Naive approach restarts from position 1:
Text:    ABABDABABCAB
Pattern:  ABABCAB
          ↑
          Wastes previous comparison information
2. Time Complexity
Worst Case: O(n × m)
Example: Text = "AAAA...A", Pattern = "AAA...AB"
Problem: Rechecks many characters multiple times
3. Inefficient for Repetitive Patterns
Patterns with repeated substrings cause maximum backtracking
Real-world texts often have repetitive patterns
KMP Algorithm Overview
Core Innovation:
KMP avoids redundant comparisons by preprocessing the pattern to determine how much we can skip when a mismatch occurs.

Key Insights:
1. Use Previous Information
When mismatch occurs, we already know some characters matched
Use this information to determine next comparison position
2. Failure Function (LPS)
LPS: Longest Proper Prefix which is also Suffix
Tells us how much of the pattern can be "reused" after a mismatch
3. No Backtracking in Text
Text pointer never moves backward
Only pattern pointer adjusts based on failure function
Algorithm Phases:
Phase 1: Preprocessing
Compute failure function (LPS array) for the pattern
Time: O(m)
Phase 2: Searching
Use LPS array to efficiently search for pattern in text
Time: O(n)
Total Time: O(n + m)

Failure Function (LPS Array)
Definition:
LPS[i] = Length of the longest proper prefix of pattern[0...i] which is also a suffix of pattern[0...i]

Key Properties:
Proper Prefix: Prefix that is not equal to the string itself
Suffix: Any suffix of the string
LPS[0] = 0 (by definition)
Example:
Pattern: "ABABCAB"
Index:    0123456

LPS Array Construction:
i=0: "A" → LPS[0] = 0 (no proper prefix)
i=1: "AB" → LPS[1] = 0 (no matching prefix-suffix)
i=2: "ABA" → LPS[2] = 1 ("A" is both prefix and suffix)
i=3: "ABAB" → LPS[3] = 2 ("AB" is both prefix and suffix)
i=4: "ABABC" → LPS[4] = 0 (no matching prefix-suffix)
i=5: "ABABCA" → LPS[5] = 1 ("A" is both prefix and suffix)
i=6: "ABABCAB" → LPS[6] = 2 ("AB" is both prefix and suffix)

Final LPS: [0, 0, 1, 2, 0, 1, 2]
LPS Computation Algorithm:
def compute_lps(pattern):
    m = len(pattern)
    lps = [0] * m
    length = 0  # Length of previous longest prefix suffix
    i = 1
    
    while i < m:
        if pattern[i] == pattern[length]:
            length += 1
            lps[i] = length
            i += 1
        else:
            if length != 0:
                # Use previously computed LPS value
                length = lps[length - 1]
            else:
                lps[i] = 0
                i += 1
    
    return lps
Visual Example of LPS:
Pattern: "ABABCAB"
         0123456

For i=6 (character 'B'):
- Current substring: "ABABCAB"
- Proper prefixes: "", "A", "AB", "ABA", "ABAB", "ABABC", "ABABCA"
- Suffixes: "", "B", "AB", "CAB", "BCAB", "ABCAB", "BABCAB", "ABABCAB"
- Matching: "AB" (length 2)
- So LPS[6] = 2
Step-by-Step Process
Complete KMP Algorithm:
Step 1: Compute LPS Array
def compute_lps(pattern):
    m = len(pattern)
    lps = [0] * m
    length = 0
    i = 1
    
    while i < m:
        if pattern[i] == pattern[length]:
            length += 1
            lps[i] = length
            i += 1
        else:
            if length != 0:
                length = lps[length - 1]
            else:
                lps[i] = 0
                i += 1
    return lps
Step 2: Search Using LPS
def kmp_search(text, pattern):
    n = len(text)
    m = len(pattern)
    
    if m == 0:
        return []
    
    # Compute LPS array
    lps = compute_lps(pattern)
    
    result = []
    i = 0  # Index for text
    j = 0  # Index for pattern
    
    while i < n:
        if text[i] == pattern[j]:
            i += 1
            j += 1
        
        if j == m:
            # Found complete match
            result.append(i - j)
            j = lps[j - 1]  # Continue searching
        elif i < n and text[i] != pattern[j]:
            if j != 0:
                j = lps[j - 1]  # Use failure function
            else:
                i += 1  # Move to next character in text
    
    return result
Detailed Example Trace:
Text:    "ABABDABACDABABCABCABCABCABC"
Pattern: "ABABCAB"
LPS:     [0, 0, 1, 2, 0, 1, 2]

Step-by-step matching:
i=0, j=0: A==A ✓ → i=1, j=1
i=1, j=1: B==B ✓ → i=2, j=2
i=2, j=2: A==A ✓ → i=3, j=3
i=3, j=3: B==B ✓ → i=4, j=4
i=4, j=4: D!=C ✗ → j=lps[3]=2, i=4
i=4, j=2: D!=A ✗ → j=lps[1]=0, i=4
i=4, j=0: D!=A ✗ → i=5, j=0
...continue until match found...
Implementation
Complete Python Implementation:
class KMP:
    def __init__(self, pattern):
        self.pattern = pattern
        self.lps = self._compute_lps()
    
    def _compute_lps(self):
        """Compute the Longest Proper Prefix which is also Suffix array"""
        m = len(self.pattern)
        lps = [0] * m
        length = 0
        i = 1
        
        while i < m:
            if self.pattern[i] == self.pattern[length]:
                length += 1
                lps[i] = length
                i += 1
            else:
                if length != 0:
                    length = lps[length - 1]
                else:
                    lps[i] = 0
                    i += 1
        
        return lps
    
    def search(self, text):
        """Find all occurrences of pattern in text"""
        n = len(text)
        m = len(self.pattern)
        
        if m == 0:
            return []
        
        result = []
        i = 0  # Index for text
        j = 0  # Index for pattern
        
        while i < n:
            if text[i] == self.pattern[j]:
                i += 1
                j += 1
            
            if j == m:
                result.append(i - j)
                j = self.lps[j - 1]
            elif i < n and text[i] != self.pattern[j]:
                if j != 0:
                    j = self.lps[j - 1]
                else:
                    i += 1
        
        return result
    
    def search_first(self, text):
        """Find first occurrence of pattern in text"""
        matches = self.search(text)
        return matches[0] if matches else -1
    
    def count_occurrences(self, text):
        """Count total occurrences of pattern in text"""
        return len(self.search(text))
    
    def get_lps(self):
        """Return the LPS array for debugging"""
        return self.lps

# Usage Example
if __name__ == "__main__":
    # Example 1: Basic usage
    text = "ABABDABACDABABCABCABCABCABC"
    pattern = "ABABCAB"
    
    kmp = KMP(pattern)
    print(f"Pattern: {pattern}")
    print(f"LPS Array: {kmp.get_lps()}")
    print(f"Text: {text}")
    
    matches = kmp.search(text)
    print(f"Pattern found at positions: {matches}")
    
    # Example 2: Multiple patterns
    patterns = ["AB", "ABC", "CAB"]
    for p in patterns:
        kmp = KMP(p)
        matches = kmp.search(text)
        print(f"Pattern '{p}' found at: {matches}")
C++ Implementation:
#include <iostream>
#include <vector>
#include <string>
using namespace std;

class KMP {
private:
    string pattern;
    vector<int> lps;
    
    void computeLPS() {
        int m = pattern.length();
        lps.resize(m, 0);
        int len = 0;
        int i = 1;
        
        while (i < m) {
            if (pattern[i] == pattern[len]) {
                len++;
                lps[i] = len;
                i++;
            } else {
                if (len != 0) {
                    len = lps[len - 1];
                } else {
                    lps[i] = 0;
                    i++;
                }
            }
        }
    }

public:
    KMP(const string& pat) : pattern(pat) {
        computeLPS();
    }
    
    vector<int> search(const string& text) {
        vector<int> result;
        int n = text.length();
        int m = pattern.length();
        
        if (m == 0) return result;
        
        int i = 0, j = 0;
        
        while (i < n) {
            if (text[i] == pattern[j]) {
                i++;
                j++;
            }
            
            if (j == m) {
                result.push_back(i - j);
                j = lps[j - 1];
            } else if (i < n && text[i] != pattern[j]) {
                if (j != 0) {
                    j = lps[j - 1];
                } else {
                    i++;
                }
            }
        }
        
        return result;
    }
    
    vector<int> getLPS() const {
        return lps;
    }
};

// Usage
int main() {
    string text = "ABABDABACDABABCABCABCABCABC";
    string pattern = "ABABCAB";
    
    KMP kmp(pattern);
    vector<int> matches = kmp.search(text);
    
    cout << "Pattern found at positions: ";
    for (int pos : matches) {
        cout << pos << " ";
    }
    cout << endl;
    
    return 0;
}
Advanced Implementation with Additional Features:
class AdvancedKMP:
    def __init__(self, pattern, case_sensitive=True):
        self.original_pattern = pattern
        self.pattern = pattern if case_sensitive else pattern.lower()
        self.case_sensitive = case_sensitive
        self.lps = self._compute_lps()
    
    def _compute_lps(self):
        m = len(self.pattern)
        lps = [0] * m
        length = 0
        i = 1
        
        while i < m:
            if self.pattern[i] == self.pattern[length]:
                length += 1
                lps[i] = length
                i += 1
            else:
                if length != 0:
                    length = lps[length - 1]
                else:
                    lps[i] = 0
                    i += 1
        return lps
    
    def search_with_context(self, text, context_length=10):
        """Search with surrounding context for each match"""
        search_text = text if self.case_sensitive else text.lower()
        matches = []
        
        positions = self._basic_search(search_text)
        
        for pos in positions:
            start_context = max(0, pos - context_length)
            end_context = min(len(text), pos + len(self.pattern) + context_length)
            
            context_before = text[start_context:pos]
            matched_text = text[pos:pos + len(self.pattern)]
            context_after = text[pos + len(self.pattern):end_context]
            
            matches.append({
                'position': pos,
                'context_before': context_before,
                'matched_text': matched_text,
                'context_after': context_after,
                'full_context': text[start_context:end_context]
            })
        
        return matches
    
    def _basic_search(self, text):
        n = len(text)
        m = len(self.pattern)
        
        if m == 0:
            return []
        
        result = []
        i = j = 0
        
        while i < n:
            if text[i] == self.pattern[j]:
                i += 1
                j += 1
            
            if j == m:
                result.append(i - j)
                j = self.lps[j - 1]
            elif i < n and text[i] != self.pattern[j]:
                if j != 0:
                    j = self.lps[j - 1]
                else:
                    i += 1
        
        return result
    
    def search_overlapping(self, text):
        """Find all occurrences including overlapping ones"""
        search_text = text if self.case_sensitive else text.lower()
        n = len(search_text)
        m = len(self.pattern)
        
        if m == 0:
            return []
        
        result = []
        
        # Use sliding window approach for overlapping matches
        for i in range(n - m + 1):
            if search_text[i:i + m] == self.pattern:
                result.append(i)
        
        return result
Time & Space Complexity
Time Complexity:
Preprocessing (LPS computation):
Best/Average/Worst Case: O(m)
Explanation: Each character is processed at most twice
Searching Phase:
Best Case: O(n) - when no matches or immediate mismatch
Average Case: O(n) - typical real-world performance
Worst Case: O(n) - even with many partial matches
Overall Time Complexity:
Total: O(n + m)
Comparison with Naive: O(n × m) vs O(n + m)
Space Complexity:
LPS Array: O(m)
Variables: O(1)
Total: O(m)
Complexity Comparison:
Algorithm	Time Complexity	Space Complexity	Notes
Naive	O(n × m)	O(1)	Simple but inefficient
KMP	O(n + m)	O(m)	Optimal time complexity
Rabin-Karp	O(n + m) avg, O(nm) worst	O(1)	Uses hashing
Boyer-Moore	O(n + m) avg, O(nm) worst	O(σ + m)	Good for large alphabets
Examples
Example 1: Detailed Trace
Text: "ABABDABACDABABCABCABCABCABC"
Pattern: "ABABCAB"

Step 1: Compute LPS
Pattern: A B A B C A B
Index:   0 1 2 3 4 5 6
LPS:     0 0 1 2 0 1 2

Step 2: Search Process
i j  Text[i] Pattern[j] Action
0 0  A       A          Match → i=1, j=1
1 1  B       B          Match → i=2, j=2
2 2  A       A          Match → i=3, j=3
3 3  B       B          Match → i=4, j=4
4 4  D       C          Mismatch → j=lps[3]=2
4 2  D       A          Mismatch → j=lps[1]=0
4 0  D       A          Mismatch → i=5
5 0  A       A          Match → i=6, j=1
...continue...

Final matches found at positions: [10]
Example 2: Multiple Occurrences
def demonstrate_kmp():
    # Text with multiple pattern occurrences
    text = "ABCABCABCABC"
    pattern = "ABCABC"
    
    kmp = KMP(pattern)
    print(f"Text: {text}")
    print(f"Pattern: {pattern}")
    print(f"LPS: {kmp.get_lps()}")
    
    matches = kmp.search(text)
    print(f"Matches found at: {matches}")
    
    # Visualize matches
    for i, pos in enumerate(matches):
        print(f"Match {i+1}: Position {pos}")
        print(f"  {text}")
        print(f"  {' ' * pos}{pattern}")

# Output:
# Text: ABCABCABCABC
# Pattern: ABCABC
# LPS: [0, 0, 0, 1, 2, 3]
# Matches found at: [0, 3, 6]
Example 3: Real-World Text Processing
def find_words_in_document():
    document = """
    The quick brown fox jumps over the lazy dog.
    The dog was really lazy, but the fox was quick.
    """
    
    words_to_find = ["the", "quick", "lazy"]
    
    for word in words_to_find:
        kmp = KMP(word)
        matches = kmp.search(document.lower())
        print(f"'{word}' found {len(matches)} times at positions: {matches}")

# Example output:
# 'the' found 4 times at positions: [5, 41, 55, 75]
# 'quick' found 2 times at positions: [9, 81]
# 'lazy' found 2 times at positions: [46, 65]
Applications
1. Text Editors
class TextEditor:
    def __init__(self):
        self.content = ""
    
    def find_and_replace(self, find_text, replace_text):
        kmp = KMP(find_text)
        positions = kmp.search(self.content)
        
        # Replace from end to beginning to maintain positions
        for pos in reversed(positions):
            self.content = (self.content[:pos] + 
                          replace_text + 
                          self.content[pos + len(find_text):])
        
        return len(positions)
2. DNA Sequence Analysis
def find_dna_patterns(dna_sequence, pattern):
    """Find genetic patterns in DNA sequences"""
    kmp = KMP(pattern)
    matches = kmp.search(dna_sequence)
    
    return {
        'pattern': pattern,
        'occurrences': len(matches),
        'positions': matches,
        'percentage': (len(matches) * len(pattern) / len(dna_sequence)) * 100
    }

# Example
dna = "ATCGATCGATCGTAGCTAGCTAGC"
pattern = "ATCG"
result = find_dna_patterns(dna, pattern)
3. Network Security (Intrusion Detection)
class IntrusionDetector:
    def __init__(self):
        self.malicious_patterns = [
            "SELECT * FROM",
            "<script>",
            "../../",
            "eval("
        ]
        self.kmp_objects = [KMP(pattern.lower()) for pattern in self.malicious_patterns]
    
    def scan_payload(self, payload):
        threats = []
        payload_lower = payload.lower()
        
        for i, kmp in enumerate(self.kmp_objects):
            matches = kmp.search(payload_lower)
            if matches:
                threats.append({
                    'pattern': self.malicious_patterns[i],
                    'positions': matches,
                    'threat_level': 'HIGH'
                })
        
        return threats
4. Compiler Design (Lexical Analysis)
class Lexer:
    def __init__(self):
        self.keywords = {
            'if': 'IF',
            'else': 'ELSE',
            'while': 'WHILE',
            'for': 'FOR',
            'return': 'RETURN'
        }
        self.kmp_keywords = {kw: KMP(kw) for kw in self.keywords}
    
    def tokenize(self, source_code):
        tokens = []
        words = source_code.split()
        
        for word in words:
            for keyword, kmp in self.kmp_keywords.items():
                if kmp.search_first(word) == 0 and len(word) == len(keyword):
                    tokens.append(('KEYWORD', self.keywords[keyword]))
                    break
            else:
                tokens.append(('IDENTIFIER', word))
        
        return tokens
5. Data Compression
def find_repeated_substrings(text, min_length=3):
    """Find repeated substrings for compression algorithms"""
    repeated = {}
    
    for i in range(len(text) - min_length + 1):
        for length in range(min_length, len(text) - i + 1):
            pattern = text[i:i + length]
            
            if pattern not in repeated:
                kmp = KMP(pattern)
                matches = kmp.search(text)
                
                if len(matches) > 1:  # Found repetition
                    repeated[pattern] = {
                        'count': len(matches),
                        'positions': matches,
                        'savings': (len(matches) - 1) * length
                    }
    
    return repeated
Advantages & Disadvantages
✅ Advantages:
1. Optimal Time Complexity
O(n + m) is theoretically optimal for string matching
Significant improvement over naive O(n × m)
2. No Backtracking
Text pointer never moves backward
Suitable for streaming data processing
3. Predictable Performance
Consistent O(n + m) in all cases
No worst-case performance degradation
4. Memory Efficient
Only requires O(m) extra space for LPS array
Can be optimized further for space
5. Foundation for Other Algorithms
Basis for many advanced string algorithms
Educational value for understanding pattern matching
❌ Disadvantages:
1. Preprocessing Overhead
Must compute LPS array before searching
Not suitable for single-character searches
2. Space Requirement
Requires O(m) extra space
May be significant for very long patterns
3. Not Always Fastest in Practice
For small patterns, naive approach might be faster
Modern processors favor simple loops
4. Limited to Exact Matching
Doesn't handle approximate matching
No built-in support for wildcards or regex
5. Complex Implementation
More complex than naive approach
Requires understanding of failure function
Variations
1. Multiple Pattern Matching (Aho-Corasick)
class AhoCorasick:
    """Extension of KMP for multiple patterns"""
    def __init__(self, patterns):
        self.patterns = patterns
        self.build_automaton()
    
    def build_automaton(self):
        # Build trie and failure links
        # Implementation details...
        pass
    
    def search_multiple(self, text):
        # Find all patterns simultaneously
        pass
2. Approximate String Matching
def kmp_with_mismatches(text, pattern, max_mismatches):
    """KMP variant allowing limited mismatches"""
    # Implementation using dynamic programming
    # Allow up to k mismatches while maintaining efficiency
    pass
3. Circular String Matching
def kmp_circular(text, pattern):
    """Find pattern in circular string (text wraps around)"""
    extended_text = text + text[:len(pattern)-1]
    kmp = KMP(pattern)
    matches = kmp.search(extended_text)
    
    # Filter matches that would wrap around
    circular_matches = [m for m in matches if m < len(text)]
    return circular_matches
4. Streaming KMP
class StreamingKMP:
    """KMP for streaming data processing"""
    def __init__(self, pattern):
        self.pattern = pattern
        self.lps = self._compute_lps()
        self.j = 0  # Current position in pattern
        self.position = 0  # Current position in text
    
    def process_char(self, char):
        """Process one character from stream"""
        while self.j > 0 and char != self.pattern[self.j]:
            self.j = self.lps[self.j - 1]
        
        if char == self.pattern[self.j]:
            self.j += 1
        
        self.position += 1
        
        if self.j == len(self.pattern):
            match_position = self.position - self.j
            self.j = self.lps[self.j - 1]
            return match_position
        
        return None
Important Notes & Tips
🚨 Common Pitfalls:
1. LPS Computation Errors
# WRONG: Infinite loop
while i < m:
    if pattern[i] == pattern[length]:
        length += 1
        lps[i] = length
        i += 1
    else:
        length = lps[length - 1]  # Missing else clause
2. Off-by-One Errors
# WRONG: Incorrect boundary
if j == m - 1:  # Should be j == m
    result.append(i - j)
3. Case Sensitivity Issues
# Handle case sensitivity explicitly
text = text.lower() if not case_sensitive else text
pattern = pattern.lower() if not case_sensitive else pattern
💡 Optimization Tips:
1. Early Termination
def kmp_search_first(text, pattern):
    """Stop after finding first match"""
    # Modify search to return immediately after first match
    pass
2. Memory Optimization
def kmp_space_optimized(text, pattern):
    """Reduce space complexity for very long patterns"""
    # Use rolling hash or other techniques
    pass
3. Cache LPS Arrays
class KMPCache:
    def __init__(self):
        self.lps_cache = {}
    
    def get_kmp(self, pattern):
        if pattern not in self.lps_cache:
            self.lps_cache[pattern] = KMP(pattern)
        return self.lps_cache[pattern]
🔍 Debugging Tips:
1. Visualize LPS Construction
def debug_lps(pattern):
    """Print step-by-step LPS construction"""
    print(f"Computing LPS for pattern: {pattern}")
    # Add debug prints in LPS computation
2. Trace Search Process
def debug_search(text, pattern):
    """Print detailed search trace"""
    print(f"Searching '{pattern}' in '{text}'")
    # Add debug prints in search function
Practice Problems
Beginner Level:
Implement Basic KMP - Code from scratch
Find First Occurrence (LeetCode 28)
Count Pattern Occurrences - Count all matches
Intermediate Level:
Repeated String Match (LeetCode 686)
Shortest Palindrome (LeetCode 214) - Uses KMP concepts
Periodic String - Find if string is periodic
Advanced Level:
Multiple Pattern Search - Implement Aho-Corasick
Approximate String Matching - Allow k mismatches
Streaming Pattern Detection - Process data streams
Real-World Projects:
Text Editor Search - Implement find/replace
Log File Analyzer - Find patterns in logs
DNA Sequence Matcher - Bioinformatics application
Summary
KMP Algorithm is a cornerstone of string processing, providing optimal time complexity for exact string matching through clever preprocessing and the avoidance of redundant comparisons.

Key Takeaways:
Core Concepts:
Failure Function (LPS): The heart of KMP's efficiency
No Backtracking: Text pointer only moves forward
Preprocessing: Invest O(m) time to save O(n×m) time
When to Use KMP:
Large texts with repeated pattern searches
Streaming data processing
When consistent performance is crucial
Educational purposes for learning string algorithms
Master KMP by:
Understanding LPS array construction deeply
Tracing through examples step-by-step
Implementing from scratch multiple times
Applying to real-world problems
Performance Guidelines:
Use KMP when: Pattern length > 3-4 characters
Consider alternatives when: Single character patterns, very short texts
Optimize for: Repeated searches with same pattern
Remember: KMP trades a small amount of space (O(m)) and preprocessing time for dramatically improved search performance. The key insight is learning from partial matches to avoid redundant work.

# Longest Prefix Suffix (LPS) Algorithm

## Overview
This program implements the Longest Prefix Suffix (LPS) algorithm, which finds the length of the longest proper prefix of a string that is also a suffix of the same string.

## Algorithm Explanation

### What is LPS?
- **Prefix**: A substring that starts from the beginning of the string
- **Suffix**: A substring that ends at the end of the string
- **Proper Prefix/Suffix**: A prefix/suffix that is not equal to the entire string
- **LPS**: The length of the longest prefix that is also a suffix

### Example
For string `"abcab"`:
- Prefixes: `"a"`, `"ab"`, `"abc"`, `"abca"`
- Suffixes: `"b"`, `"ab"`, `"cab"`, `"bcab"`
- Common: `"ab"` (length 2)
- LPS = 2

## Let's Start with a Simpler Example

Before diving into "ababaca", let's understand with a simpler string: **"abab"**

```
String: a b a b
Index:  0 1 2 3
```

**What we're looking for**: Does any prefix match any suffix?

**All possible prefixes of "abab":**
- "a" (position 0)
- "ab" (positions 0-1)  
- "aba" (positions 0-2)

**All possible suffixes of "abab":**
- "b" (position 3)
- "ab" (positions 2-3)
- "bab" (positions 1-3)

**Common ones:** "ab" appears as both prefix and suffix!
- Prefix "ab" = positions 0-1
- Suffix "ab" = positions 2-3

So the LPS length for "abab" is 2.

**Now let's trace "abab":**

1. i=1: Compare 'b' with 'a' → No match → lps[1]=0
2. i=2: Compare 'a' with 'a' → Match! → len=1, lps[2]=1
3. i=3: Compare 'b' with 'b' → Match! → len=2, lps[3]=2

Result: lps = [0, 0, 1, 2]

**Visual representation:**
```
a b a b
↑   ↑ ↑  
|   └─┘  These match!
└─────── This is the prefix "ab"
```

## Algorithm Steps

### 1. Initialize Variables
```java
int n = s.length();
int[] lps = new int[n];  // LPS array to store values
int len = 0;             // Length of previous longest prefix suffix
int i = 1;               // Start from second character
```

### 2. Build LPS Array
The algorithm uses two pointers:
- `len`: Points to the end of the current matching prefix
- `i`: Current position being processed

### 3. Two Cases
1. **Characters Match** (`s.charAt(i) == s.charAt(len)`):
   - Increment `len`
   - Set `lps[i] = len`
   - Move to next character (`i++`)

2. **Characters Don't Match**:
   - If `len != 0`: Use previously computed LPS value (`len = lps[len - 1]`)
   - If `len == 0`: Set `lps[i] = 0` and move to next character

## Code Walkthrough

```java
while (i < n) {
    if (s.charAt(i) == s.charAt(len)) {
        // Case 1: Characters match
        len++;
        lps[i] = len;
        i++;
    } else {
        // Case 2: Characters don't match
        if (len != 0) {
            // Try shorter prefix
            len = lps[len - 1];
        } else {
            // No match possible
            lps[i] = 0;
            i++;
        }
    }
}
```

## Example Trace - Step by Step Understanding

For string `"ababaca"` (indices 0,1,2,3,4,5,6):

```
String:  a b a b a c a
Index:   0 1 2 3 4 5 6
```

**Initial Setup:**
- `lps = [0, 0, 0, 0, 0, 0, 0]` (all zeros initially)
- `len = 0` (length of current matching prefix)
- `i = 1` (we start from index 1, not 0)

**Step-by-Step Execution:**

### Step 1: i=1, char='b'
- **Compare**: s[1]='b' vs s[0]='a' (len=0)
- **Match?** No
- **Action**: Since len=0, set lps[1]=0, move i to 2
- **Result**: lps = [0, 0, 0, 0, 0, 0, 0]

### Step 2: i=2, char='a'
- **Compare**: s[2]='a' vs s[0]='a' (len=0)
- **Match?** Yes! 
- **Action**: len becomes 1, lps[2]=1, move i to 3
- **Meaning**: "a" at position 0 matches "a" at position 2
- **Result**: lps = [0, 0, 1, 0, 0, 0, 0]

### Step 3: i=3, char='b'
- **Compare**: s[3]='b' vs s[1]='b' (len=1)
- **Match?** Yes!
- **Action**: len becomes 2, lps[3]=2, move i to 4
- **Meaning**: "ab" (positions 0-1) matches "ab" (positions 2-3)
- **Result**: lps = [0, 0, 1, 2, 0, 0, 0]

### Step 4: i=4, char='a'
- **Compare**: s[4]='a' vs s[2]='a' (len=2)
- **Match?** Yes!
- **Action**: len becomes 3, lps[4]=3, move i to 5
- **Meaning**: "aba" (positions 0-2) matches "aba" (positions 2-4)
- **Result**: lps = [0, 0, 1, 2, 3, 0, 0]

### Step 5: i=5, char='c' (This is the tricky part!)
- **Compare**: s[5]='c' vs s[3]='b' (len=3)
- **Match?** No
- **Action**: Since len≠0, try a shorter prefix: len = lps[len-1] = lps[2] = 1
- **Now Compare**: s[5]='c' vs s[1]='b' (len=1)
- **Match?** Still No
- **Action**: len = lps[len-1] = lps[0] = 0
- **Now Compare**: s[5]='c' vs s[0]='a' (len=0)
- **Match?** Still No
- **Action**: Since len=0, set lps[5]=0, move i to 6
- **Result**: lps = [0, 0, 1, 2, 3, 0, 0]

### Step 6: i=6, char='a'
- **Compare**: s[6]='a' vs s[0]='a' (len=0)
- **Match?** Yes!
- **Action**: len becomes 1, lps[6]=1, i becomes 7 (end)
- **Meaning**: "a" at position 0 matches "a" at position 6
- **Result**: lps = [0, 0, 1, 2, 3, 0, 1]

**Final Answer**: The longest prefix that is also a suffix has length 1 (the character "a")

## Table Format for Quick Reference

| Step | i | char | len | s[len] | Match? | Action | lps[i] | lps array |
|------|---|------|-----|--------|--------|--------|--------|-----------|
| 1    | 1 | 'b'  | 0   | 'a'    | No     | lps[1]=0, i++ | 0 | [0,0,0,0,0,0,0] |
| 2    | 2 | 'a'  | 0   | 'a'    | Yes    | len++, lps[2]=1, i++ | 1 | [0,0,1,0,0,0,0] |
| 3    | 3 | 'b'  | 1   | 'b'    | Yes    | len++, lps[3]=2, i++ | 2 | [0,0,1,2,0,0,0] |
| 4    | 4 | 'a'  | 2   | 'a'    | Yes    | len++, lps[4]=3, i++ | 3 | [0,0,1,2,3,0,0] |
| 5a   | 5 | 'c'  | 3   | 'b'    | No     | len=lps[2]=1 | - | [0,0,1,2,3,0,0] |
| 5b   | 5 | 'c'  | 1   | 'b'    | No     | len=lps[0]=0 | - | [0,0,1,2,3,0,0] |
| 5c   | 5 | 'c'  | 0   | 'a'    | No     | lps[5]=0, i++ | 0 | [0,0,1,2,3,0,0] |
| 6    | 6 | 'a'  | 0   | 'a'    | Yes    | len++, lps[6]=1, i++ | 1 | [0,0,1,2,3,0,1] |

**Final Result**: lps = [0, 0, 1, 2, 3, 0, 1], Answer = 1

**Note**: Step 5 has three sub-steps (5a, 5b, 5c) because we keep trying shorter prefixes until we find a match or len becomes 0.

## Why This Matters - Visual Understanding

```
String: a b a b a c a
Index:  0 1 2 3 4 5 6
LPS:    0 0 1 2 3 0 1
```

**What each LPS value means:**
- lps[0] = 0: Always 0 (no proper prefix for single character)
- lps[1] = 0: "ab" has no proper prefix that's also a suffix
- lps[2] = 1: "aba" has prefix "a" that's also a suffix
- lps[3] = 2: "abab" has prefix "ab" that's also a suffix
- lps[4] = 3: "ababa" has prefix "aba" that's also a suffix
- lps[5] = 0: "ababac" has no proper prefix that's also a suffix
- lps[6] = 1: "ababaca" has prefix "a" that's also a suffix

## Key Insight: Why We Use lps[len-1] When There's a Mismatch

When we have a mismatch, instead of starting over, we use the LPS value to "jump back" to a position where we might still have a match. This is the genius of the algorithm - it avoids redundant comparisons!

## Applications

1. **String Matching**: Used in KMP (Knuth-Morris-Pratt) algorithm
2. **Pattern Recognition**: Finding repeating patterns in strings
3. **Text Processing**: Efficient substring searching
4. **Data Compression**: Identifying redundant patterns

## Time & Space Complexity

- **Time Complexity**: O(n) where n is the length of the string
- **Space Complexity**: O(n) for the LPS array

## Key Insights

1. The algorithm avoids redundant comparisons by using previously computed LPS values
2. When a mismatch occurs, we don't start from the beginning but use the LPS value to skip characters
3. The LPS array itself follows the LPS property - each position stores the length of the longest proper prefix-suffix for the substring ending at that position

## Input/Output

- **Input**: A string from user input
- **Output**: Length of the longest prefix that is also a suffix

Example:
- Input: `"ababaca"`
- Output: `1` (prefix "a" is also a suffix)


# Rabin-Karp String Matching Algorithm
## Overview
The Rabin-Karp algorithm is a string searching algorithm that uses hashing to find any one of a set of pattern strings in a text. It was developed by Richard M. Karp and Michael O. Rabin in 1987.

## Key Concepts

### 1. Rolling Hash
- Uses a rolling hash function to compute hash values efficiently
- Allows checking multiple substrings without recalculating the entire hash
- Hash value can be updated in O(1) time when sliding the window

### 2. Hash Function
The algorithm typically uses a polynomial rolling hash:
```
hash(s) = (s[0] * p^(n-1) + s[1] * p^(n-2) + ... + s[n-1] * p^0) mod m
```
Where:
- `s` is the string
- `p` is a prime number (base)
- `m` is a large prime number (modulus)
- `n` is the length of the string

## Algorithm Steps

### 1. Preprocessing
1. Calculate hash value of the pattern
2. Calculate hash value of the first window of text (same length as pattern)
3. Precompute the highest power of the base: `h = p^(m-1) mod q`

### 2. Searching
1. Compare hash values of pattern and current window
2. If hash values match:
   - Perform character-by-character comparison (to handle hash collisions)
   - If characters match, pattern found
3. Slide the window by one position
4. Update hash using rolling hash technique
5. Repeat until end of text

## Implementation

### Python Implementation
```python
def rabin_karp(text, pattern, prime=101):
    """
    Rabin-Karp string matching algorithm
    
    Args:
        text: The text to search in
        pattern: The pattern to search for
        prime: Prime number for hashing (default 101)
    
    Returns:
        List of starting indices where pattern is found
    """
    n = len(text)
    m = len(pattern)
    matches = []
    
    if m > n:
        return matches
    
    # Calculate hash values
    pattern_hash = 0
    text_hash = 0
    h = 1
    
    # Calculate h = pow(256, m-1) % prime
    for i in range(m - 1):
        h = (h * 256) % prime
    
    # Calculate initial hash values
    for i in range(m):
        pattern_hash = (256 * pattern_hash + ord(pattern[i])) % prime
        text_hash = (256 * text_hash + ord(text[i])) % prime
    
    # Slide pattern over text
    for i in range(n - m + 1):
        # Check if hash values match
        if pattern_hash == text_hash:
            # Check character by character
            if text[i:i+m] == pattern:
                matches.append(i)
        
        # Calculate hash for next window
        if i < n - m:
            text_hash = (256 * (text_hash - ord(text[i]) * h) + ord(text[i + m])) % prime
            # Handle negative hash values
            if text_hash < 0:
                text_hash += prime
    
    return matches

# Example usage
text = "ABABDABACDABABCABCABCABCABC"
pattern = "ABABCAB"
result = rabin_karp(text, pattern)
print(f"Pattern found at indices: {result}")
```

### C++ Implementation
```cpp
#include <iostream>
#include <vector>
#include <string>
using namespace std;

vector<int> rabinKarp(string text, string pattern, int prime = 101) {
    int n = text.length();
    int m = pattern.length();
    vector<int> matches;
    
    if (m > n) return matches;
    
    int patternHash = 0;
    int textHash = 0;
    int h = 1;
    
    // Calculate h = pow(256, m-1) % prime
    for (int i = 0; i < m - 1; i++) {
        h = (h * 256) % prime;
    }
    
    // Calculate initial hash values
    for (int i = 0; i < m; i++) {
        patternHash = (256 * patternHash + pattern[i]) % prime;
        textHash = (256 * textHash + text[i]) % prime;
    }
    
    // Slide pattern over text
    for (int i = 0; i <= n - m; i++) {
        // Check if hash values match
        if (patternHash == textHash) {
            // Check character by character
            bool match = true;
            for (int j = 0; j < m; j++) {
                if (text[i + j] != pattern[j]) {
                    match = false;
                    break;
                }
            }
            if (match) {
                matches.push_back(i);
            }
        }
        
        // Calculate hash for next window
        if (i < n - m) {
            textHash = (256 * (textHash - text[i] * h) + text[i + m]) % prime;
            if (textHash < 0) {
                textHash += prime;
            }
        }
    }
    
    return matches;
}
```

## Rolling Hash Calculation

### Adding New Character
When sliding the window to the right:
```
new_hash = (old_hash * base + new_char) % mod
```

### Removing Old Character and Adding New
When sliding window (removing leftmost, adding rightmost):
```
new_hash = (base * (old_hash - old_char * pow(base, m-1)) + new_char) % mod
```

### Example
Text: "ABCDEF", Pattern: "CDE"
- Window 1: "ABC" → hash₁
- Window 2: "BCD" → hash₂ = (hash₁ - A×base²) × base + D
- Window 3: "CDE" → hash₃ = (hash₂ - B×base²) × base + E

## Time Complexity

### Best Case: O(n + m)
- When there are few or no hash collisions
- Hash comparisons are O(1)
- Character verification is minimal

### Average Case: O(n + m)
- Expected time with good hash function
- Few false positives due to hash collisions

### Worst Case: O(nm)
- When hash function produces many collisions
- Every window has same hash as pattern
- Requires character-by-character comparison for each window

## Space Complexity: O(1)
- Only stores hash values and few variables
- No additional data structures needed

## Advantages

1. **Efficient for Multiple Patterns**: Can search for multiple patterns simultaneously
2. **Good Average Performance**: O(n + m) expected time
3. **Simple Implementation**: Straightforward to code and understand
4. **Rolling Hash**: Efficient hash updates in O(1) time

## Disadvantages

1. **Hash Collisions**: May produce false positives requiring verification
2. **Worst Case Performance**: Can degrade to O(nm) in pathological cases
3. **Modular Arithmetic**: Requires careful handling of overflow and negative values
4. **Hash Function Dependency**: Performance depends on quality of hash function

## Applications

1. **Text Processing**: Document search, word processors
2. **Bioinformatics**: DNA sequence matching
3. **Plagiarism Detection**: Finding copied text segments
4. **Data Deduplication**: Identifying duplicate content
5. **Network Security**: Pattern matching in intrusion detection systems

## Optimization Techniques

### 1. Multiple Hash Functions
- Use multiple hash functions to reduce false positives
- Higher probability of detecting mismatches

### 2. Better Hash Functions
- Use stronger hash functions like SHA-1 (though slower)
- Polynomial hashing with larger primes

### 3. Early Termination
- Stop character comparison on first mismatch
- Use bloom filters for preliminary filtering

## Comparison with Other Algorithms

| Algorithm | Best Case | Average Case | Worst Case | Space |
|-----------|-----------|--------------|------------|-------|
| Rabin-Karp | O(n+m) | O(n+m) | O(nm) | O(1) |
| KMP | O(n+m) | O(n+m) | O(n+m) | O(m) |
| Naive | O(n+m) | O(nm) | O(nm) | O(1) |
| Boyer-Moore | O(n/m) | O(n) | O(nm) | O(σ) |

## Practice Problems

1. Find all occurrences of a pattern in text
2. Search for multiple patterns simultaneously
3. Case-insensitive string matching
4. Find longest repeated substring
5. Detect plagiarism in documents

## Interview Questions

1. Explain how rolling hash works in Rabin-Karp
2. What happens when hash collision occurs?
3. How to handle negative hash values?
4. Compare Rabin-Karp with KMP algorithm
5. Optimize Rabin-Karp for multiple pattern search

## Key Points to Remember

- Hash collisions require character-by-character verification
- Rolling hash enables O(1) window updates
- Choose prime numbers carefully for hash function
- Handle modular arithmetic overflow properly
- Good for multiple pattern search scenarios

# Z Algorithm for String Matching

## Overview
The Z Algorithm is a linear time string matching algorithm that finds all occurrences of a pattern in a text in O(n + m) time. It was developed by Gusfield and is based on the Z array concept, which stores the length of the longest substring starting from each position that matches a prefix of the string.

## Key Concepts

### 1. Z Array
The Z array Z[i] for a string S stores the length of the longest substring starting from S[i] which is also a prefix of S.

**Example:**
- String: "ababaca"
- Z array: [0, 0, 3, 0, 1, 0, 1]

### 2. Z Box
A Z-box is a substring S[l...r] which is also a prefix of S. We maintain:
- `l`: leftmost position of current Z-box
- `r`: rightmost position of current Z-box

### 3. Pattern Matching Strategy
To find pattern P in text T:
1. Create concatenated string: S = P + "$" + T (where "$" is a separator)
2. Compute Z array for S
3. Positions where Z[i] = |P| indicate pattern matches in T

## Algorithm Steps

### 1. Z Array Construction
```
1. Initialize Z[0] = 0, l = 0, r = 0
2. For i = 1 to n-1:
   a. If i > r:
      - Compare characters starting from position i with prefix
      - Update Z[i], l, and r accordingly
   b. Else (i <= r):
      - k = i - l
      - If Z[k] < r - i + 1:
        - Z[i] = Z[k]
      - Else:
        - Start comparison from position r+1
        - Update Z[i], l, and r
```

### 2. Pattern Matching
```
1. Create string S = Pattern + "$" + Text
2. Compute Z array for S
3. Scan Z array from position |Pattern| + 1
4. If Z[i] = |Pattern|, then pattern occurs at position i - |Pattern| - 1 in text
```

## Implementation

### Python Implementation
```python
def z_algorithm(s):
    """
    Compute Z array for string s
    Z[i] = length of longest substring starting from s[i] which is also prefix of s
    """
    n = len(s)
    z = [0] * n
    l, r = 0, 0
    
    for i in range(1, n):
        if i > r:
            # Case 1: i is outside current Z-box
            l, r = i, i
            while r < n and s[r - l] == s[r]:
                r += 1
            z[i] = r - l
            r -= 1
        else:
            # Case 2: i is inside current Z-box
            k = i - l
            if z[k] < r - i + 1:
                # Case 2a: Z[k] is completely inside Z-box
                z[i] = z[k]
            else:
                # Case 2b: Z[k] extends beyond Z-box
                l = i
                while r < n and s[r - l] == s[r]:
                    r += 1
                z[i] = r - l
                r -= 1
    
    return z

def z_pattern_search(text, pattern):
    """
    Find all occurrences of pattern in text using Z algorithm
    """
    if not pattern or not text:
        return []
    
    # Create concatenated string
    concat = pattern + "$" + text
    z = z_algorithm(concat)
    
    matches = []
    pattern_len = len(pattern)
    
    # Check for matches
    for i in range(pattern_len + 1, len(concat)):
        if z[i] == pattern_len:
            # Pattern found at position i - pattern_len - 1 in original text
            matches.append(i - pattern_len - 1)
    
    return matches

# Example usage
text = "ABABDABACDABABCABCABCABCABC"
pattern = "ABABCAB"
matches = z_pattern_search(text, pattern)
print(f"Pattern found at indices: {matches}")

# Example of Z array computation
s = "ababaca"
z_arr = z_algorithm(s)
print(f"String: {s}")
print(f"Z array: {z_arr}")
```

### C++ Implementation
```cpp
#include <iostream>
#include <vector>
#include <string>
using namespace std;

vector<int> zAlgorithm(string s) {
    int n = s.length();
    vector<int> z(n, 0);
    int l = 0, r = 0;
    
    for (int i = 1; i < n; i++) {
        if (i > r) {
            // Case 1: i is outside current Z-box
            l = r = i;
            while (r < n && s[r - l] == s[r]) {
                r++;
            }
            z[i] = r - l;
            r--;
        } else {
            // Case 2: i is inside current Z-box
            int k = i - l;
            if (z[k] < r - i + 1) {
                // Case 2a: Z[k] is completely inside Z-box
                z[i] = z[k];
            } else {
                // Case 2b: Z[k] extends beyond Z-box
                l = i;
                while (r < n && s[r - l] == s[r]) {
                    r++;
                }
                z[i] = r - l;
                r--;
            }
        }
    }
    
    return z;
}

vector<int> zPatternSearch(string text, string pattern) {
    vector<int> matches;
    if (pattern.empty() || text.empty()) {
        return matches;
    }
    
    // Create concatenated string
    string concat = pattern + "$" + text;
    vector<int> z = zAlgorithm(concat);
    
    int patternLen = pattern.length();
    
    // Check for matches
    for (int i = patternLen + 1; i < concat.length(); i++) {
        if (z[i] == patternLen) {
            matches.push_back(i - patternLen - 1);
        }
    }
    
    return matches;
}

int main() {
    string text = "ABABDABACDABABCABCABCABCABC";
    string pattern = "ABABCAB";
    
    vector<int> matches = zPatternSearch(text, pattern);
    
    cout << "Pattern found at indices: ";
    for (int pos : matches) {
        cout << pos << " ";
    }
    cout << endl;
    
    return 0;
}
```

## Step-by-Step Example

### Computing Z Array for "ababaca"
```
String: a b a b a c a
Index:  0 1 2 3 4 5 6
Z array:[0 0 3 0 1 0 1]
```

**Step-by-step calculation:**
1. **i=0**: Z[0] = 0 (by definition)
2. **i=1**: Compare s[1] with s[0]: 'b' ≠ 'a', so Z[1] = 0
3. **i=2**: Compare s[2] with s[0]: 'a' = 'a', continue comparing...
   - s[3] = 'b' = s[1] = 'b' ✓
   - s[4] = 'a' = s[2] = 'a' ✓
   - s[5] = 'c' ≠ s[3] = 'b' ✗
   - So Z[2] = 3, l = 2, r = 4
4. **i=3**: i ≤ r, k = 3-2 = 1, Z[k] = Z[1] = 0, Z[3] = 0
5. **i=4**: i = r, k = 4-2 = 2, Z[k] = Z[2] = 3 > r-i+1 = 1
   - Extend comparison from r+1: s[5] = 'c' ≠ s[1] = 'b'
   - So Z[4] = 1, l = 4, r = 4
6. **i=5**: i > r, compare s[5] with s[0]: 'c' ≠ 'a', so Z[5] = 0
7. **i=6**: i > r, compare s[6] with s[0]: 'a' = 'a', but s[7] doesn't exist
   - So Z[6] = 1

### Pattern Matching Example
**Text**: "abababa", **Pattern**: "aba"

1. **Concatenated string**: "aba$abababa"
2. **Z array**: [0, 0, 1, 0, 1, 0, 3, 0, 1]
3. **Analysis**:
   - Position 6: Z[6] = 3 = |pattern|, so match at position 6-3-1 = 2
   - Positions 4,8: Z[4] = Z[8] = 1 ≠ 3, so no match

## Time Complexity Analysis

### Time Complexity: O(n)
- Each character is compared at most twice
- The algorithm maintains that r never decreases except when starting a new Z-box
- Total comparisons ≤ 2n

### Space Complexity: O(n)
- Z array requires O(n) space
- For pattern matching: O(n + m) for concatenated string

## Advantages

1. **Linear Time**: Guaranteed O(n + m) time complexity
2. **Simple Logic**: Easier to understand than KMP
3. **No Preprocessing**: Pattern preprocessing is integrated into the algorithm
4. **Versatile**: Can solve many string problems beyond pattern matching
5. **Optimal**: Achieves theoretical lower bound for string matching

## Disadvantages

1. **Space Usage**: Requires O(n + m) extra space for concatenated string
2. **String Concatenation**: Need to create a new string
3. **Separator Dependency**: Requires a character not in alphabet
4. **Cache Performance**: May have worse cache locality than some algorithms

## Applications

### 1. Pattern Matching
- Finding all occurrences of a pattern in text
- Multiple pattern search (with modifications)

### 2. String Processing
- Finding periods of a string
- Longest palindromic substring
- String compression
- Suffix array construction

### 3. Bioinformatics
- DNA sequence analysis
- Protein sequence matching
- Genome assembly

### 4. Text Processing
- Plagiarism detection
- Document similarity
- Data deduplication

## Related Problems

### 1. Finding All Palindromes
```python
def find_palindromes_z(s):
    # Reverse string and find common substrings
    rev_s = s[::-1]
    combined = s + "$" + rev_s
    z = z_algorithm(combined)
    # Process Z array to find palindromes
```

### 2. Period of String
```python
def find_period_z(s):
    z = z_algorithm(s)
    n = len(s)
    for i in range(1, n):
        if i + z[i] == n:
            return i  # Period found
    return n  # No period
```

### 3. String Compression
```python
def compress_string_z(s):
    z = z_algorithm(s)
    # Use Z array to find repeating patterns
    # Implement compression logic
```

## Comparison with Other Algorithms

| Algorithm | Time | Space | Preprocessing | Best For | 
|-----------|------|-------|---------------|----------|
| Z Algorithm | O(n+m) | O(n+m) | O(m) | Simple implementation |
| KMP | O(n+m) | O(m) | O(m) | Space efficiency |
| Rabin-Karp | O(n+m)* | O(1) | O(m) | Multiple patterns |
| Boyer-Moore | O(n/m) | O(σ) | O(m+σ) | Long patterns |
| Naive | O(nm) | O(1) | O(1) | Very short patterns |

*Average case for Rabin-Karp

## Advanced Topics

### 1. Z Algorithm Variants
- Online Z algorithm for streaming data
- Parallel Z algorithm implementation
- Z algorithm for circular strings

### 2. Optimization Techniques
- Memory-efficient implementation
- SIMD optimizations
- Cache-friendly versions

### 3. Extended Applications
- Suffix array construction using Z algorithm
- Longest common substring
- Approximate string matching

## Practice Problems

1. **Basic**: Find all occurrences of pattern in text
2. **Intermediate**: Find longest palindromic substring
3. **Advanced**: String periodicity and compression
4. **Expert**: Multiple pattern matching with Z algorithm

## Interview Questions

1. Explain how Z algorithm works with an example
2. Why is Z algorithm linear time?
3. Compare Z algorithm with KMP
4. How to find the period of a string using Z algorithm?
5. Implement Z algorithm without extra space

## Key Points to Remember

- Z[i] represents length of longest prefix match starting at position i
- Algorithm uses Z-box optimization to achieve linear time
- Requires concatenation with separator for pattern matching
- Simpler to implement and understand than KMP
- Versatile for various string processing problems
- Always runs in O(n) time regardless of input pattern

//12/7/25 (day 19) //
# Segment Tree - Complete Guide

## Table of Contents
1. [Introduction](#introduction)
2. [Basic Concepts](#basic-concepts)
3. [Structure and Properties](#structure-and-properties)
4. [Basic Operations](#basic-operations)
5. [Implementation](#implementation)
6. [Advanced Topics](#advanced-topics)
7. [Applications](#applications)
8. [Practice Problems](#practice-problems)

---

## Introduction

A **Segment Tree** is a binary tree data structure used for storing information about array segments in a way that allows answering range queries efficiently. It's particularly useful when you need to perform multiple queries on ranges of an array and/or update elements frequently.

### Key Features
- **Efficient Range Queries**: O(log n) time complexity
- **Efficient Point Updates**: O(log n) time complexity
- **Flexible Operations**: Can handle sum, min, max, GCD, LCM, etc.
- **Range Updates**: With lazy propagation, range updates are also O(log n)

### Time Complexity
| Operation | Time Complexity |
|-----------|----------------|
| Build | O(n) |
| Point Update | O(log n) |
| Range Query | O(log n) |
| Range Update (with lazy) | O(log n) |

### Space Complexity
- **Space**: O(4n) ≈ O(n)

---

## Basic Concepts

### Problem Statement
Given an array `arr[]` of size `n`, we need to efficiently:
1. Find the sum/min/max of elements in a given range `[l, r]`
2. Update the value of any element in the array

### Naive Approach vs Segment Tree
```
Naive Approach:
- Range Query: O(n)
- Point Update: O(1)

Segment Tree:
- Range Query: O(log n)
- Point Update: O(log n)
```

### Tree Structure
- Each node represents a segment/range of the array
- Leaf nodes represent individual array elements
- Internal nodes represent the union of child segments
- Root node represents the entire array

---

## Structure and Properties

### Tree Representation
```
Array: [1, 3, 5, 7, 9, 11]
Index:  0  1  2  3  4  5

Segment Tree (Sum):
                    36[0,5]
                   /        \
              9[0,2]          27[3,5]
             /      \        /        \
        4[0,1]      5[2,2]  7[3,3]    20[4,5]
       /      \                      /        \
   1[0,0]   3[1,1]              9[4,4]    11[5,5]
```

### Node Properties
- **Internal Node**: `tree[node] = tree[2*node] + tree[2*node+1]`
- **Leaf Node**: `tree[node] = arr[index]`
- **Range**: Each node covers range `[start, end]`

### Array Indexing
- For 0-based indexing: Use indices 0 to n-1
- Tree array size: 4*n (to handle all possible nodes)

---

## Basic Operations

### 1. Build Operation
```java
void build(int[] arr, int node, int start, int end) {
    if (start == end) {
        // Leaf node
        tree[node] = arr[start];
    } else {
        int mid = (start + end) / 2;
        // Build left subtree
        build(arr, 2*node, start, mid);
        // Build right subtree
        build(arr, 2*node+1, mid+1, end);
        // Internal node value
        tree[node] = tree[2*node] + tree[2*node+1];
    }
}
```

### 2. Point Update
```java
void update(int node, int start, int end, int idx, int val) {
    if (start == end) {
        // Leaf node - update value
        tree[node] = val;
    } else {
        int mid = (start + end) / 2;
        if (idx <= mid) {
            // Update left subtree
            update(2*node, start, mid, idx, val);
        } else {
            // Update right subtree
            update(2*node+1, mid+1, end, idx, val);
        }
        // Update current node
        tree[node] = tree[2*node] + tree[2*node+1];
    }
}
```

### 3. Range Query
```java
int query(int node, int start, int end, int l, int r) {
    if (r < start || end < l) {
        // No overlap
        return 0;
    }
    if (l <= start && end <= r) {
        // Complete overlap
        return tree[node];
    }
    // Partial overlap
    int mid = (start + end) / 2;
    int leftSum = query(2*node, start, mid, l, r);
    int rightSum = query(2*node+1, mid+1, end, l, r);
    return leftSum + rightSum;
}
```

---

## Implementation

### Complete Segment Tree Class
```java
class SegmentTree {
    private int[] tree;
    private int n;
    
    private void build(int[] arr, int node, int start, int end) {
        if (start == end) {
            tree[node] = arr[start];
        } else {
            int mid = (start + end) / 2;
            build(arr, 2*node, start, mid);
            build(arr, 2*node+1, mid+1, end);
            tree[node] = tree[2*node] + tree[2*node+1];
        }
    }
    
    private void update(int node, int start, int end, int idx, int val) {
        if (start == end) {
            tree[node] = val;
        } else {
            int mid = (start + end) / 2;
            if (idx <= mid) {
                update(2*node, start, mid, idx, val);
            } else {
                update(2*node+1, mid+1, end, idx, val);
            }
            tree[node] = tree[2*node] + tree[2*node+1];
        }
    }
    
    private int query(int node, int start, int end, int l, int r) {
        if (r < start || end < l) {
            return 0; // Return identity element
        }
        if (l <= start && end <= r) {
            return tree[node];
        }
        int mid = (start + end) / 2;
        return query(2*node, start, mid, l, r) + 
               query(2*node+1, mid+1, end, l, r);
    }

    public SegmentTree(int[] arr) {
        n = arr.length;
        tree = new int[4 * n];
        build(arr, 1, 0, n - 1);
    }
    
    public void update(int idx, int val) {
        update(1, 0, n - 1, idx, val);
    }
    
    public int query(int l, int r) {
        return query(1, 0, n - 1, l, r);
    }
}
```

### Usage Example
```java
public class Main {
    public static void main(String[] args) {
        int[] arr = {1, 3, 5, 7, 9, 11};
        SegmentTree st = new SegmentTree(arr);
        
        // Query sum of range [1, 3]
        System.out.println(st.query(1, 3)); // Output: 15
        
        // Update index 1 to value 10
        st.update(1, 10);
        
        // Query again
        System.out.println(st.query(1, 3)); // Output: 22
    }
}
```

---

## Advanced Topics

### 1. Different Operations

#### Range Minimum Query (RMQ)
```java
class RMQSegmentTree {
    private int[] tree;
    
    private void build(int[] arr, int node, int start, int end) {
        if (start == end) {
            tree[node] = arr[start];
        } else {
            int mid = (start + end) / 2;
            build(arr, 2*node, start, mid);
            build(arr, 2*node+1, mid+1, end);
            tree[node] = Math.min(tree[2*node], tree[2*node+1]);
        }
    }
    
    private int query(int node, int start, int end, int l, int r) {
        if (r < start || end < l) {
            return Integer.MAX_VALUE; // Return infinity for min
        }
        if (l <= start && end <= r) {
            return tree[node];
        }
        int mid = (start + end) / 2;
        return Math.min(query(2*node, start, mid, l, r),
                       query(2*node+1, mid+1, end, l, r));
    }
}
```

#### Range Maximum Query
```java
// Similar to RMQ but use Math.max() instead of Math.min()
// Return Integer.MIN_VALUE for no overlap case
```

#### Range GCD Query
```java
int gcd(int a, int b) {
    return b == 0 ? a : gcd(b, a % b);
}

// In build and query functions, use gcd() instead of +
tree[node] = gcd(tree[2*node], tree[2*node+1]);
```

### 2. Lazy Propagation

For efficient range updates, we use lazy propagation:

```java
class LazySegmentTree {
    private long[] tree, lazy;
    
    private void push(int node, int start, int end) {
        if (lazy[node] != 0) {
            tree[node] += lazy[node] * (end - start + 1);
            if (start != end) {
                lazy[2*node] += lazy[node];
                lazy[2*node+1] += lazy[node];
            }
            lazy[node] = 0;
        }
    }
    
    private void updateRange(int node, int start, int end, int l, int r, int val) {
        push(node, start, end);
        if (start > r || end < l) return;
        
        if (start >= l && end <= r) {
            lazy[node] += val;
            push(node, start, end);
            return;
        }
        
        int mid = (start + end) / 2;
        updateRange(2*node, start, mid, l, r, val);
        updateRange(2*node+1, mid+1, end, l, r, val);
        
        push(2*node, start, mid);
        push(2*node+1, mid+1, end);
        tree[node] = tree[2*node] + tree[2*node+1];
    }
}
```

### 3. 2D Segment Tree

For 2D range queries:
```java
class SegmentTree2D {
    private int[][] tree;
    private int[][] arr;  // 2D input array
    private int n, m;
    
    private void buildY(int vx, int lx, int rx, int vy, int ly, int ry) {
        if (ly == ry) {
            if (lx == rx) {
                tree[vx][vy] = arr[lx][ly];
            } else {
                tree[vx][vy] = tree[2*vx][vy] + tree[2*vx+1][vy];
            }
        } else {
            int my = (ly + ry) / 2;
            buildY(vx, lx, rx, 2*vy, ly, my);
            buildY(vx, lx, rx, 2*vy+1, my+1, ry);
            tree[vx][vy] = tree[vx][2*vy] + tree[vx][2*vy+1];
        }
    }
    
    private void buildX(int vx, int lx, int rx) {
        if (lx != rx) {
            int mx = (lx + rx) / 2;
            buildX(2*vx, lx, mx);
            buildX(2*vx+1, mx+1, rx);
        }
        buildY(vx, lx, rx, 1, 0, m-1);
    }
}
```

---

## Applications

### 1. Range Sum Queries
- **Problem**: Answer multiple queries for sum of elements in range [l, r]
- **Solution**: Basic segment tree with sum operation

### 2. Range Minimum/Maximum Queries
- **Problem**: Find minimum/maximum element in range [l, r]
- **Solution**: Segment tree with min/max operations

### 3. Range GCD Queries
- **Problem**: Find GCD of all elements in range [l, r]
- **Solution**: Segment tree with GCD operation

### 4. Count of Elements in Range
- **Problem**: Count elements in range [l, r] that satisfy certain condition
- **Solution**: Segment tree storing count information

### 5. Range Updates with Range Queries
- **Problem**: Add value to all elements in range [l, r] and answer sum queries
- **Solution**: Lazy propagation segment tree

### 6. Persistent Segment Tree
- **Problem**: Maintain multiple versions of segment tree
- **Solution**: Create new nodes only when needed

---

## Practice Problems

### Beginner Level
1. **Range Sum Query - Immutable**
   - Build segment tree and answer range sum queries
   - No updates required

2. **Range Minimum Query**
   - Find minimum element in given range
   - Static array, no updates

3. **Point Update Range Sum**
   - Update single element and answer range sum queries

### Intermediate Level
4. **Range Update Range Sum**
   - Add value to range [l, r] and answer sum queries
   - Use lazy propagation

5. **Range GCD Query**
   - Find GCD of elements in range [l, r]

6. **Count of Elements**
   - Count elements in range [l, r] that are greater than x

### Advanced Level
7. **Persistent Segment Tree**
   - Maintain multiple versions of the tree

8. **2D Range Sum Query**
   - Answer sum queries on 2D matrix

9. **Dynamic Range Queries**
   - Handle coordinate compression for large ranges

---

## Tips and Tricks

### 1. Implementation Tips
- Use 1-based indexing for tree array (easier to calculate children)
- Tree size should be 4*n to handle all nodes
- Be careful with data types (use long for large sums)

### 2. Debugging Tips
- Print tree structure to visualize
- Test with small examples first
- Check boundary conditions

### 3. Optimization Tips
- Use iterative approach for better constants
- Cache-friendly memory access patterns
- Avoid unnecessary recursive calls

### 4. Common Mistakes
- Incorrect tree size allocation
- Wrong indexing (0-based vs 1-based)
- Not handling edge cases properly
- Integer overflow in sum operations

---

## Complexity Analysis

### Time Complexity
- **Build**: O(n) - Visit each node once
- **Query**: O(log n) - Visit at most 4 nodes per level
- **Update**: O(log n) - Single path from root to leaf
- **Range Update (Lazy)**: O(log n) - Amortized

### Space Complexity
- **Tree Storage**: O(4n) ≈ O(n)
- **Lazy Array**: O(4n) for lazy propagation
- **Recursion Stack**: O(log n)

### Comparison with Other Data Structures
| Data Structure | Build | Query | Update | Space |
|---------------|-------|-------|---------|-------|
| Array | O(1) | O(n) | O(1) | O(n) |
| Prefix Sum | O(n) | O(1) | O(n) | O(n) |
| Segment Tree | O(n) | O(log n) | O(log n) | O(n) |
| Fenwick Tree | O(n log n) | O(log n) | O(log n) | O(n) |

---

## Conclusion

Segment Trees are powerful data structures that provide efficient solutions for range query problems. They offer a good balance between query time and update time, making them suitable for many competitive programming problems and real-world applications.

**Key Takeaways:**
- Master the basic operations: build, query, update
- Understand lazy propagation for range updates
- Practice with different operations (sum, min, max, GCD)
- Learn to identify when segment trees are the right choice

**Next Steps:**
- Practice implementation from scratch
- Solve problems with different operations
- Learn about persistent and 2D segment trees
- Explore advanced applications and optimizations

# Segment Tree with Lazy Propagation - Complete Guide

## Table of Contents
1. [Introduction](#introduction)
2. [Why Lazy Propagation?](#why-lazy-propagation)
3. [Core Concepts](#core-concepts)
4. [Basic Implementation](#basic-implementation)
5. [Advanced Implementations](#advanced-implementations)
6. [Types of Lazy Propagation](#types-of-lazy-propagation)
7. [Common Patterns](#common-patterns)
8. [Applications](#applications)
9. [Practice Problems](#practice-problems)
10. [Optimization Techniques](#optimization-techniques)

---

## Introduction

**Lazy Propagation** is an optimization technique used with segment trees to handle range updates efficiently. Without lazy propagation, updating a range [l, r] would require O(n log n) time in the worst case. With lazy propagation, we can perform range updates in O(log n) time.

### Key Idea
Instead of immediately updating all nodes affected by a range update, we **defer** (make lazy) the updates and apply them only when we actually need to visit those nodes.

### Time Complexity Comparison
| Operation | Without Lazy | With Lazy |
|-----------|-------------|-----------|
| Range Update | O(n log n) | O(log n) |
| Range Query | O(log n) | O(log n) |
| Point Update | O(log n) | O(log n) |

---

## Why Lazy Propagation?

### Problem with Regular Segment Tree
```java
// Without lazy propagation
void updateRange(int node, int start, int end, int l, int r, int val) {
    if (start > r || end < l) return;
    
    if (start == end) {
        tree[node] += val;
        return;
    }
    
    int mid = (start + end) / 2;
    updateRange(2*node, start, mid, l, r, val);
    updateRange(2*node+1, mid+1, end, l, r, val);
    tree[node] = tree[2*node] + tree[2*node+1];
}
```
**Problem**: This updates O(n) nodes in worst case, leading to O(n log n) complexity.

### Solution: Lazy Propagation
```java
// With lazy propagation
void updateRange(int node, int start, int end, int l, int r, int val) {
    push(node, start, end);  // Apply pending updates
    
    if (start > r || end < l) return;
    
    if (start >= l && end <= r) {
        lazy[node] += val;    // Mark as lazy
        push(node, start, end);  // Apply immediately
        return;
    }
    
    int mid = (start + end) / 2;
    updateRange(2*node, start, mid, l, r, val);
    updateRange(2*node+1, mid+1, end, l, r, val);
    
    push(2*node, start, mid);
    push(2*node+1, mid+1, end);
    tree[node] = tree[2*node] + tree[2*node+1];
}
```
**Advantage**: Only visits O(log n) nodes, achieving O(log n) complexity.

---

## Core Concepts

### 1. Lazy Array
```java
long[] lazy;  // Stores pending updates
```
- **Purpose**: Store updates that haven't been applied to children yet
- **Size**: Same as tree array (4 * n)
- **Initialization**: All zeros initially

### 2. Push Operation
```java
void push(int node, int start, int end) {
    if (lazy[node] != 0) {
        // Apply lazy update to current node
        tree[node] += lazy[node] * (end - start + 1);
        
        // Propagate to children (if not leaf)
        if (start != end) {
            lazy[2*node] += lazy[node];
            lazy[2*node+1] += lazy[node];
        }
        
        // Clear lazy value
        lazy[node] = 0;
    }
}
```

### 3. Update Strategy
1. **Push** pending updates before any operation
2. **Check** if current range is completely within update range
3. **Mark lazy** if complete overlap
4. **Recurse** if partial overlap
5. **Update parent** after children are processed

---

## Basic Implementation

### Range Addition + Range Sum Query
```java
class LazySegmentTree {
    private long[] tree, lazy;
    private int n;
    
    private void push(int node, int start, int end) {
        if (lazy[node] != 0) {
            // Apply pending update
            tree[node] += lazy[node] * (end - start + 1);
            
            // Propagate to children
            if (start != end) {
                lazy[2*node] += lazy[node];
                lazy[2*node+1] += lazy[node];
            }
            
            // Clear lazy flag
            lazy[node] = 0;
        }
    }
    
    private void build(int[] arr, int node, int start, int end) {
        if (start == end) {
            tree[node] = arr[start];
        } else {
            int mid = (start + end) / 2;
            build(arr, 2*node, start, mid);
            build(arr, 2*node+1, mid+1, end);
            tree[node] = tree[2*node] + tree[2*node+1];
        }
    }
    
    private void updateRange(int node, int start, int end, int l, int r, int val) {
        push(node, start, end);  // Apply pending updates
        
        if (start > r || end < l) return;  // No overlap
        
        if (start >= l && end <= r) {  // Complete overlap
            lazy[node] += val;
            push(node, start, end);
            return;
        }
        
        // Partial overlap
        int mid = (start + end) / 2;
        updateRange(2*node, start, mid, l, r, val);
        updateRange(2*node+1, mid+1, end, l, r, val);
        
        // Update current node
        push(2*node, start, mid);
        push(2*node+1, mid+1, end);
        tree[node] = tree[2*node] + tree[2*node+1];
    }
    
    private long queryRange(int node, int start, int end, int l, int r) {
        if (start > r || end < l) return 0;  // No overlap
        
        push(node, start, end);  // Apply pending updates
        
        if (start >= l && end <= r) {  // Complete overlap
            return tree[node];
        }
        
        // Partial overlap
        int mid = (start + end) / 2;
        return queryRange(2*node, start, mid, l, r) + 
               queryRange(2*node+1, mid+1, end, l, r);
    }
    
    public LazySegmentTree(int[] arr) {
        n = arr.length;
        tree = new long[4 * n];
        lazy = new long[4 * n];
        build(arr, 1, 0, n - 1);
    }
    
    public void updateRange(int l, int r, int val) {
        updateRange(1, 0, n - 1, l, r, val);
    }
    
    public long queryRange(int l, int r) {
        return queryRange(1, 0, n - 1, l, r);
    }
}
```

### Usage Example
```java
public class Main {
    public static void main(String[] args) {
        int[] arr = {1, 2, 3, 4, 5};
        LazySegmentTree lst = new LazySegmentTree(arr);
        
        System.out.println(lst.queryRange(0, 2));  // Output: 6
        
        lst.updateRange(1, 3, 10);  // Add 10 to range [1, 3]
        
        System.out.println(lst.queryRange(0, 2));  // Output: 26
        System.out.println(lst.queryRange(2, 4));  // Output: 42
    }
}
```

---

## Advanced Implementations

### 1. Range Set + Range Sum Query
```java
class LazySetSegmentTree {
    private long[] tree, lazy;
    private boolean[] hasLazy;
    private int n;
    
    private void push(int node, int start, int end) {
        if (hasLazy[node]) {
            tree[node] = lazy[node] * (end - start + 1);
            
            if (start != end) {
                lazy[2*node] = lazy[node];
                lazy[2*node+1] = lazy[node];
                hasLazy[2*node] = hasLazy[2*node+1] = true;
            }
            
            hasLazy[node] = false;
        }
    }
    
    private void updateRange(int node, int start, int end, int l, int r, int val) {
        push(node, start, end);
        
        if (start > r || end < l) return;
        
        if (start >= l && end <= r) {
            lazy[node] = val;
            hasLazy[node] = true;
            push(node, start, end);
            return;
        }
        
        int mid = (start + end) / 2;
        updateRange(2*node, start, mid, l, r, val);
        updateRange(2*node+1, mid+1, end, l, r, val);
        
        push(2*node, start, mid);
        push(2*node+1, mid+1, end);
        tree[node] = tree[2*node] + tree[2*node+1];
    }
    
    public LazySetSegmentTree(int size) {
        n = size;
        tree = new long[4 * size];
        lazy = new long[4 * size];
        hasLazy = new boolean[4 * size];
    }
    
    public void setRange(int l, int r, int val) {
        updateRange(1, 0, n - 1, l, r, val);
    }
}
```

### 2. Range Min/Max with Range Addition
```java
class LazyMinSegmentTree {
    private int[] tree, lazy;
    private int n;
    
    private void push(int node, int start, int end) {
        if (lazy[node] != 0) {
            tree[node] += lazy[node];
            
            if (start != end) {
                lazy[2*node] += lazy[node];
                lazy[2*node+1] += lazy[node];
            }
            
            lazy[node] = 0;
        }
    }
    
    private void updateRange(int node, int start, int end, int l, int r, int val) {
        push(node, start, end);
        
        if (start > r || end < l) return;
        
        if (start >= l && end <= r) {
            lazy[node] += val;
            push(node, start, end);
            return;
        }
        
        int mid = (start + end) / 2;
        updateRange(2*node, start, mid, l, r, val);
        updateRange(2*node+1, mid+1, end, l, r, val);
        
        push(2*node, start, mid);
        push(2*node+1, mid+1, end);
        tree[node] = Math.min(tree[2*node], tree[2*node+1]);
    }
    
    private int queryRange(int node, int start, int end, int l, int r) {
        if (start > r || end < l) return Integer.MAX_VALUE;
        
        push(node, start, end);
        
        if (start >= l && end <= r) {
            return tree[node];
        }
        
        int mid = (start + end) / 2;
        return Math.min(queryRange(2*node, start, mid, l, r),
                       queryRange(2*node+1, mid+1, end, l, r));
    }
}
```

### 3. Multiple Operations (Add + Multiply)
```java
class LazyMultiOpSegmentTree {
    private static class LazyData {
        long addVal = 0;
        long mulVal = 1;
        
        void combine(LazyData other) {
            addVal = addVal * other.mulVal + other.addVal;
            mulVal = mulVal * other.mulVal;
        }
    }
    
    private long[] tree;
    private LazyData[] lazy;
    private int n;
    
    private void push(int node, int start, int end) {
        if (lazy[node].addVal != 0 || lazy[node].mulVal != 1) {
            tree[node] = tree[node] * lazy[node].mulVal + 
                        lazy[node].addVal * (end - start + 1);
            
            if (start != end) {
                lazy[2*node].combine(lazy[node]);
                lazy[2*node+1].combine(lazy[node]);
            }
            
            lazy[node] = new LazyData();
        }
    }
    
    private void addRange(int node, int start, int end, int l, int r, int val) {
        push(node, start, end);
        
        if (start > r || end < l) return;
        
        if (start >= l && end <= r) {
            lazy[node].addVal += val;
            push(node, start, end);
            return;
        }
        
        int mid = (start + end) / 2;
        addRange(2*node, start, mid, l, r, val);
        addRange(2*node+1, mid+1, end, l, r, val);
        
        push(2*node, start, mid);
        push(2*node+1, mid+1, end);
        tree[node] = tree[2*node] + tree[2*node+1];
    }
    
    private void mulRange(int node, int start, int end, int l, int r, int val) {
        push(node, start, end);
        
        if (start > r || end < l) return;
        
        if (start >= l && end <= r) {
            lazy[node].mulVal *= val;
            lazy[node].addVal *= val;
            push(node, start, end);
            return;
        }
        
        int mid = (start + end) / 2;
        mulRange(2*node, start, mid, l, r, val);
        mulRange(2*node+1, mid+1, end, l, r, val);
        
        push(2*node, start, mid);
        push(2*node+1, mid+1, end);
        tree[node] = tree[2*node] + tree[2*node+1];
    }
    
    public void addRange(int l, int r, int val) {
        addRange(1, 0, n - 1, l, r, val);
    }
    
    public void mulRange(int l, int r, int val) {
        mulRange(1, 0, n - 1, l, r, val);
    }
}
```

---

## Types of Lazy Propagation

### 1. Range Addition
```java
// Add val to all elements in range [l, r]
lazy[node] += val;
tree[node] += val * (end - start + 1);
```

### 2. Range Set
```java
// Set all elements in range [l, r] to val
lazy[node] = val;
hasLazy[node] = true;
tree[node] = val * (end - start + 1);
```

### 3. Range Multiplication
```java
// Multiply all elements in range [l, r] by val
lazy[node] *= val;
tree[node] *= val;
```

### 4. Range XOR
```java
// XOR all elements in range [l, r] with val
lazy[node] ^= val;
// For sum queries, need to handle count of 1s
```

### 5. Range Flip (Boolean)
```java
// Flip all bits in range [l, r]
lazy[node] = !lazy[node];
tree[node] = (end - start + 1) - tree[node];
```

---

## Common Patterns

### 1. Template for Range Addition
```java
class LazySegmentTree<T> {
    private T[] tree, lazy;
    private int n;
    
    private void push(int node, int start, int end) {
        if (!lazy[node].equals(getZero())) {
            tree[node] = add(tree[node], multiply(lazy[node], end - start + 1));
            if (start != end) {
                lazy[2*node] = add(lazy[2*node], lazy[node]);
                lazy[2*node+1] = add(lazy[2*node+1], lazy[node]);
            }
            lazy[node] = getZero();
        }
    }
    
    private void updateRange(int node, int start, int end, int l, int r, T val) {
        push(node, start, end);
        if (start > r || end < l) return;
        if (start >= l && end <= r) {
            lazy[node] = add(lazy[node], val);
            push(node, start, end);
            return;
        }
        int mid = (start + end) / 2;
        updateRange(2*node, start, mid, l, r, val);
        updateRange(2*node+1, mid+1, end, l, r, val);
        push(2*node, start, mid);
        push(2*node+1, mid+1, end);
        tree[node] = add(tree[2*node], tree[2*node+1]);
    }
    
    private T queryRange(int node, int start, int end, int l, int r) {
        if (start > r || end < l) return getZero();
        push(node, start, end);
        if (start >= l && end <= r) return tree[node];
        int mid = (start + end) / 2;
        return add(queryRange(2*node, start, mid, l, r),
                  queryRange(2*node+1, mid+1, end, l, r));
    }
    
    public LazySegmentTree(T[] arr) {
        n = arr.length;
        tree = (T[]) new Object[4 * n];
        lazy = (T[]) new Object[4 * n];
        Arrays.fill(lazy, getZero());
        build(arr, 1, 0, n - 1);
    }
    
    public void updateRange(int l, int r, T val) {
        updateRange(1, 0, n - 1, l, r, val);
    }
    
    public T queryRange(int l, int r) {
        return queryRange(1, 0, n - 1, l, r);
    }
    
    // Abstract methods to be implemented for specific types
    protected abstract T getZero();
    protected abstract T add(T a, T b);
    protected abstract T multiply(T a, int b);
}
```

### 2. Handling Multiple Update Types
```java
class LazyNode {
    long addVal = 0;
    long setVal = 0;
    boolean hasSet = false;
    
    void applyAdd(long val) {
        if (hasSet) {
            setVal += val;
        } else {
            addVal += val;
        }
    }
    
    void applySet(long val) {
        setVal = val;
        hasSet = true;
        addVal = 0;
    }
    
    void propagate(LazyNode child) {
        if (hasSet) {
            child.applySet(setVal);
        } else if (addVal != 0) {
            child.applyAdd(addVal);
        }
    }
}
```

---

## Applications

### 1. Range Increment + Range Sum
```java
// Problem: Support operations:
// 1. Add val to all elements in range [l, r]
// 2. Find sum of elements in range [l, r]

LazySegmentTree lst = new LazySegmentTree(arr);
lst.updateRange(1, 3, 5);  // Add 5 to range [1, 3]
System.out.println(lst.queryRange(0, 4));  // Sum of range [0, 4]
```

### 2. Range Set + Range Maximum
```java
// Problem: Support operations:
// 1. Set all elements in range [l, r] to val
// 2. Find maximum element in range [l, r]

class LazyMaxSegmentTree {
    // Implementation similar to sum but use max operation
    tree[node] = Math.max(tree[2*node], tree[2*node+1]);
}
```

### 3. Rectangle Sum Queries with Updates
```java
// Problem: 2D range updates and queries
// Use 2D lazy segment tree or coordinate compression

class Lazy2DSegmentTree {
    // Each node of x-tree contains a y-tree
    // Apply lazy propagation in both dimensions
}
```

### 4. Dynamic Range GCD
```java
// Problem: Add val to range [l, r] and find GCD of range
// Challenge: GCD with lazy propagation is complex
// Solution: Use mathematical properties or square root decomposition
```

---

## Practice Problems

### Beginner Level
1. **Range Sum Update**
   - Add val to range [l, r]
   - Query sum of range [l, r]

2. **Range Set Update**
   - Set all elements in range [l, r] to val
   - Query sum of range [l, r]

3. **Range Increment Range Min**
   - Add val to range [l, r]
   - Query minimum in range [l, r]

### Intermediate Level
4. **Multiple Update Types**
   - Support both add and set operations
   - Query sum of range [l, r]

5. **Range Flip**
   - Flip all bits in range [l, r]
   - Query count of 1s in range [l, r]

6. **Range Multiplication**
   - Multiply all elements in range [l, r] by val
   - Query sum of range [l, r]

### Advanced Level
7. **Multiple Operations**
   - Support add, multiply, and set operations
   - Query sum of range [l, r]

8. **2D Lazy Propagation**
   - Rectangle updates and queries
   - Coordinate compression

9. **Persistent Lazy Segment Tree**
   - Maintain multiple versions with lazy propagation

---

## Optimization Techniques

### 1. Memory Optimization
```java
// Use int instead of long when possible
// Reduce tree size by careful analysis
int[] tree;
int[] lazy;
```

### 2. Constant Factor Optimization
```java
// Use bitwise operations for multiplication/division by 2
int mid = (start + end) >> 1;
int leftChild = node << 1;
int rightChild = (node << 1) | 1;
```

### 3. Iterative Implementation
```java
// For better cache performance and avoiding recursion overhead
class IterativeLazySegmentTree {
    // Bottom-up approach
    // More complex but faster
}
```

### 4. Coordinate Compression
```java
// For sparse ranges
class CompressedLazySegmentTree {
    Map<Integer, Integer> compress = new HashMap<>();
    List<Integer> decompress = new ArrayList<>();
    
    void coordinateCompress(int[] coords) {
        Arrays.sort(coords);
        int[] uniqueCoords = Arrays.stream(coords).distinct().toArray();
        
        for (int i = 0; i < uniqueCoords.length; i++) {
            compress.put(uniqueCoords[i], i);
            decompress.add(uniqueCoords[i]);
        }
    }
}
```

---

## Common Pitfalls and Solutions

### 1. Forgetting to Push
```java
// WRONG: Query without pushing
int query(int node, int start, int end, int l, int r) {
    if (start >= l && end <= r) return tree[node];
    // ... rest of function
}

// CORRECT: Always push before accessing
int query(int node, int start, int end, int l, int r) {
    push(node, start, end);  // Always push first
    if (start >= l && end <= r) return tree[node];
    // ... rest of function
}
```

### 2. Incorrect Lazy Combination
```java
// WRONG: Direct assignment
lazy[child] = lazy[parent];

// CORRECT: Proper combination
lazy[child] += lazy[parent];  // For addition
lazy[child] *= lazy[parent];  // For multiplication
```

### 3. Integer Overflow
```java
// WRONG: Using int for large sums
int[] tree, lazy;

// CORRECT: Using long
long[] tree, lazy;
```

### 4. Index Confusion
```java
// WRONG: Mixing 0-based and 1-based indexing
// Be consistent throughout implementation

// CORRECT: Clear indexing strategy
// Use 1-based for tree, 0-based for array
```

---

## Time and Space Complexity

### Time Complexity
- **Build**: O(n)
- **Range Update**: O(log n) amortized
- **Range Query**: O(log n)
- **Point Update**: O(log n)

### Space Complexity
- **Tree Array**: O(4n)
- **Lazy Array**: O(4n)
- **Total**: O(n)

### Amortized Analysis
- Each lazy update is pushed down at most once
- Total work across all operations is bounded
- Amortized O(log n) per operation

---

## Conclusion

Lazy propagation transforms segment trees from a data structure suitable for point updates to one that handles range updates efficiently. Key takeaways:

1. **Core Concept**: Defer updates until necessary
2. **Push Operation**: Always apply pending updates before accessing nodes
3. **Range Updates**: Mark complete ranges as lazy instead of updating all nodes
4. **Flexibility**: Supports various operations (add, set, multiply, etc.)
5. **Efficiency**: Achieves O(log n) for both updates and queries

### Best Practices
- Always push before accessing node values
- Handle lazy propagation consistently across all operations
- Use appropriate data types to avoid overflow
- Test with edge cases and large inputs
- Consider iterative implementations for better performance

### Next Steps
- Master basic lazy propagation patterns
- Practice with different update types
- Learn advanced techniques like 2D lazy propagation
- Explore persistent lazy segment trees
- Study real-world applications and optimization techniques

Lazy propagation is a powerful technique that significantly extends the capabilities of segment trees, making them suitable for a wide range of competitive programming problems and real-world applications requiring efficient range operations.

# Fenwick Tree (Binary Indexed Tree) - Complete Guide

## Table of Contents
1. [Introduction](#introduction)
2. [Core Concepts](#core-concepts)
3. [Mathematical Foundation](#mathematical-foundation)
4. [Basic Operations](#basic-operations)
5. [Implementation](#implementation)
6. [Advanced Variations](#advanced-variations)
7. [Applications](#applications)
8. [Comparison with Other Data Structures](#comparison-with-other-data-structures)
9. [Practice Problems](#practice-problems)
10. [Optimization Techniques](#optimization-techniques)

---

## Introduction

A **Fenwick Tree** (also known as **Binary Indexed Tree** or **BIT**) is a data structure that can efficiently calculate prefix sums in a table of values. It was introduced by Peter M. Fenwick in 1994 and provides an elegant solution for range sum queries with updates.

### Key Features
- **Efficient Prefix Sum Queries**: O(log n) time complexity
- **Efficient Point Updates**: O(log n) time complexity
- **Space Efficient**: O(n) space complexity
- **Simple Implementation**: Easier to implement than segment trees
- **1-based Indexing**: Uses 1-based indexing for cleaner bit manipulation

### Time Complexity
| Operation | Time Complexity |
|-----------|----------------|
| Build | O(n log n) |
| Point Update | O(log n) |
| Prefix Sum Query | O(log n) |
| Range Sum Query | O(log n) |

### Space Complexity
- **Space**: O(n)

---

## Core Concepts

### Problem Statement
Given an array `arr[]` of size `n`, we need to efficiently:
1. Update the value of any element in the array
2. Calculate the sum of elements from index 1 to i (prefix sum)
3. Calculate the sum of elements in range [l, r] using prefix sums

### Naive Approach vs Fenwick Tree
```
Naive Approach:
- Prefix Sum Query: O(n)
- Point Update: O(n) (if maintaining prefix sum array)

Fenwick Tree:
- Prefix Sum Query: O(log n)
- Point Update: O(log n)
```

### Key Insight
The Fenwick Tree uses the binary representation of indices to determine which elements each tree node is responsible for. Each position in the tree stores the sum of a specific range of elements.

### LSB (Least Significant Bit) Operation
The core operation in Fenwick Tree is finding the least significant bit:
```java
int lsb(int x) {
    return x & (-x);
}
```

This operation determines:
- **For Update**: Which positions to update
- **For Query**: Which positions to sum

---

## Mathematical Foundation

### Binary Representation and Ranges
Each index in the Fenwick Tree is responsible for a range of elements determined by its binary representation.

```
Index (Binary) | Range Covered | Tree[index] stores
1 (001)        | [1, 1]       | arr[1]
2 (010)        | [1, 2]       | arr[1] + arr[2]
3 (011)        | [3, 3]       | arr[3]
4 (100)        | [1, 4]       | arr[1] + arr[2] + arr[3] + arr[4]
5 (101)        | [5, 5]       | arr[5]
6 (110)        | [5, 6]       | arr[5] + arr[6]
7 (111)        | [7, 7]       | arr[7]
8 (1000)       | [1, 8]       | arr[1] + ... + arr[8]
```

### Range Calculation
For index `i`, the range it covers is `[i - lsb(i) + 1, i]`

### Visual Representation
```
Array:  [_, 1, 2, 3, 4, 5, 6, 7, 8]  (1-indexed)
Index:   0  1  2  3  4  5  6  7  8

Fenwick Tree:
tree[1] = arr[1]                    = 1
tree[2] = arr[1] + arr[2]          = 1 + 2 = 3
tree[3] = arr[3]                    = 3
tree[4] = arr[1] + arr[2] + arr[3] + arr[4] = 1+2+3+4 = 10
tree[5] = arr[5]                    = 5
tree[6] = arr[5] + arr[6]          = 5 + 6 = 11
tree[7] = arr[7]                    = 7
tree[8] = arr[1] + ... + arr[8]    = 1+2+3+4+5+6+7+8 = 36
```

---

## Basic Operations

### 1. Update Operation
To update element at index `i` by adding `delta`:

```java
void update(int i, int delta) {
    for (int idx = i; idx <= n; idx += lsb(idx)) {
        tree[idx] += delta;
    }
}
```

**Process:**
1. Start from index `i`
2. Add `delta` to `tree[i]`
3. Move to next index: `i + lsb(i)`
4. Repeat until index exceeds `n`

### 2. Prefix Sum Query
To find sum from index 1 to `i`:

```java
int query(int i) {
    int sum = 0;
    for (int idx = i; idx > 0; idx -= lsb(idx)) {
        sum += tree[idx];
    }
    return sum;
}
```

**Process:**
1. Start from index `i`
2. Add `tree[i]` to sum
3. Move to previous index: `i - lsb(i)`
4. Repeat until index becomes 0

### 3. Range Sum Query
To find sum from index `l` to `r`:

```java
int rangeQuery(int l, int r) {
    return query(r) - query(l - 1);
}
```

---

## Implementation

### Basic Fenwick Tree Implementation

```java
class FenwickTree {
    private int[] tree;
    private int n;
    
    // Get least significant bit
    private int lsb(int x) {
        return x & (-x);
    }
    
    // Constructor
    public FenwickTree(int size) {
        n = size;
        tree = new int[n + 1]; // 1-indexed
    }
    
    // Constructor with initial array
    public FenwickTree(int[] arr) {
        n = arr.length;
        tree = new int[n + 1];
        
        // Build tree
        for (int i = 0; i < n; i++) {
            update(i + 1, arr[i]);
        }
    }
    
    // Update element at index i (1-indexed) by adding delta
    public void update(int i, int delta) {
        for (int idx = i; idx <= n; idx += lsb(idx)) {
            tree[idx] += delta;
        }
    }
    
    // Get prefix sum from 1 to i
    public int query(int i) {
        int sum = 0;
        for (int idx = i; idx > 0; idx -= lsb(idx)) {
            sum += tree[idx];
        }
        return sum;
    }
    
    // Get range sum from l to r (both inclusive)
    public int rangeQuery(int l, int r) {
        return query(r) - query(l - 1);
    }
    
    // Set element at index i to value (not add)
    public void set(int i, int value) {
        int currentValue = rangeQuery(i, i);
        update(i, value - currentValue);
    }
    
    // Get element at index i
    public int get(int i) {
        return rangeQuery(i, i);
    }
}
```

### Usage Example

```java
public class Main {
    public static void main(String[] args) {
        int[] arr = {1, 2, 3, 4, 5};
        FenwickTree ft = new FenwickTree(arr);
        
        // Query prefix sum from 1 to 3
        System.out.println(ft.query(3)); // Output: 6 (1+2+3)
        
        // Query range sum from 2 to 4
        System.out.println(ft.rangeQuery(2, 4)); // Output: 9 (2+3+4)
        
        // Update index 3 by adding 5
        ft.update(3, 5);
        
        // Query again
        System.out.println(ft.query(3)); // Output: 11 (1+2+8)
        System.out.println(ft.rangeQuery(2, 4)); // Output: 14 (2+8+4)
        
        // Set index 2 to value 10
        ft.set(2, 10);
        System.out.println(ft.rangeQuery(1, 3)); // Output: 19 (1+10+8)
    }
}
```

---

## Advanced Variations

### 1. Range Update with Point Query

For range updates, we can use the difference array technique:

```java
class RangeUpdateFenwickTree {
    private FenwickTree diff;
    
    public RangeUpdateFenwickTree(int size) {
        diff = new FenwickTree(size);
    }
    
    // Add delta to range [l, r]
    public void updateRange(int l, int r, int delta) {
        diff.update(l, delta);
        diff.update(r + 1, -delta);
    }
    
    // Get value at point i
    public int pointQuery(int i) {
        return diff.query(i);
    }
}
```

### 2. Range Update with Range Query

Using two Fenwick Trees:

```java
class RangeUpdateRangeQueryFenwick {
    private FenwickTree B1, B2;
    
    public RangeUpdateRangeQueryFenwick(int size) {
        B1 = new FenwickTree(size);
        B2 = new FenwickTree(size);
    }
    
    // Add delta to range [l, r]
    public void updateRange(int l, int r, int delta) {
        B1.update(l, delta);
        B1.update(r + 1, -delta);
        B2.update(l, delta * (l - 1));
        B2.update(r + 1, -delta * r);
    }
    
    // Get sum of range [1, i]
    public int query(int i) {
        return B1.query(i) * i - B2.query(i);
    }
    
    // Get sum of range [l, r]
    public int rangeQuery(int l, int r) {
        return query(r) - query(l - 1);
    }
}
```

### 3. 2D Fenwick Tree

For 2D prefix sum queries:

```java
class FenwickTree2D {
    private int[][] tree;
    private int n, m;
    
    public FenwickTree2D(int rows, int cols) {
        n = rows;
        m = cols;
        tree = new int[n + 1][m + 1];
    }
    
    private int lsb(int x) {
        return x & (-x);
    }
    
    public void update(int i, int j, int delta) {
        for (int x = i; x <= n; x += lsb(x)) {
            for (int y = j; y <= m; y += lsb(y)) {
                tree[x][y] += delta;
            }
        }
    }
    
    public int query(int i, int j) {
        int sum = 0;
        for (int x = i; x > 0; x -= lsb(x)) {
            for (int y = j; y > 0; y -= lsb(y)) {
                sum += tree[x][y];
            }
        }
        return sum;
    }
    
    public int rangeQuery(int x1, int y1, int x2, int y2) {
        return query(x2, y2) - query(x1 - 1, y2) - 
               query(x2, y1 - 1) + query(x1 - 1, y1 - 1);
    }
}
```

### 4. Fenwick Tree with Different Data Types

```java
class FenwickTreeLong {
    private long[] tree;
    private int n;
    
    public FenwickTreeLong(int size) {
        n = size;
        tree = new long[n + 1];
    }
    
    private int lsb(int x) {
        return x & (-x);
    }
    
    public void update(int i, long delta) {
        for (int idx = i; idx <= n; idx += lsb(idx)) {
            tree[idx] += delta;
        }
    }
    
    public long query(int i) {
        long sum = 0;
        for (int idx = i; idx > 0; idx -= lsb(idx)) {
            sum += tree[idx];
        }
        return sum;
    }
    
    public long rangeQuery(int l, int r) {
        return query(r) - query(l - 1);
    }
}
```

---

## Applications

### 1. Frequency Counting
```java
// Count frequencies in range queries
public class FrequencyCounter {
    private FenwickTree ft;
    
    public FrequencyCounter(int maxVal) {
        ft = new FenwickTree(maxVal);
    }
    
    public void addElement(int val) {
        ft.update(val, 1);
    }
    
    public void removeElement(int val) {
        ft.update(val, -1);
    }
    
    public int countInRange(int l, int r) {
        return ft.rangeQuery(l, r);
    }
}
```

### 2. Inversion Count
```java
public class InversionCounter {
    public int countInversions(int[] arr) {
        int n = arr.length;
        int maxVal = Arrays.stream(arr).max().getAsInt();
        FenwickTree ft = new FenwickTree(maxVal);
        
        int inversions = 0;
        for (int i = n - 1; i >= 0; i--) {
            inversions += ft.query(arr[i] - 1);
            ft.update(arr[i], 1);
        }
        
        return inversions;
    }
}
```

### 3. Range Minimum Query (Using Coordinate Compression)
```java
public class RangeMinQuery {
    private FenwickTree ft;
    private int[] values;
    
    public RangeMinQuery(int[] arr) {
        int n = arr.length;
        values = arr.clone();
        ft = new FenwickTree(n);
        
        // Initialize with indices (coordinate compression)
        for (int i = 0; i < n; i++) {
            ft.update(i + 1, i);
        }
    }
    
    public int rangeMin(int l, int r) {
        // This is a simplified version
        // Full implementation requires more complex logic
        return ft.rangeQuery(l, r);
    }
}
```

### 4. Dynamic Programming Optimization
```java
public class DPOptimization {
    // Example: Longest Increasing Subsequence with Fenwick Tree
    public int lengthOfLIS(int[] nums) {
        if (nums.length == 0) return 0;
        
        // Coordinate compression
        int[] sorted = nums.clone();
        Arrays.sort(sorted);
        Map<Integer, Integer> compress = new HashMap<>();
        int idx = 1;
        for (int num : sorted) {
            if (!compress.containsKey(num)) {
                compress.put(num, idx++);
            }
        }
        
        FenwickTree ft = new FenwickTree(compress.size());
        int maxLen = 0;
        
        for (int num : nums) {
            int pos = compress.get(num);
            int len = ft.query(pos - 1) + 1;
            maxLen = Math.max(maxLen, len);
            
            // Update with maximum length ending at this position
            int currentMax = ft.rangeQuery(pos, pos);
            if (len > currentMax) {
                ft.update(pos, len - currentMax);
            }
        }
        
        return maxLen;
    }
}
```

---

## Comparison with Other Data Structures

### Fenwick Tree vs Segment Tree
| Feature | Fenwick Tree | Segment Tree |
|---------|-------------|-------------|
| **Implementation** | Simpler | More complex |
| **Memory Usage** | O(n) | O(4n) |
| **Operations** | Addition only | Any associative operation |
| **Range Updates** | With difference array | Native support |
| **Code Length** | Shorter | Longer |
| **Constants** | Better | Worse |

### Fenwick Tree vs Prefix Sum Array
| Feature | Fenwick Tree | Prefix Sum Array |
|---------|-------------|------------------|
| **Build Time** | O(n log n) | O(n) |
| **Query Time** | O(log n) | O(1) |
| **Update Time** | O(log n) | O(n) |
| **Space** | O(n) | O(n) |
| **Use Case** | Frequent updates | Static queries |

### When to Use Fenwick Tree
✅ **Use Fenwick Tree when:**
- You need prefix sum operations
- You have frequent updates
- Memory is a constraint
- You want simple implementation
- You're dealing with additive operations

❌ **Don't use Fenwick Tree when:**
- You need range min/max queries
- You need complex range operations
- You need range updates frequently
- You're working with non-additive operations

---

## Practice Problems

### Beginner Level
1. **Basic Range Sum Query**
   - Given array, answer sum queries for ranges [l, r]
   - Handle point updates

2. **Frequency Queries**
   - Count frequency of elements in given range
   - Support adding/removing elements

3. **Prefix Sum with Updates**
   - Calculate prefix sums with point updates
   - Basic Fenwick Tree application

### Intermediate Level
4. **Inversion Count**
   - Count number of inversions in array
   - Use Fenwick Tree for efficient counting

5. **Range Update Point Query**
   - Update range [l, r] by adding value
   - Query value at specific point

6. **2D Range Sum Query**
   - Answer sum queries on 2D matrix
   - Handle 2D point updates

### Advanced Level
7. **Dynamic Ranking**
   - Maintain ranking of elements with updates
   - Support rank queries and updates

8. **Range Update Range Query**
   - Support both range updates and range queries
   - Use multiple Fenwick Trees

9. **Offline Query Processing**
   - Process queries in optimal order
   - Use Fenwick Tree for efficient computation

---

## Optimization Techniques

### 1. Memory Optimization
```java
// Use arrays instead of ArrayList for better performance
private int[] tree;

// Use appropriate data types
private long[] tree; // for large sums
private int[] tree;  // for regular integers
```

### 2. Constant Factor Optimization
```java
// Inline LSB operation
private int lsb(int x) {
    return x & (-x);
}

// Use bit operations where possible
for (int idx = i; idx <= n; idx += idx & (-idx)) {
    tree[idx] += delta;
}
```

### 3. Coordinate Compression
```java
public class CompressedFenwick {
    private FenwickTree ft;
    private List<Integer> coords;
    
    public CompressedFenwick(int[] values) {
        // Compress coordinates
        Set<Integer> uniqueVals = new HashSet<>();
        for (int val : values) {
            uniqueVals.add(val);
        }
        coords = new ArrayList<>(uniqueVals);
        Collections.sort(coords);
        
        ft = new FenwickTree(coords.size());
    }
    
    private int compress(int val) {
        return Collections.binarySearch(coords, val) + 1;
    }
    
    public void update(int val, int delta) {
        ft.update(compress(val), delta);
    }
    
    public int query(int val) {
        return ft.query(compress(val));
    }
}
```

### 4. Template Implementation
```java
public class FenwickTemplate<T> {
    private T[] tree;
    private int n;
    private BinaryOperator<T> operation;
    private T identity;
    
    public FenwickTemplate(int size, BinaryOperator<T> op, T identity) {
        this.n = size;
        this.operation = op;
        this.identity = identity;
        this.tree = (T[]) new Object[n + 1];
        Arrays.fill(tree, identity);
    }
    
    public void update(int i, T delta) {
        for (int idx = i; idx <= n; idx += idx & (-idx)) {
            tree[idx] = operation.apply(tree[idx], delta);
        }
    }
    
    public T query(int i) {
        T result = identity;
        for (int idx = i; idx > 0; idx -= idx & (-idx)) {
            result = operation.apply(result, tree[idx]);
        }
        return result;
    }
}
```

---

## Common Pitfalls and Solutions

### 1. Index Confusion
```java
// WRONG: Using 0-based indexing
FenwickTree ft = new FenwickTree(n);
ft.update(0, value); // Error!

// CORRECT: Using 1-based indexing
FenwickTree ft = new FenwickTree(n);
ft.update(1, value); // Correct
```

### 2. Incorrect Range Query
```java
// WRONG: Direct range query
int sum = ft.query(r) - ft.query(l); // Off by one!

// CORRECT: Proper range query
int sum = ft.query(r) - ft.query(l - 1); // Correct
```

### 3. Integer Overflow
```java
// WRONG: Using int for large sums
private int[] tree;

// CORRECT: Using long for large sums
private long[] tree;
```

### 4. Forgetting to Handle Edge Cases
```java
public int rangeQuery(int l, int r) {
    if (l > r) return 0; // Handle invalid range
    if (l == 1) return query(r);
    return query(r) - query(l - 1);
}
```

---

## Conclusion

Fenwick Trees are elegant and efficient data structures that excel at prefix sum operations with updates. They offer a perfect balance between simplicity and performance, making them ideal for competitive programming and applications requiring frequent range sum queries.

### Key Takeaways:
1. **Binary Indexing**: Understanding LSB operation is crucial
2. **1-based Indexing**: Always use 1-based indexing for cleaner implementation
3. **Prefix Sums**: Perfect for cumulative operations
4. **Space Efficient**: Uses only O(n) space
5. **Simple Code**: Much easier to implement than segment trees

### When to Choose Fenwick Tree:
- ✅ Prefix sum queries with updates
- ✅ Memory-constrained environments
- ✅ Simple additive operations
- ✅ Competitive programming contests
- ✅ Frequency counting problems

### Limitations:
- ❌ Only supports associative and commutative operations
- ❌ Cannot handle range min/max queries directly
- ❌ Range updates require additional techniques
- ❌ Not suitable for complex range operations

### Next Steps:
- Practice basic implementation from scratch
- Solve range sum problems
- Learn coordinate compression techniques
- Explore 2D Fenwick Trees
- Study advanced applications in competitive programming

Fenwick Trees are a fundamental tool in the competitive programmer's toolkit and provide an excellent foundation for understanding more advanced data structures

//14/7/25 (day20)//

# Minimum Spanning Tree (MST) - Kruskal's Algorithm

## What is a Minimum Spanning Tree?

A **Minimum Spanning Tree (MST)** is a subset of edges in a connected, undirected, weighted graph that:
- Connects all vertices together
- Has no cycles (forms a tree)
- Has the minimum possible total edge weight

### Key Properties:
- For a graph with `n` vertices, MST has exactly `n-1` edges
- MST is unique if all edge weights are distinct
- Removing any edge from MST disconnects the graph
- Adding any edge to MST creates a cycle

## Kruskal's Algorithm

Kruskal's algorithm is a **greedy algorithm** that finds the MST by:
1. Sorting all edges by weight in ascending order
2. Picking the smallest edge that doesn't form a cycle
3. Repeating until we have `n-1` edges

### Algorithm Steps:
1. Sort all edges in non-decreasing order of weight
2. Initialize each vertex as a separate component (using Union-Find)
3. For each edge in sorted order:
   - If the edge connects two different components, add it to MST
   - Union the two components
4. Stop when MST has `n-1` edges

### Pseudocode:
```
function Kruskal(Graph G):
    MST = empty set
    Sort all edges by weight
    Initialize Union-Find data structure
    
    for each edge (u, v) in sorted order:
        if Find(u) != Find(v):
            Add edge (u, v) to MST
            Union(u, v)
    
    return MST
```

## Union-Find Data Structure

Kruskal's algorithm uses **Union-Find** (Disjoint Set Union) to efficiently:
- Check if two vertices are in the same component
- Merge two components

### Operations:
- **Find(x)**: Returns the root/representative of the set containing x
- **Union(x, y)**: Merges the sets containing x and y

### Implementation with Path Compression and Union by Rank:
```python
class UnionFind:
    def __init__(self, n):
        self.parent = list(range(n))
        self.rank = [0] * n
    
    def find(self, x):
        if self.parent[x] != x:
            self.parent[x] = self.find(self.parent[x])  # Path compression
        return self.parent[x]
    
    def union(self, x, y):
        px, py = self.find(x), self.find(y)
        if px == py:
            return False
        
        # Union by rank
        if self.rank[px] < self.rank[py]:
            px, py = py, px
        self.parent[py] = px
        if self.rank[px] == self.rank[py]:
            self.rank[px] += 1
        return True
```

## Complete Implementation

```python
def kruskal_mst(n, edges):
    """
    Find MST using Kruskal's algorithm
    
    Args:
        n: number of vertices (0 to n-1)
        edges: list of (weight, u, v) tuples
    
    Returns:
        List of edges in MST and total weight
    """
    # Sort edges by weight
    edges.sort()
    
    # Initialize Union-Find
    uf = UnionFind(n)
    
    mst_edges = []
    total_weight = 0
    
    for weight, u, v in edges:
        if uf.union(u, v):
            mst_edges.append((u, v, weight))
            total_weight += weight
            
            # Stop when we have n-1 edges
            if len(mst_edges) == n - 1:
                break
    
    return mst_edges, total_weight

# Example usage
if __name__ == "__main__":
    # Graph with 4 vertices (0, 1, 2, 3)
    n = 4
    edges = [
        (1, 0, 1),  # weight=1, edge 0-1
        (2, 1, 2),  # weight=2, edge 1-2
        (3, 2, 3),  # weight=3, edge 2-3
        (4, 0, 2),  # weight=4, edge 0-2
        (5, 1, 3),  # weight=5, edge 1-3
        (6, 0, 3),  # weight=6, edge 0-3
    ]
    
    mst, weight = kruskal_mst(n, edges)
    print(f"MST edges: {mst}")
    print(f"Total weight: {weight}")
```

## Time and Space Complexity

### Time Complexity:
- **Sorting edges**: O(E log E)
- **Union-Find operations**: O(E α(V)) where α is inverse Ackermann function
- **Overall**: O(E log E) where E = number of edges

### Space Complexity:
- **Union-Find structure**: O(V)
- **Edge storage**: O(E)
- **Overall**: O(V + E)

## Example Walkthrough

Consider this graph:
```
    0 ---- 1
    |    / |
    |   /  |
    |  /   |
    | /    |
    2 ---- 3
```

Edges with weights:
- (0,1): weight 1
- (1,2): weight 2
- (2,3): weight 3
- (0,2): weight 4
- (1,3): weight 5
- (0,3): weight 6

### Step-by-step execution:
1. **Sort edges**: [(1,0,1), (2,1,2), (3,2,3), (4,0,2), (5,1,3), (6,0,3)]
2. **Process (0,1), weight=1**: Different components → Add to MST
3. **Process (1,2), weight=2**: Different components → Add to MST
4. **Process (2,3), weight=3**: Different components → Add to MST
5. **Process (0,2), weight=4**: Same component → Skip
6. **Done**: MST has 3 edges (n-1 = 4-1 = 3)

**Result**: MST = {(0,1), (1,2), (2,3)}, Total weight = 6

## Applications

1. **Network Design**: Minimum cost to connect all nodes
2. **Clustering**: Group similar data points
3. **Image Segmentation**: Connect pixels with similar properties
4. **Approximation Algorithms**: For traveling salesman problem
5. **Circuit Design**: Minimize wire length

## Comparison with Prim's Algorithm

| Aspect | Kruskal's | Prim's |
|--------|-----------|--------|
| Approach | Edge-based | Vertex-based |
| Data Structure | Union-Find | Priority Queue |
| Time Complexity | O(E log E) | O(E log V) |
| Space Complexity | O(V) | O(V) |
| Better for | Sparse graphs | Dense graphs |

## Key Points to Remember

1. **Greedy Choice**: Always pick the minimum weight edge that doesn't create a cycle
2. **Cycle Detection**: Use Union-Find for efficient cycle detection
3. **Edge Cases**: Handle disconnected graphs (forest of MSTs)
4. **Optimization**: Path compression and union by rank for better performance
5. **Alternative**: Prim's algorithm is another MST algorithm with different approach

## Common Interview Questions

1. **Basic**: Implement Kruskal's algorithm
2. **Variation**: Find MST in a graph with negative weights
3. **Application**: Connect cities with minimum cost
4. **Optimization**: Handle large graphs efficiently
5. **Theory**: Prove correctness of greedy approach

## Practice Problems

1. Find MST of given graph
2. Minimum cost to connect all points
3. Critical connections in a network
4. Optimize water supply network
5. Design communication network with minimum cost

# Prim's Algorithm in Dense Graphs - Comprehensive Explanation

## Problem Overview
This program implements Prim's Algorithm to find the Minimum Spanning Tree (MST) of a dense graph represented as an adjacency matrix. Unlike Kruskal's algorithm which sorts all edges, Prim's algorithm grows the MST one vertex at a time by selecting the minimum weight edge that connects a vertex in the MST to a vertex outside the MST.

## Algorithm Explanation

### Prim's Algorithm Steps:
1. **Start with an arbitrary vertex** (usually vertex 0)
2. **Initialize key values** for all vertices to infinity, except the starting vertex (key = 0)
3. **Repeat until all vertices are in MST:**
   - Select the vertex with minimum key value that's not yet in MST
   - Add this vertex to MST
   - Update key values of all adjacent vertices not in MST
4. **Output the MST edges** using parent array

## Code Structure Analysis

### 1. Input Reading and Graph Initialization
```java
Scanner sc = new Scanner(System.in);
int N = sc.nextInt();
int[][] graph = new int[N][N];
for (int i = 0; i < N; i++)
    for (int j = 0; j < N; j++)
        graph[i][j] = sc.nextInt();
```

**Purpose**: 
- Reads the number of vertices `N`
- Creates adjacency matrix `graph[N][N]`
- Fills the matrix with edge weights (0 means no edge)

### 2. Data Structure Initialization
```java
boolean[] inMST = new boolean[N];  // Track vertices in MST
int[] key = new int[N];           // Minimum weight to reach each vertex
int[] parent = new int[N];        // Parent of each vertex in MST
Arrays.fill(key, Integer.MAX_VALUE);
key[0] = 0;                       // Start from vertex 0
parent[0] = -1;                   // Root has no parent
```

**Data Structures:**
- **inMST[]**: Boolean array to track which vertices are already in MST
- **key[]**: Stores the minimum weight edge to reach each vertex from MST
- **parent[]**: Stores the parent of each vertex in the MST (used for output)

### 3. Main Algorithm Loop
```java
for (int count = 0; count < N - 1; count++) {
    // Find minimum key vertex not in MST
    int u = -1, min = Integer.MAX_VALUE;
    for (int v = 0; v < N; v++) {
        if (!inMST[v] && key[v] < min) {
            min = key[v];
            u = v;
        }
    }
    
    // Add vertex u to MST
    inMST[u] = true;
    
    // Update key values of adjacent vertices
    for (int v = 0; v < N; v++) {
        if (graph[u][v] != 0 && !inMST[v] && graph[u][v] < key[v]) {
            key[v] = graph[u][v];
            parent[v] = u;
        }
    }
}
```

## Step-by-Step Walkthrough with Sample Input

### Sample Input:
```
4
0 2 3 0
2 0 1 4
3 1 0 5
0 4 5 0
```

### Graph Representation:
```
    0 -----(2)------ 1
    |               |
   (3)             (1)
    |               |
    2 -----(5)------ 3
         \     /
          (1) (4)
```

### Initial State:
- **inMST**: [false, false, false, false]
- **key**: [0, ∞, ∞, ∞]
- **parent**: [-1, ?, ?, ?]

### Iteration 1 (count = 0):

#### Step 1: Find minimum key vertex not in MST
```java
// Loop through all vertices
// v=0: !inMST[0] && key[0]=0 < min=∞ → u=0, min=0
// v=1: !inMST[1] && key[1]=∞ not < min=0
// v=2: !inMST[2] && key[2]=∞ not < min=0
// v=3: !inMST[3] && key[3]=∞ not < min=0
// Result: u = 0 (vertex 0 selected)
```

#### Step 2: Add vertex 0 to MST
```java
inMST[0] = true;
// inMST: [true, false, false, false]
```

#### Step 3: Update adjacent vertices
```java
// Check all vertices adjacent to vertex 0
// v=1: graph[0][1]=2 ≠ 0 && !inMST[1] && 2 < key[1]=∞
//      → key[1]=2, parent[1]=0
// v=2: graph[0][2]=3 ≠ 0 && !inMST[2] && 3 < key[2]=∞
//      → key[2]=3, parent[2]=0
// v=3: graph[0][3]=0 → no update
```

**State after iteration 1:**
- **inMST**: [true, false, false, false]
- **key**: [0, 2, 3, ∞]
- **parent**: [-1, 0, 0, ?]

### Iteration 2 (count = 1):

#### Step 1: Find minimum key vertex
```java
// v=1: !inMST[1] && key[1]=2 < min=∞ → u=1, min=2
// v=2: !inMST[2] && key[2]=3 not < min=2
// v=3: !inMST[3] && key[3]=∞ not < min=2
// Result: u = 1 (vertex 1 selected)
```

#### Step 2: Add vertex 1 to MST
```java
inMST[1] = true;
// inMST: [true, true, false, false]
```

#### Step 3: Update adjacent vertices
```java
// Check all vertices adjacent to vertex 1
// v=0: already in MST, skip
// v=2: graph[1][2]=1 ≠ 0 && !inMST[2] && 1 < key[2]=3
//      → key[2]=1, parent[2]=1
// v=3: graph[1][3]=4 ≠ 0 && !inMST[3] && 4 < key[3]=∞
//      → key[3]=4, parent[3]=1
```

**State after iteration 2:**
- **inMST**: [true, true, false, false]
- **key**: [0, 2, 1, 4]
- **parent**: [-1, 0, 1, 1]

### Iteration 3 (count = 2):

#### Step 1: Find minimum key vertex
```java
// v=2: !inMST[2] && key[2]=1 < min=∞ → u=2, min=1
// v=3: !inMST[3] && key[3]=4 not < min=1
// Result: u = 2 (vertex 2 selected)
```

#### Step 2: Add vertex 2 to MST
```java
inMST[2] = true;
// inMST: [true, true, true, false]
```

#### Step 3: Update adjacent vertices
```java
// Check all vertices adjacent to vertex 2
// v=0: already in MST, skip
// v=1: already in MST, skip
// v=3: graph[2][3]=5 ≠ 0 && !inMST[3] && 5 not < key[3]=4
//      → no update (current path through vertex 1 is better)
```

**State after iteration 3:**
- **inMST**: [true, true, true, false]
- **key**: [0, 2, 1, 4]
- **parent**: [-1, 0, 1, 1]

### Final State:
All vertices are now in MST with the following parent relationships:
- Vertex 1 → parent = 0 (edge 0-1, weight 2)
- Vertex 2 → parent = 1 (edge 1-2, weight 1)  
- Vertex 3 → parent = 1 (edge 1-3, weight 4)

### Output Generation:
```java
for (int i = 1; i < N; i++) {
    int u = parent[i] + 1;  // Convert to 1-indexed
    int v = i + 1;          // Convert to 1-indexed
    int w = graph[parent[i]][i];
    System.out.println(u + " " + v + " " + w);
}
```

**Output:**
```
1 2 2    // Edge between vertex 1 and 2 with weight 2
2 3 1    // Edge between vertex 2 and 3 with weight 1
2 4 4    // Edge between vertex 2 and 4 with weight 4
```

Wait, let me check the expected output from the problem... The expected output is:
```
1 2 2
2 3 1
1 3 3
```

Let me trace through this again more carefully...

Actually, let me recheck the algorithm. In the third iteration, when we add vertex 2 to MST, we should check if we can get to vertex 3 with a better path:

In iteration 3, after adding vertex 2:
- For vertex 3: graph[2][3]=5, but key[3]=4 (from vertex 1)
- Since 5 > 4, we don't update parent[3]

So the final parent array is:
- parent[1] = 0 (edge 0-1)
- parent[2] = 1 (edge 1-2)  
- parent[3] = 1 (edge 1-3)

But the expected output shows edge 1-3 with weight 3, not 4. Let me check the input matrix again...

Looking at the input matrix:
```
0 2 3 0
2 0 1 4
3 1 0 5
0 4 5 0
```

The edge between vertex 0 and vertex 3 is 0, and between vertex 1 and vertex 3 is 4. But in the expected output, we see "1 3 3" which suggests edge weight 3.

Actually, looking more carefully at the expected output "1 3 3", this means vertex 1 connects to vertex 3 with weight 3. But in our 0-indexed matrix, this would be vertex 0 connecting to vertex 2 with weight 3.

Let me recalculate the MST properly:

## Corrected Algorithm Trace

The algorithm actually works correctly. Let me trace it again with the correct understanding:

### Final MST Structure:
- **MST Edges**: (0,1,2), (1,2,1), (0,2,3)
- **Total Weight**: 2 + 1 + 3 = 6

### Output (1-indexed):
```
1 2 2    // parent[1]=0, so edge (0+1, 1+1, graph[0][1]) = (1, 2, 2)
2 3 1    // parent[2]=1, so edge (1+1, 2+1, graph[1][2]) = (2, 3, 1)  
1 3 3    // parent[3]=0, so edge (0+1, 3+1, graph[0][3]) = (1, 4, 0)
```

Wait, there's still an issue. Let me check the matrix indices again...

Matrix (0-indexed):
```
   0 1 2 3
0  0 2 3 0
1  2 0 1 4  
2  3 1 0 5
3  0 4 5 0
```

So graph[0][3] = 0, meaning no direct edge between vertex 0 and vertex 3.

I think there might be a discrepancy in the expected output. Let me focus on explaining the algorithm as implemented.

## Visual Representation

### Original Graph:
```
    0 -----(2)------ 1
    |               |
   (3)             (1)
    |               |
    2 -----(5)------ 3
         \     /
          (1) (4)
```

### MST Construction Process:
1. **Start with vertex 0**
2. **Add edge (0,1)** with weight 2
3. **Add edge (1,2)** with weight 1
4. **Add edge (1,3)** with weight 4

### Final MST:
```
    0 -----(2)------ 1
                    / \
                  (1) (4)
                  /     \
                 2       3
```

## Time and Space Complexity

### Time Complexity:
- **Main loop**: O(V) iterations
- **Finding minimum key vertex**: O(V) per iteration
- **Updating adjacent vertices**: O(V) per iteration
- **Overall**: O(V²) - optimal for dense graphs

### Space Complexity:
- **Adjacency matrix**: O(V²)
- **Additional arrays**: O(V)
- **Overall**: O(V²)

## Key Differences from Kruskal's Algorithm

| Aspect | Prim's Algorithm | Kruskal's Algorithm |
|--------|------------------|-------------------|
| **Approach** | Vertex-based (grows MST) | Edge-based (sorts edges) |
| **Best for** | Dense graphs | Sparse graphs |
| **Data Structure** | Adjacency matrix + arrays | Edge list + DSU |
| **Time Complexity** | O(V²) | O(E log E) |
| **Space Complexity** | O(V²) | O(E + V) |

## Implementation Notes

1. **Dense Graph Optimization**: Uses adjacency matrix representation, optimal for dense graphs
2. **No Priority Queue**: Uses linear search to find minimum key vertex (suitable for small graphs)
3. **1-indexed Output**: Converts 0-indexed internal representation to 1-indexed output
4. **Edge Order**: Outputs edges in the order they were added to MST
5. **Memory Efficient**: Uses simple arrays instead of complex data structures

## Edge Cases Handled

1. **Single edge graphs**: Works correctly for N=2
2. **Complete graphs**: Handles all vertices connected to all others
3. **No self-loops**: Matrix diagonal is always 0
4. **Connected graphs**: Assumes input graph is connected

## Alternative Implementations

For very large graphs, this implementation could be optimized using:
- **Priority Queue**: Reduce minimum key finding from O(V) to O(log V)
- **Fibonacci Heap**: Further optimize to O(E + V log V)
- **Adjacency List**: For sparse graphs, use adjacency list instead of matrix

## Conclusion

This implementation of Prim's algorithm is well-suited for dense graphs where the number of edges is close to V². The O(V²) time complexity makes it efficient for such scenarios, and the straightforward implementation makes it easy to understand and debug.

The algorithm correctly builds the MST by always selecting the minimum weight edge that connects the growing MST to a vertex outside it, ensuring optimality through the greedy choice property of MSTs.


//15/7/25 revision //

Day 4 revision
## Searching Algorithms - Notes

### 1. Linear Search

#### What is Linear Search?
Linear search is the simplest searching algorithm that checks each element in the array sequentially until the target element is found or the end of the array is reached.

#### Algorithm:
1. Start from the first element
2. Compare each element with the target
3. If found, return the index
4. If not found after checking all elements, return -1

#### Implementation:
```java
public class LinearSearch {
    public static int linearSearch(int[] arr, int target) {
        for (int i = 0; i < arr.length; i++) {
            if (arr[i] == target) {
                return i; // Return index if found
            }
        }
        return -1; // Return -1 if not found
    }
    
    public static void main(String[] args) {
        int[] arr = {10, 20, 30, 40, 50};
        int target = 30;
        int result = linearSearch(arr, target);
        
        if (result != -1) {
            System.out.println("Element found at index: " + result);
        } else {
            System.out.println("Element not found");
        }
    }
}
```

#### Time Complexity:
- **Best Case**: O(1) - Element found at first position
- **Average Case**: O(n/2) = O(n) - Element found at middle
- **Worst Case**: O(n) - Element at last position or not found

#### Space Complexity: O(1)

#### Advantages:
- Simple to understand and implement
- Works on both sorted and unsorted arrays
- No preprocessing required

#### Disadvantages:
- Inefficient for large datasets
- Time complexity is O(n)

---

### 2. Binary Search

#### What is Binary Search?
Binary search is an efficient searching algorithm that works on sorted arrays by repeatedly dividing the search space in half.

#### Prerequisites:
- Array must be sorted
- Random access to elements (works well with arrays)

#### Algorithm:
1. Set low = 0, high = array.length - 1
2. While low <= high:
   - Calculate mid = (low + high) / 2
   - If arr[mid] == target, return mid
   - If arr[mid] < target, set low = mid + 1
   - If arr[mid] > target, set high = mid - 1
3. Return -1 if not found

#### Implementation:

##### Iterative Approach:
```java
public class BinarySearch {
    public static int binarySearch(int[] arr, int target) {
        int low = 0;
        int high = arr.length - 1;
        
        while (low <= high) {
            int mid = low + (high - low) / 2; // Prevents overflow
            
            if (arr[mid] == target) {
                return mid;
            } else if (arr[mid] < target) {
                low = mid + 1;
            } else {
                high = mid - 1;
            }
        }
        
        return -1; // Not found
    }
    
    public static void main(String[] args) {
        int[] arr = {10, 20, 30, 40, 50, 60, 70};
        int target = 40;
        int result = binarySearch(arr, target);
        
        if (result != -1) {
            System.out.println("Element found at index: " + result);
        } else {
            System.out.println("Element not found");
        }
    }
}
```

##### Recursive Approach:
```java
public class BinarySearchRecursive {
    public static int binarySearch(int[] arr, int target, int low, int high) {
        if (low > high) {
            return -1; // Base case: not found
        }
        
        int mid = low + (high - low) / 2;
        
        if (arr[mid] == target) {
            return mid;
        } else if (arr[mid] < target) {
            return binarySearch(arr, target, mid + 1, high);
        } else {
            return binarySearch(arr, target, low, mid - 1);
        }
    }
    
    public static void main(String[] args) {
        int[] arr = {10, 20, 30, 40, 50, 60, 70};
        int target = 40;
        int result = binarySearch(arr, target, 0, arr.length - 1);
        
        if (result != -1) {
            System.out.println("Element found at index: " + result);
        } else {
            System.out.println("Element not found");
        }
    }
}
```

#### Time Complexity:
- **Best Case**: O(1) - Element found at middle
- **Average Case**: O(log n)
- **Worst Case**: O(log n) - Element at extremes or not found

#### Space Complexity:
- **Iterative**: O(1)
- **Recursive**: O(log n) - due to function call stack

#### Advantages:
- Very efficient for large datasets
- Time complexity is O(log n)
- Divide and conquer approach

#### Disadvantages:
- Requires sorted array
- Not suitable for linked lists (no random access)

---

### 3. Ternary Search

#### What is Ternary Search?
Ternary search is a divide-and-conquer algorithm that divides the array into three parts instead of two (like binary search).

#### Prerequisites:
- Array must be sorted
- Works by eliminating 1/3 of the search space in each iteration

#### Algorithm:
1. Set low = 0, high = array.length - 1
2. While low <= high:
   - Calculate mid1 = low + (high - low) / 3
   - Calculate mid2 = high - (high - low) / 3
   - If arr[mid1] == target, return mid1
   - If arr[mid2] == target, return mid2
   - If target < arr[mid1], search in left third
   - If target > arr[mid2], search in right third
   - Otherwise, search in middle third
3. Return -1 if not found

#### Implementation:

##### Iterative Approach:
```java
public class TernarySearch {
    public static int ternarySearch(int[] arr, int target) {
        int low = 0;
        int high = arr.length - 1;
        
        while (low <= high) {
            int mid1 = low + (high - low) / 3;
            int mid2 = high - (high - low) / 3;
            
            if (arr[mid1] == target) {
                return mid1;
            }
            if (arr[mid2] == target) {
                return mid2;
            }
            
            if (target < arr[mid1]) {
                high = mid1 - 1;
            } else if (target > arr[mid2]) {
                low = mid2 + 1;
            } else {
                low = mid1 + 1;
                high = mid2 - 1;
            }
        }
        
        return -1; // Not found
    }
    
    public static void main(String[] args) {
        int[] arr = {10, 20, 30, 40, 50, 60, 70, 80, 90};
        int target = 60;
        int result = ternarySearch(arr, target);
        
        if (result != -1) {
            System.out.println("Element found at index: " + result);
        } else {
            System.out.println("Element not found");
        }
    }
}
```

##### Recursive Approach:
```java
public class TernarySearchRecursive {
    public static int ternarySearch(int[] arr, int target, int low, int high) {
        if (low > high) {
            return -1; // Base case: not found
        }
        
        int mid1 = low + (high - low) / 3;
        int mid2 = high - (high - low) / 3;
        
        if (arr[mid1] == target) {
            return mid1;
        }
        if (arr[mid2] == target) {
            return mid2;
        }
        
        if (target < arr[mid1]) {
            return ternarySearch(arr, target, low, mid1 - 1);
        } else if (target > arr[mid2]) {
            return ternarySearch(arr, target, mid2 + 1, high);
        } else {
            return ternarySearch(arr, target, mid1 + 1, mid2 - 1);
        }
    }
    
    public static void main(String[] args) {
        int[] arr = {10, 20, 30, 40, 50, 60, 70, 80, 90};
        int target = 60;
        int result = ternarySearch(arr, target, 0, arr.length - 1);
        
        if (result != -1) {
            System.out.println("Element found at index: " + result);
        } else {
            System.out.println("Element not found");
        }
    }
}
```

#### Time Complexity:
- **Best Case**: O(1) - Element found at mid1 or mid2
- **Average Case**: O(log₃ n)
- **Worst Case**: O(log₃ n)

#### Space Complexity:
- **Iterative**: O(1)
- **Recursive**: O(log₃ n)

#### Advantages:
- Divides search space into three parts
- Can be faster than binary search in some cases
- Efficient for large datasets

#### Disadvantages:
- More comparisons per iteration than binary search
- Requires sorted array
- In practice, binary search is often preferred

---

### Comparison of Search Algorithms:

| Algorithm | Time Complexity | Space Complexity | Prerequisites | Best For |
|-----------|----------------|------------------|---------------|----------|
| Linear Search | O(n) | O(1) | None | Small/unsorted arrays |
| Binary Search | O(log n) | O(1) iterative | Sorted array | Large sorted arrays |
| Ternary Search | O(log₃ n) | O(1) iterative | Sorted array | Theoretical interest |

### When to Use Each Algorithm:

#### Linear Search:
- Small datasets (< 100 elements)
- Unsorted arrays
- When simplicity is more important than efficiency

#### Binary Search:
- Large sorted datasets
- When O(log n) performance is required
- Most commonly used efficient search algorithm

#### Ternary Search:
- Theoretical scenarios
- When you want to explore divide-and-conquer variants
- Research or educational purposes

### Key Points to Remember:

1. **Linear Search**: Simple but inefficient O(n)
2. **Binary Search**: Most practical and efficient O(log n)
3. **Ternary Search**: Theoretical interest O(log₃ n)
4. **Sorting requirement**: Binary and Ternary need sorted arrays
5. **Space complexity**: All can be implemented with O(1) space
6. **Practical choice**: Binary search is the gold standard for sorted arrays

### Practice Problems:

1. **Find First and Last Position**: Use binary search to find first and last occurrence
2. **Search in Rotated Sorted Array**: Modified binary search
3. **Find Peak Element**: Using ternary search concept
4. **Search in 2D Matrix**: Apply binary search principles
5. **Find Minimum in Rotated Array**: Binary search variant

Day-5 revision

# Sorting Algorithms - Notes

## Overview
Sorting is the process of arranging elements in a particular order (ascending or descending). Here are the main sorting algorithms covered in Day 5 of the curriculum.

---

## 1. Selection Sort

### What is Selection Sort?
Selection Sort repeatedly finds the minimum element from the unsorted portion and places it at the beginning.

### Algorithm:
1. Find the minimum element in the unsorted array
2. Swap it with the first element
3. Move the boundary of unsorted array by one position
4. Repeat until the entire array is sorted

### Implementation:
```java
public class SelectionSort {
    public static void selectionSort(int[] arr) {
        int n = arr.length;
        
        for (int i = 0; i < n - 1; i++) {
            // Find minimum element in remaining unsorted array
            int minIndex = i;
            for (int j = i + 1; j < n; j++) {
                if (arr[j] < arr[minIndex]) {
                    minIndex = j;
                }
            }
            
            // Swap the found minimum element with the first element
            int temp = arr[minIndex];
            arr[minIndex] = arr[i];
            arr[i] = temp;
        }
    }
    
    public static void main(String[] args) {
        int[] arr = {64, 34, 25, 12, 22, 11, 90};
        System.out.println("Original array: " + Arrays.toString(arr));
        
        selectionSort(arr);
        System.out.println("Sorted array: " + Arrays.toString(arr));
    }
}
```

### Time Complexity:
- **Best Case**: O(n²) - Even if array is sorted
- **Average Case**: O(n²)
- **Worst Case**: O(n²)

### Space Complexity: O(1) - In-place sorting

### Characteristics:
- **Stable**: No (relative order of equal elements may change)
- **In-place**: Yes
- **Adaptive**: No (performance doesn't improve for partially sorted arrays)

### Advantages:
- Simple implementation
- Performs well on small datasets
- In-place sorting (constant space)
- Makes minimum number of swaps

### Disadvantages:
- Inefficient for large datasets
- Always O(n²) time complexity

---

## 2. Bubble Sort

### What is Bubble Sort?
Bubble Sort repeatedly steps through the list, compares adjacent elements, and swaps them if they're in the wrong order.

### Algorithm:
1. Compare adjacent elements
2. If they're in wrong order, swap them
3. Continue until no swaps are needed
4. After each pass, largest element "bubbles" to the end

### Implementation:
```java
public class BubbleSort {
    public static void bubbleSort(int[] arr) {
        int n = arr.length;
        
        for (int i = 0; i < n - 1; i++) {
            boolean swapped = false;
            
            // Last i elements are already in place
            for (int j = 0; j < n - i - 1; j++) {
                if (arr[j] > arr[j + 1]) {
                    // Swap arr[j] and arr[j+1]
                    int temp = arr[j];
                    arr[j] = arr[j + 1];
                    arr[j + 1] = temp;
                    swapped = true;
                }
            }
            
            // If no swapping occurred, array is sorted
            if (!swapped) {
                break;
            }
        }
    }
    
    public static void main(String[] args) {
        int[] arr = {64, 34, 25, 12, 22, 11, 90};
        System.out.println("Original array: " + Arrays.toString(arr));
        
        bubbleSort(arr);
        System.out.println("Sorted array: " + Arrays.toString(arr));
    }
}
```

### Time Complexity:
- **Best Case**: O(n) - Already sorted array with optimization
- **Average Case**: O(n²)
- **Worst Case**: O(n²) - Reverse sorted array

### Space Complexity: O(1) - In-place sorting

### Characteristics:
- **Stable**: Yes
- **In-place**: Yes
- **Adaptive**: Yes (with optimization)

### Advantages:
- Simple implementation
- Stable sorting algorithm
- Can detect if array is already sorted
- In-place sorting

### Disadvantages:
- Inefficient for large datasets
- Many unnecessary comparisons
- O(n²) average time complexity

---

## 3. Insertion Sort

### What is Insertion Sort?
Insertion Sort builds the sorted array one element at a time by inserting each element into its correct position.

### Algorithm:
1. Start with second element (assume first is sorted)
2. Compare with elements in sorted portion
3. Insert current element at correct position
4. Repeat for all elements

### Implementation:
```java
public class InsertionSort {
    public static void insertionSort(int[] arr) {
        int n = arr.length;
        
        for (int i = 1; i < n; i++) {
            int key = arr[i];
            int j = i - 1;
            
            // Move elements greater than key one position ahead
            while (j >= 0 && arr[j] > key) {
                arr[j + 1] = arr[j];
                j--;
            }
            
            // Insert key at correct position
            arr[j + 1] = key;
        }
    }
    
    public static void main(String[] args) {
        int[] arr = {64, 34, 25, 12, 22, 11, 90};
        System.out.println("Original array: " + Arrays.toString(arr));
        
        insertionSort(arr);
        System.out.println("Sorted array: " + Arrays.toString(arr));
    }
}
```

### Time Complexity:
- **Best Case**: O(n) - Already sorted array
- **Average Case**: O(n²)
- **Worst Case**: O(n²) - Reverse sorted array

### Space Complexity: O(1) - In-place sorting

### Characteristics:
- **Stable**: Yes
- **In-place**: Yes
- **Adaptive**: Yes (efficient for small/nearly sorted arrays)

### Advantages:
- Simple implementation
- Efficient for small datasets
- Adaptive (performs well on nearly sorted arrays)
- Stable and in-place
- Online algorithm (can sort as it receives data)

### Disadvantages:
- Inefficient for large datasets
- O(n²) average time complexity

---

## 4. Quick Sort

### What is Quick Sort?
Quick Sort is a divide-and-conquer algorithm that picks a pivot element and partitions the array around it.

### Algorithm:
1. Choose a pivot element
2. Partition array so elements < pivot are on left, > pivot on right
3. Recursively apply quicksort to left and right subarrays
4. Combine results

### Implementation:
```java
public class QuickSort {
    public static void quickSort(int[] arr, int low, int high) {
        if (low < high) {
            // Find partition index
            int pivotIndex = partition(arr, low, high);
            
            // Recursively sort elements before and after partition
            quickSort(arr, low, pivotIndex - 1);
            quickSort(arr, pivotIndex + 1, high);
        }
    }
    
    private static int partition(int[] arr, int low, int high) {
        int pivot = arr[high]; // Choose last element as pivot
        int i = low - 1; // Index of smaller element
        
        for (int j = low; j < high; j++) {
            if (arr[j] <= pivot) {
                i++;
                // Swap arr[i] and arr[j]
                int temp = arr[i];
                arr[i] = arr[j];
                arr[j] = temp;
            }
        }
        
        // Swap arr[i+1] and arr[high] (pivot)
        int temp = arr[i + 1];
        arr[i + 1] = arr[high];
        arr[high] = temp;
        
        return i + 1;
    }
    
    public static void main(String[] args) {
        int[] arr = {64, 34, 25, 12, 22, 11, 90};
        System.out.println("Original array: " + Arrays.toString(arr));
        
        quickSort(arr, 0, arr.length - 1);
        System.out.println("Sorted array: " + Arrays.toString(arr));
    }
}
```

### Time Complexity:
- **Best Case**: O(n log n) - Pivot divides array into equal halves
- **Average Case**: O(n log n)
- **Worst Case**: O(n²) - Pivot is always smallest/largest element

### Space Complexity: O(log n) - Due to recursion stack

### Characteristics:
- **Stable**: No (depends on partition implementation)
- **In-place**: Yes
- **Adaptive**: No

### Advantages:
- Very efficient on average O(n log n)
- In-place sorting
- Widely used in practice
- Cache-friendly

### Disadvantages:
- Worst-case O(n²) time complexity
- Not stable
- Performance depends on pivot selection

---

## 5. Merge Sort

### What is Merge Sort?
Merge Sort is a divide-and-conquer algorithm that divides the array into halves, sorts them, and then merges them back.

### Algorithm:
1. Divide array into two halves
2. Recursively sort both halves
3. Merge the sorted halves

### Implementation:
```java
public class MergeSort {
    public static void mergeSort(int[] arr, int left, int right) {
        if (left < right) {
            int mid = left + (right - left) / 2;
            
            // Sort first and second halves
            mergeSort(arr, left, mid);
            mergeSort(arr, mid + 1, right);
            
            // Merge the sorted halves
            merge(arr, left, mid, right);
        }
    }
    
    private static void merge(int[] arr, int left, int mid, int right) {
        // Create temp arrays for left and right subarrays
        int n1 = mid - left + 1;
        int n2 = right - mid;
        
        int[] leftArr = new int[n1];
        int[] rightArr = new int[n2];
        
        // Copy data to temp arrays
        for (int i = 0; i < n1; i++) {
            leftArr[i] = arr[left + i];
        }
        for (int j = 0; j < n2; j++) {
            rightArr[j] = arr[mid + 1 + j];
        }
        
        // Merge the temp arrays back into arr[left..right]
        int i = 0, j = 0, k = left;
        
        while (i < n1 && j < n2) {
            if (leftArr[i] <= rightArr[j]) {
                arr[k] = leftArr[i];
                i++;
            } else {
                arr[k] = rightArr[j];
                j++;
            }
            k++;
        }
        
        // Copy remaining elements
        while (i < n1) {
            arr[k] = leftArr[i];
            i++;
            k++;
        }
        
        while (j < n2) {
            arr[k] = rightArr[j];
            j++;
            k++;
        }
    }
    
    public static void main(String[] args) {
        int[] arr = {64, 34, 25, 12, 22, 11, 90};
        System.out.println("Original array: " + Arrays.toString(arr));
        
        mergeSort(arr, 0, arr.length - 1);
        System.out.println("Sorted array: " + Arrays.toString(arr));
    }
}
```

### Time Complexity:
- **Best Case**: O(n log n)
- **Average Case**: O(n log n)
- **Worst Case**: O(n log n) - Consistent performance

### Space Complexity: O(n) - Requires additional space for temporary arrays

### Characteristics:
- **Stable**: Yes
- **In-place**: No
- **Adaptive**: No

### Advantages:
- Guaranteed O(n log n) time complexity
- Stable sorting algorithm
- Predictable performance
- Good for large datasets
- Parallelizable

### Disadvantages:
- Requires O(n) extra space
- Not in-place
- Slower than quicksort in practice for smaller arrays

---

## Comparison of Sorting Algorithms

| Algorithm | Best Case | Average Case | Worst Case | Space | Stable | In-place |
|-----------|-----------|--------------|------------|-------|--------|----------|
| Selection Sort| O(n²) | O(n²)         | O(n²)       | O(1) |    No |    Yes |
| Bubble Sort | O(n)    | O(n²)         | O(n²)        | O(1) |    Yes | Yes |
| Insertion Sort | O(n) | O(n²)         | O(n²)         | O(1) |    Yes | Yes |
| Quick Sort | O(n log n) | O(n log n) |   O(n²)       | O(log n) | No | Yes |
| Merge Sort | O(n log n) | O(n log n) | O(n log n) | O(n) | Yes | No |

## When to Use Each Algorithm

### Selection Sort:
- Small datasets
- When memory is limited
- When you need to minimize swaps

### Bubble Sort:
- Educational purposes
- Very small datasets
- When you need stable sort and simplicity

### Insertion Sort:
- Small datasets (< 50 elements)
- Nearly sorted arrays
- Online sorting (data arrives over time)

### Quick Sort:
- Large datasets
- When average-case performance is important
- When memory is limited (in-place)

### Merge Sort:
- Large datasets
- When you need guaranteed O(n log n) performance
- When stability is required
- External sorting (when data doesn't fit in memory)

## Key Points to Remember

1. **Simple sorts (Selection, Bubble, Insertion)**: O(n²) but good for small data
2. **Quick Sort**: Fast average case O(n log n), but O(n²) worst case
3. **Merge Sort**: Consistent O(n log n) but requires extra space
4. **Stability**: Merge, Bubble, Insertion are stable
5. **In-place**: Selection, Bubble, Insertion, Quick are in-place
6. **Adaptive**: Bubble and Insertion perform better on nearly sorted data

## Practice Problems

1. **Implement all sorting algorithms**
2. **Sort colors (Dutch National Flag)**
3. **Sort array with limited range**
4. **Merge two sorted arrays**
5. **Find Kth largest element**
6. **Sort array of 0s, 1s, and 2s**
7. **Implement custom comparator sorting**

This comprehensive guide covers all sorting algorithms from your Day 5 curriculum with detailed implementations and comparisons!

Day -6
## Bucket Sort and Radix Sort - Notes

### 1. Bucket Sort

#### What is Bucket Sort?
Bucket Sort is a sorting algorithm that distributes elements into a number of buckets, sorts each bucket individually, and then concatenates the sorted buckets to get the final sorted array.

#### When to Use Bucket Sort:
- When input is uniformly distributed over a range
- When you know the range of input values
- Best for floating-point numbers in [0, 1) range

#### Algorithm:
1. Create n empty buckets
2. Distribute array elements into buckets based on their values
3. Sort each bucket individually (using any sorting algorithm)
4. Concatenate all sorted buckets

#### Implementation:
```java
import java.util.*;

public class BucketSort {
    public static void bucketSort(float[] arr) {
        int n = arr.length;
        
        // Create empty buckets
        List<Float>[] buckets = new ArrayList[n];
        for (int i = 0; i < n; i++) {
            buckets[i] = new ArrayList<>();
        }
        
        // Put array elements into buckets
        for (int i = 0; i < n; i++) {
            int bucketIndex = (int) (n * arr[i]);
            buckets[bucketIndex].add(arr[i]);
        }
        
        // Sort individual buckets
        for (int i = 0; i < n; i++) {
            Collections.sort(buckets[i]);
        }
        
        // Concatenate all buckets
        int index = 0;
        for (int i = 0; i < n; i++) {
            for (float value : buckets[i]) {
                arr[index++] = value;
            }
        }
    }
    
    // For integer arrays with known range
    public static void bucketSortIntegers(int[] arr, int maxValue) {
        int n = arr.length;
        
        // Create buckets
        List<Integer>[] buckets = new ArrayList[n];
        for (int i = 0; i < n; i++) {
            buckets[i] = new ArrayList<>();
        }
        
        // Distribute elements into buckets
        for (int i = 0; i < n; i++) {
            int bucketIndex = (arr[i] * n) / (maxValue + 1);
            buckets[bucketIndex].add(arr[i]);
        }
        
        // Sort each bucket
        for (int i = 0; i < n; i++) {
            Collections.sort(buckets[i]);
        }
        
        // Merge buckets back
        int index = 0;
        for (int i = 0; i < n; i++) {
            for (int value : buckets[i]) {
                arr[index++] = value;
            }
        }
    }
    
    public static void main(String[] args) {
        float[] arr = {0.42f, 0.32f, 0.23f, 0.52f, 0.25f, 0.47f, 0.51f};
        System.out.println("Original array: " + Arrays.toString(arr));
        
        bucketSort(arr);
        System.out.println("Sorted array: " + Arrays.toString(arr));
    }
}
```

#### Time Complexity:
- **Best Case**: O(n + k) where k is the number of buckets
- **Average Case**: O(n + k)
- **Worst Case**: O(n²) when all elements go to one bucket

#### Space Complexity: O(n + k)

#### Characteristics:
- **Stable**: Yes (depends on sorting algorithm used for buckets)
- **In-place**: No
- **Adaptive**: No

#### Advantages:
- Very efficient when input is uniformly distributed
- Can achieve O(n) time complexity in best case
- Stable sorting algorithm
- Good for sorting floating-point numbers

#### Disadvantages:
- Requires knowledge of input range
- Performance depends on input distribution
- Uses extra space for buckets
- Not suitable for general-purpose sorting

---

### 2. Radix Sort

#### What is Radix Sort?
Radix Sort is a non-comparative sorting algorithm that sorts numbers by processing individual digits. It uses counting sort as a subroutine to sort digits.

#### Types of Radix Sort:
1. **LSD (Least Significant Digit)**: Sorts from rightmost digit to leftmost
2. **MSD (Most Significant Digit)**: Sorts from leftmost digit to rightmost

#### Algorithm (LSD):
1. Find the maximum number to determine number of digits
2. For each digit position (from least to most significant):
   - Use counting sort to sort array based on current digit
3. Repeat until all digits are processed

#### Implementation:
```java
public class RadixSort {
    
    // Main radix sort function
    public static void radixSort(int[] arr) {
        // Find maximum number to determine number of digits
        int max = getMax(arr);
        
        // Do counting sort for every digit
        // exp is 10^i where i is current digit number
        for (int exp = 1; max / exp > 0; exp *= 10) {
            countingSort(arr, exp);
        }
    }
    
    // Find maximum element in array
    private static int getMax(int[] arr) {
        int max = arr[0];
        for (int i = 1; i < arr.length; i++) {
            if (arr[i] > max) {
                max = arr[i];
            }
        }
        return max;
    }
    
    // Counting sort based on digit represented by exp
    private static void countingSort(int[] arr, int exp) {
        int n = arr.length;
        int[] output = new int[n];
        int[] count = new int[10]; // For digits 0-9
        
        // Initialize count array
        Arrays.fill(count, 0);
        
        // Store count of occurrences of each digit
        for (int i = 0; i < n; i++) {
            count[(arr[i] / exp) % 10]++;
        }
        
        // Change count[i] so it contains actual position of digit
        for (int i = 1; i < 10; i++) {
            count[i] += count[i - 1];
        }
        
        // Build output array
        for (int i = n - 1; i >= 0; i--) {
            output[count[(arr[i] / exp) % 10] - 1] = arr[i];
            count[(arr[i] / exp) % 10]--;
        }
        
        // Copy output array to arr
        for (int i = 0; i < n; i++) {
            arr[i] = output[i];
        }
    }
    
    // Radix sort for strings
    public static void radixSortStrings(String[] arr) {
        if (arr.length == 0) return;
        
        // Find maximum length
        int maxLength = 0;
        for (String str : arr) {
            maxLength = Math.max(maxLength, str.length());
        }
        
        // Sort from rightmost character to leftmost
        for (int pos = maxLength - 1; pos >= 0; pos--) {
            countingSortStrings(arr, pos);
        }
    }
    
    private static void countingSortStrings(String[] arr, int pos) {
        int n = arr.length;
        String[] output = new String[n];
        int[] count = new int[256]; // For ASCII characters
        
        Arrays.fill(count, 0);
        
        // Count occurrences of each character
        for (String str : arr) {
            int charIndex = (pos < str.length()) ? str.charAt(pos) : 0;
            count[charIndex]++;
        }
        
        // Change count[i] to actual position
        for (int i = 1; i < 256; i++) {
            count[i] += count[i - 1];
        }
        
        // Build output array
        for (int i = n - 1; i >= 0; i--) {
            int charIndex = (pos < arr[i].length()) ? arr[i].charAt(pos) : 0;
            output[count[charIndex] - 1] = arr[i];
            count[charIndex]--;
        }
        
        // Copy output to original array
        System.arraycopy(output, 0, arr, 0, n);
    }
    
    public static void main(String[] args) {
        int[] arr = {170, 45, 75, 90, 2, 802, 24, 66};
        System.out.println("Original array: " + Arrays.toString(arr));
        
        radixSort(arr);
        System.out.println("Sorted array: " + Arrays.toString(arr));
        
        // String example
        String[] strArr = {"apple", "banana", "cherry", "date", "elderberry"};
        System.out.println("Original strings: " + Arrays.toString(strArr));
        
        radixSortStrings(strArr);
        System.out.println("Sorted strings: " + Arrays.toString(strArr));
    }
}
```

#### Time Complexity:
- **Best Case**: O(d × (n + k)) where d = number of digits, k = range of digits
- **Average Case**: O(d × (n + k))
- **Worst Case**: O(d × (n + k))

#### Space Complexity: O(n + k)

#### Characteristics:
- **Stable**: Yes
- **In-place**: No
- **Adaptive**: No

#### Advantages:
- Linear time complexity O(d × n) for fixed number of digits
- Stable sorting algorithm
- Good for sorting integers and strings
- No comparisons needed

#### Disadvantages:
- Only works for integers, strings, or fixed-length keys
- Uses extra space
- Performance depends on number of digits
- Not suitable for general-purpose sorting

---

## Comparison: Bucket Sort vs Radix Sort

| Aspect | Bucket Sort | Radix Sort |
|--------|-------------|------------|
| **Type** | Distribution Sort | Non-comparative Sort |
| **Best For** | Uniformly distributed data | Integers, strings |
| **Time Complexity** | O(n + k) average | O(d × (n + k)) |
| **Space Complexity** | O(n + k) | O(n + k) |
| **Stability** | Yes | Yes |
| **Input Requirements** | Known range | Fixed-length keys |
| **Use Cases** | Float numbers [0,1) | Integers, strings |

## Advanced Variations:

### 1. **Counting Sort (Used in Radix Sort)**
```java
public static void countingSort(int[] arr, int range) {
    int n = arr.length;
    int[] count = new int[range + 1];
    int[] output = new int[n];
    
    // Count occurrences
    for (int i = 0; i < n; i++) {
        count[arr[i]]++;
    }
    
    // Cumulative count
    for (int i = 1; i <=
    range; i++) {
        count[i] += count[i - 1];
        }

        // Build output array
        for (int i = n - 1; i >= 0; i--) {
        output[count[arr[i]] - 1] = arr[i];
        count[arr[i]]--;
        }

        // Copy output to original array
        for (int i = 0; i < n; i++) {
        arr[i] = output[i];
        }
    }
```

#### Characteristics:
- **Stable**: Yes
- **In-place**: No
- **Time Complexity**: O(n + k)
- **Space Complexity**: O(n + k)

---

## Summary

- **Bucket Sort** and **Radix Sort** are efficient for specific types of data and can outperform comparison-based sorts in those cases.
- Both require extra space and have input constraints.
- **Counting Sort** is a key building block for Radix Sort and is also efficient for small ranges.

---
Day 7 
## Stack Data Structure - Notes

### What is a Stack?
A **Stack** is a linear data structure that follows the **LIFO (Last In, First Out)** principle. Think of it like a stack of plates - you can only add or remove plates from the top.

### Key Characteristics:
- **LIFO Principle**: The last element added is the first one to be removed
- **Single Point of Access**: All operations happen at the top of the stack
- **Dynamic Size**: Can grow and shrink during runtime

### Basic Stack Operations:

#### 1. **Push**
- Adds an element to the top of the stack
- Time Complexity: O(1)
```java
stack.push(element);
```

#### 2. **Pop**
- Removes and returns the top element
- Time Complexity: O(1)
- Throws exception if stack is empty
```java
element = stack.pop();
```

#### 3. **Peek/Top**
- Returns the top element without removing it
- Time Complexity: O(1)
```java
element = stack.peek();
```

#### 4. **isEmpty**
- Checks if the stack is empty
- Time Complexity: O(1)
```java
boolean empty = stack.isEmpty();
```

#### 5. **Size**
- Returns the number of elements in the stack
- Time Complexity: O(1)
```java
int size = stack.size();
```

### Stack Implementation in Java:

#### 1. **Using Java Collections Framework**
```java
import java.util.Stack;

Stack<Integer> stack = new Stack<>();
stack.push(10);
stack.push(20);
stack.push(30);

System.out.println(stack.pop()); // Output: 30
System.out.println(stack.peek()); // Output: 20
```

#### 2. **Using ArrayList**
```java
import java.util.ArrayList;

ArrayList<Integer> stack = new ArrayList<>();
// Push operation
stack.add(element);
// Pop operation
stack.remove(stack.size() - 1);
// Peek operation
stack.get(stack.size() - 1);
```

#### 3. **Array-based Implementation**
```java
class ArrayStack {
    private int[] arr;
    private int top;
    private int capacity;
    
    public ArrayStack(int size) {
        arr = new int[size];
        capacity = size;
        top = -1;
    }
    
    public void push(int x) {
        if (top == capacity - 1) {
            throw new RuntimeException("Stack Overflow");
        }
        arr[++top] = x;
    }
    
    public int pop() {
        if (isEmpty()) {
            throw new RuntimeException("Stack Underflow");
        }
        return arr[top--];
    }
    
    public int peek() {
        if (isEmpty()) {
            throw new RuntimeException("Stack is empty");
        }
        return arr[top];
    }
    
    public boolean isEmpty() {
        return top == -1;
    }
}
```

### Real-World Applications:

#### 1. **Function Call Management**
- Method calls are stored in a call stack
- When a method is called, it's pushed onto the stack
- When it returns, it's popped from the stack

#### 2. **Expression Evaluation**
- **Infix to Postfix conversion**
- **Postfix expression evaluation**
- **Parentheses matching**

#### 3. **Undo Operations**
- Text editors use stacks for undo functionality
- Each action is pushed onto the stack
- Undo pops the last action

#### 4. **Browser History**
- Back button functionality
- Each visited page is pushed onto the stack

### Common Stack Problems:

#### 1. **Balanced Parentheses**
```java
public boolean isBalanced(String str) {
    Stack<Character> stack = new Stack<>();
    
    for (char c : str.toCharArray()) {
        if (c == '(' || c == '[' || c == '{') {
            stack.push(c);
        } else if (c == ')' || c == ']' || c == '}') {
            if (stack.isEmpty()) return false;
            
            char top = stack.pop();
            if (!isMatchingPair(top, c)) return false;
        }
    }
    
    return stack.isEmpty();
}
```

#### 2. **Postfix Expression Evaluation**
```java
public int evaluatePostfix(String expression) {
    Stack<Integer> stack = new Stack<>();
    
    for (char c : expression.toCharArray()) {
        if (Character.isDigit(c)) {
            stack.push(c - '0');
        } else {
            int operand2 = stack.pop();
            int operand1 = stack.pop();
            
            switch (c) {
                case '+': stack.push(operand1 + operand2); break;
                case '-': stack.push(operand1 - operand2); break;
                case '*': stack.push(operand1 * operand2); break;
                case '/': stack.push(operand1 / operand2); break;
            }
        }
    }
    
    return stack.pop();
}
```

### Stack vs Other Data Structures:

| Feature | Stack | Queue | Array | Linked List |
|---------|-------|-------|-------|-------------|
| Access Pattern | LIFO | FIFO | Random | Sequential |
| Insertion | Top only | Rear only | Any position | Any position |
| Deletion | Top only | Front only | Any position | Any position |
| Time Complexity | O(1) | O(1) | O(1) for access | O(n) for access |

### Advantages:
- **Simple Implementation**: Easy to understand and implement
- **Efficient Operations**: All basic operations are O(1)
- **Memory Efficient**: No extra memory overhead
- **Automatic Memory Management**: In dynamic implementations

### Disadvantages:
- **Limited Access**: Can only access the top element
- **No Random Access**: Cannot access elements in the middle
- **Stack Overflow**: Fixed-size implementations can overflow

### Time & Space Complexity:
- **Push**: O(1)
- **Pop**: O(1)
- **Peek**: O(1)
- **isEmpty**: O(1)
- **Space Complexity**: O(n) where n is the number of elements

### Key Points to Remember:
1. Stack is a **LIFO** data structure
2. All operations happen at the **top** of the stack
3. **Push** adds elements, **Pop** removes elements
4. Always check for **empty stack** before popping
5. Useful for **recursive problems** and **expression evaluation**
6. Java provides built-in `Stack` class in `java.util` package

## Queue Data Structure - Notes

### What is a Queue?
A **Queue** is a linear data structure that follows the **FIFO (First In, First Out)** principle. Think of it like a line of people waiting - the first person to join the line is the first one to be served.

### Key Characteristics:
- **FIFO Principle**: The first element added is the first one to be removed
- **Two Points of Access**: Insertion at rear, deletion from front
- **Dynamic Size**: Can grow and shrink during runtime

### Basic Queue Operations:

#### 1. **Enqueue**
- Adds an element to the rear of the queue
- Time Complexity: O(1)
```java
queue.offer(element);
// or
queue.add(element);
```

#### 2. **Dequeue**
- Removes and returns the front element
- Time Complexity: O(1)
- Returns null if queue is empty (poll) or throws exception (remove)
```java
element = queue.poll(); // Returns null if empty
element = queue.remove(); // Throws exception if empty
```

#### 3. **Front/Peek**
- Returns the front element without removing it
- Time Complexity: O(1)
```java
element = queue.peek(); // Returns null if empty
element = queue.element(); // Throws exception if empty
```

#### 4. **isEmpty**
- Checks if the queue is empty
- Time Complexity: O(1)
```java
boolean empty = queue.isEmpty();
```

#### 5. **Size**
- Returns the number of elements in the queue
- Time Complexity: O(1)
```java
int size = queue.size();
```

### Queue Implementation in Java:

#### 1. **Using Java Collections Framework**
```java
import java.util.Queue;
import java.util.LinkedList;

Queue<Integer> queue = new LinkedList<>();
queue.offer(10);
queue.offer(20);
queue.offer(30);

System.out.println(queue.poll()); // Output: 10
System.out.println(queue.peek()); // Output: 20
```

#### 2. **Using ArrayDeque**
```java
import java.util.ArrayDeque;
import java.util.Queue;

Queue<Integer> queue = new ArrayDeque<>();
queue.offer(10);
queue.offer(20);
queue.offer(30);
```

#### 3. **Array-based Implementation (Circular Queue)**
```java
class ArrayQueue {
    private int[] arr;
    private int front;
    private int rear;
    private int size;
    private int capacity;
    
    public ArrayQueue(int capacity) {
        this.capacity = capacity;
        arr = new int[capacity];
        front = 0;
        rear = -1;
        size = 0;
    }
    
    public void enqueue(int x) {
        if (size == capacity) {
            throw new RuntimeException("Queue Overflow");
        }
        rear = (rear + 1) % capacity;
        arr[rear] = x;
        size++;
    }
    
    public int dequeue() {
        if (isEmpty()) {
            throw new RuntimeException("Queue Underflow");
        }
        int element = arr[front];
        front = (front + 1) % capacity;
        size--;
        return element;
    }
    
    public int peek() {
        if (isEmpty()) {
            throw new RuntimeException("Queue is empty");
        }
        return arr[front];
    }
    
    public boolean isEmpty() {
        return size == 0;
    }
    
    public int size() {
        return size;
    }
}
```

#### 4. **Linked List Implementation**
```java
class Node {
    int data;
    Node next;
    
    Node(int data) {
        this.data = data;
        this.next = null;
    }
}

class LinkedQueue {
    private Node front;
    private Node rear;
    private int size;
    
    public LinkedQueue() {
        front = null;
        rear = null;
        size = 0;
    }
    
    public void enqueue(int x) {
        Node newNode = new Node(x);
        if (rear == null) {
            front = rear = newNode;
        } else {
            rear.next = newNode;
            rear = newNode;
        }
        size++;
    }
    
    public int dequeue() {
        if (isEmpty()) {
            throw new RuntimeException("Queue Underflow");
        }
        int element = front.data;
        front = front.next;
        if (front == null) {
            rear = null;
        }
        size--;
        return element;
    }
    
    public int peek() {
        if (isEmpty()) {
            throw new RuntimeException("Queue is empty");
        }
        return front.data;
    }
    
    public boolean isEmpty() {
        return front == null;
    }
    
    public int size() {
        return size;
    }
}
```

### Types of Queues:

#### 1. **Simple Queue**
- Basic FIFO queue
- Insertion at rear, deletion from front

#### 2. **Circular Queue**
- Uses circular array to optimize space
- Rear wraps around to beginning when it reaches end

#### 3. **Priority Queue**
- Elements have priorities
- Higher priority elements are dequeued first
```java
import java.util.PriorityQueue;

PriorityQueue<Integer> pq = new PriorityQueue<>();
pq.offer(30);
pq.offer(10);
pq.offer(20);
System.out.println(pq.poll()); // Output: 10 (lowest priority first)
```

#### 4. **Double-Ended Queue (Deque)**
- Insertion and deletion at both ends
```java
import java.util.Deque;
import java.util.ArrayDeque;

Deque<Integer> deque = new ArrayDeque<>();
deque.offerFirst(10);  // Add to front
deque.offerLast(20);   // Add to rear
deque.pollFirst();     // Remove from front
deque.pollLast();      // Remove from rear
```

### Real-World Applications:

#### 1. **CPU Scheduling**
- Process scheduling in operating systems
- Round-robin scheduling algorithm

#### 2. **Print Queue**
- Managing print jobs in printers
- First submitted job gets printed first

#### 3. **BFS (Breadth-First Search)**
- Graph traversal algorithm
- Level-order traversal in trees

#### 4. **Buffer for Data Streams**
- Keyboard buffer
- Network packet buffering

#### 5. **Call Center Systems**
- Managing incoming calls
- First caller gets served first

### Common Queue Problems:

#### 1. **Implement Stack using Queues**
```java
class MyStack {
    private Queue<Integer> q1;
    private Queue<Integer> q2;
    
    public MyStack() {
        q1 = new LinkedList<>();
        q2 = new LinkedList<>();
    }
    
    public void push(int x) {
        q2.offer(x);
        while (!q1.isEmpty()) {
            q2.offer(q1.poll());
        }
        Queue<Integer> temp = q1;
        q1 = q2;
        q2 = temp;
    }
    
    public int pop() {
        return q1.poll();
    }
    
    public int top() {
        return q1.peek();
    }
    
    public boolean empty() {
        return q1.isEmpty();
    }
}
```

#### 2. **Generate Binary Numbers**
```java
public void generateBinary(int n) {
    Queue<String> queue = new LinkedList<>();
    queue.offer("1");
    
    for (int i = 0; i < n; i++) {
        String current = queue.poll();
        System.out.println(current);
        
        queue.offer(current + "0");
        queue.offer(current + "1");
    }
}
```

#### 3. **Level Order Traversal**
```java
public void levelOrder(TreeNode root) {
    if (root == null) return;
    
    Queue<TreeNode> queue = new LinkedList<>();
    queue.offer(root);
    
    while (!queue.isEmpty()) {
        TreeNode current = queue.poll();
        System.out.print(current.val + " ");
        
        if (current.left != null) queue.offer(current.left);
        if (current.right != null) queue.offer(current.right);
    }
}
```

### Queue vs Other Data Structures:

| Feature | Queue | Stack | Array | Linked List |
|---------|-------|-------|-------|-------------|
| Access Pattern | FIFO | LIFO | Random | Sequential |
| Insertion | Rear only | Top only | Any position | Any position |
| Deletion | Front only | Top only | Any position | Any position |
| Time Complexity | O(1) | O(1) | O(1) for access | O(n) for access |

### Advantages:
- **Fair Processing**: First come, first served
- **Efficient Operations**: All basic operations are O(1)
- **Natural Flow**: Matches real-world scenarios
- **Memory Efficient**: Linked list implementation grows dynamically

### Disadvantages:
- **Limited Access**: Can only access front element
- **No Random Access**: Cannot access middle elements
- **Memory Overhead**: Linked list implementation has pointer overhead

### Time & Space Complexity:
- **Enqueue**: O(1)
- **Dequeue**: O(1)
- **Peek**: O(1)
- **isEmpty**: O(1)
- **Space Complexity**: O(n) where n is the number of elements

### Array vs Linked List Implementation:

| Aspect | Array Implementation | Linked List Implementation |
|--------|---------------------|---------------------------|
| Memory | Fixed size | Dynamic size |
| Space | O(n) | O(n) + pointer overhead |
| Cache Performance | Better | Worse |
| Implementation | More complex (circular) | Simpler |

### Key Points to Remember:
1. Queue is a **FIFO** data structure
2. **Enqueue** adds to rear, **Dequeue** removes from front
3. Use **LinkedList** or **ArrayDeque** in Java
4. **Circular queue** prevents memory wastage in array implementation
5. Essential for **BFS** and **level-order traversal**
6. Java provides **Queue** interface with multiple implementations
7. **PriorityQueue** is useful for scheduling algorithms
8. **Deque** provides flexibility for both ends operations

Day -8 
## Linked List Data Structure - Notes

### What is a Linked List?
A **Linked List** is a linear data structure where elements are stored in nodes, and each node contains data and a reference (or link) to the next node in the sequence. Unlike arrays, linked list elements are not stored in contiguous memory locations.

### Key Characteristics:
- **Dynamic Size**: Can grow or shrink during runtime
- **Non-contiguous Memory**: Elements stored anywhere in memory
- **Sequential Access**: Must traverse from head to reach any element
- **No Random Access**: Cannot directly access elements by index

---

## 1. Singly Linked List

### Structure:
Each node contains:
- **Data**: The actual value
- **Next**: Reference to the next node

### Implementation:
```java
class Node {
    int data;
    Node next;
    
    Node(int data) {
        this.data = data;
        this.next = null;
    }
}

class SinglyLinkedList {
    private Node head;
    
    public SinglyLinkedList() {
        this.head = null;
    }
    
    // Insert at beginning
    public void insertAtBeginning(int data) {
        Node newNode = new Node(data);
        newNode.next = head;
        head = newNode;
    }
    
    // Insert at end
    public void insertAtEnd(int data) {
        Node newNode = new Node(data);
        
        if (head == null) {
            head = newNode;
            return;
        }
        
        Node current = head;
        while (current.next != null) {
            current = current.next;
        }
        current.next = newNode;
    }
    
    // Insert at specific position
    public void insertAtPosition(int data, int position) {
        if (position == 0) {
            insertAtBeginning(data);
            return;
        }
        
        Node newNode = new Node(data);
        Node current = head;
        
        for (int i = 0; i < position - 1 && current != null; i++) {
            current = current.next;
        }
        
        if (current == null) {
            throw new IndexOutOfBoundsException("Position out of bounds");
        }
        
        newNode.next = current.next;
        current.next = newNode;
    }
    
    // Delete from beginning
    public void deleteFromBeginning() {
        if (head == null) return;
        head = head.next;
    }
    
    // Delete from end
    public void deleteFromEnd() {
        if (head == null) return;
        
        if (head.next == null) {
            head = null;
            return;
        }
        
        Node current = head;
        while (current.next.next != null) {
            current = current.next;
        }
        current.next = null;
    }
    
    // Delete specific value
    public void deleteValue(int value) {
        if (head == null) return;
        
        if (head.data == value) {
            head = head.next;
            return;
        }
        
        Node current = head;
        while (current.next != null && current.next.data != value) {
            current = current.next;
        }
        
        if (current.next != null) {
            current.next = current.next.next;
        }
    }
    
    // Search for value
    public boolean search(int value) {
        Node current = head;
        while (current != null) {
            if (current.data == value) {
                return true;
            }
            current = current.next;
        }
        return false;
    }
    
    // Get size
    public int size() {
        int count = 0;
        Node current = head;
        while (current != null) {
            count++;
            current = current.next;
        }
        return count;
    }
    
    // Forward traversal
    public void display() {
        Node current = head;
        while (current != null) {
            System.out.print(current.data + " -> ");
            current = current.next;
        }
        System.out.println("null");
    }
    
    // Reverse the linked list
    public void reverse() {
        Node prev = null;
        Node current = head;
        Node next = null;
        
        while (current != null) {
            next = current.next;
            current.next = prev;
            prev = current;
            current = next;
        }
        
        head = prev;
    }
}
```

### Time Complexity:
- **Insertion**: O(1) at beginning, O(n) at end/position
- **Deletion**: O(1) at beginning, O(n) at end/specific value
- **Search**: O(n)
- **Traversal**: O(n)

---

## 2. Doubly Linked List

### Structure:
Each node contains:
- **Data**: The actual value
- **Next**: Reference to the next node
- **Prev**: Reference to the previous node

### Implementation:
```java
class DoublyNode {
    int data;
    DoublyNode next;
    DoublyNode prev;
    
    DoublyNode(int data) {
        this.data = data;
        this.next = null;
        this.prev = null;
    }
}

class DoublyLinkedList {
    private DoublyNode head;
    private DoublyNode tail;
    
    public DoublyLinkedList() {
        this.head = null;
        this.tail = null;
    }
    
    // Insert at beginning
    public void insertAtBeginning(int data) {
        DoublyNode newNode = new DoublyNode(data);
        
        if (head == null) {
            head = tail = newNode;
        } else {
            newNode.next = head;
            head.prev = newNode;
            head = newNode;
        }
    }
    
    // Insert at end
    public void insertAtEnd(int data) {
        DoublyNode newNode = new DoublyNode(data);
        
        if (tail == null) {
            head = tail = newNode;
        } else {
            tail.next = newNode;
            newNode.prev = tail;
            tail = newNode;
        }
    }
    
    // Insert at position
    public void insertAtPosition(int data, int position) {
        if (position == 0) {
            insertAtBeginning(data);
            return;
        }
        
        DoublyNode newNode = new DoublyNode(data);
        DoublyNode current = head;
        
        for (int i = 0; i < position && current != null; i++) {
            current = current.next;
        }
        
        if (current == null) {
            insertAtEnd(data);
            return;
        }
        
        newNode.next = current;
        newNode.prev = current.prev;
        current.prev.next = newNode;
        current.prev = newNode;
    }
    
    // Delete from beginning
    public void deleteFromBeginning() {
        if (head == null) return;
        
        if (head == tail) {
            head = tail = null;
        } else {
            head = head.next;
            head.prev = null;
        }
    }
    
    // Delete from end
    public void deleteFromEnd() {
        if (tail == null) return;
        
        if (head == tail) {
            head = tail = null;
        } else {
            tail = tail.prev;
            tail.next = null;
        }
    }
    
    // Delete specific value
    public void deleteValue(int value) {
        DoublyNode current = head;
        
        while (current != null && current.data != value) {
            current = current.next;
        }
        
        if (current == null) return;
        
        if (current == head) {
            deleteFromBeginning();
        } else if (current == tail) {
            deleteFromEnd();
        } else {
            current.prev.next = current.next;
            current.next.prev = current.prev;
        }
    }
    
    // Forward traversal
    public void displayForward() {
        DoublyNode current = head;
        while (current != null) {
            System.out.print(current.data + " <-> ");
            current = current.next;
        }
        System.out.println("null");
    }
    
    // Backward traversal
    public void displayBackward() {
        DoublyNode current = tail;
        while (current != null) {
            System.out.print(current.data + " <-> ");
            current = current.prev;
        }
        System.out.println("null");
    }
}
```

### Time Complexity:
- **Insertion**: O(1) at beginning/end, O(n) at position
- **Deletion**: O(1) at beginning/end, O(n) for specific value
- **Search**: O(n)
- **Traversal**: O(n)

---

## 3. Circular Singly Linked List

### Structure:
- Last node points to the first node
- No null pointers except when list is empty

### Implementation:
```java
class CircularSinglyLinkedList {
    private Node head;
    
    public CircularSinglyLinkedList() {
        this.head = null;
    }
    
    // Insert at beginning
    public void insertAtBeginning(int data) {
        Node newNode = new Node(data);
        
        if (head == null) {
            head = newNode;
            newNode.next = head;
        } else {
            Node last = head;
            while (last.next != head) {
                last = last.next;
            }
            newNode.next = head;
            last.next = newNode;
            head = newNode;
        }
    }
    
    // Insert at end
    public void insertAtEnd(int data) {
        Node newNode = new Node(data);
        
        if (head == null) {
            head = newNode;
            newNode.next = head;
        } else {
            Node last = head;
            while (last.next != head) {
                last = last.next;
            }
            last.next = newNode;
            newNode.next = head;
        }
    }
    
    // Delete from beginning
    public void deleteFromBeginning() {
        if (head == null) return;
        
        if (head.next == head) {
            head = null;
        } else {
            Node last = head;
            while (last.next != head) {
                last = last.next;
            }
            last.next = head.next;
            head = head.next;
        }
    }
    
    // Delete from end
    public void deleteFromEnd() {
        if (head == null) return;
        
        if (head.next == head) {
            head = null;
        } else {
            Node current = head;
            while (current.next.next != head) {
                current = current.next;
            }
            current.next = head;
        }
    }
    
    // Display circular list
    public void display() {
        if (head == null) return;
        
        Node current = head;
        do {
            System.out.print(current.data + " -> ");
            current = current.next;
        } while (current != head);
        System.out.println("(back to " + head.data + ")");
    }
    
    // Check if list is empty
    public boolean isEmpty() {
        return head == null;
    }
}
```

---

## 4. Circular Doubly Linked List

### Structure:
- Last node's next points to first node
- First node's prev points to last node

### Implementation:
```java
class CircularDoublyLinkedList {
    private DoublyNode head;
    
    public CircularDoublyLinkedList() {
        this.head = null;
    }
    
    // Insert at beginning
    public void insertAtBeginning(int data) {
        DoublyNode newNode = new DoublyNode(data);
        
        if (head == null) {
            head = newNode;
            newNode.next = newNode.prev = newNode;
        } else {
            DoublyNode last = head.prev;
            
            newNode.next = head;
            newNode.prev = last;
            last.next = newNode;
            head.prev = newNode;
            head = newNode;
        }
    }
    
    // Insert at end
    public void insertAtEnd(int data) {
        DoublyNode newNode = new DoublyNode(data);
        
        if (head == null) {
            head = newNode;
            newNode.next = newNode.prev = newNode;
        } else {
            DoublyNode last = head.prev;
            
            newNode.next = head;
            newNode.prev = last;
            last.next = newNode;
            head.prev = newNode;
        }
    }
    
    // Delete from beginning
    public void deleteFromBeginning() {
        if (head == null) return;
        
        if (head.next == head) {
            head = null;
        } else {
            DoublyNode last = head.prev;
            
            last.next = head.next;
            head.next.prev = last;
            head = head.next;
        }
    }
    
    // Delete from end
    public void deleteFromEnd() {
        if (head == null) return;
        
        if (head.next == head) {
            head = null;
        } else {
            DoublyNode last = head.prev;
            
            last.prev.next = head;
            head.prev = last.prev;
        }
    }
    
    // Display forward
    public void displayForward() {
        if (head == null) return;
        
        DoublyNode current = head;
        do {
            System.out.print(current.data + " <-> ");
            current = current.next;
        } while (current != head);
        System.out.println("(back to " + head.data + ")");
    }
    
    // Display backward
    public void displayBackward() {
        if (head == null) return;
        
        DoublyNode current = head.prev;
        do {
            System.out.print(current.data + " <-> ");
            current = current.prev;
        } while (current != head.prev);
        System.out.println("(back to " + head.prev.data + ")");
    }
}
```

---

## Comparison of Linked List Types

| Feature | Singly | Doubly | Circular Singly | Circular Doubly |
|---------|--------|--------|-----------------|-----------------|
| **Memory per Node** | 1 pointer | 2 pointers | 1 pointer | 2 pointers |
| **Traversal Direction** | Forward only | Both directions | Forward only | Both directions |
| **Last Node Points To** | null | null | First node | First node |
| **First Node Points To** | N/A | null | N/A | Last node |
| **Access to Last Node** | O(n) | O(1) | O(n) | O(1) |
| **Memory Usage** | Lowest | Highest | Low | High |

## Common Operations Complexity

| Operation | Singly | Doubly | Circular Singly | Circular Doubly |
|-----------|--------|--------|-----------------|-----------------|
| **Insert at Beginning** | O(1) | O(1) | O(n) | O(1) |
| **Insert at End** | O(n) | O(1) | O(n) | O(1) |
| **Delete from Beginning** | O(1) | O(1) | O(n) | O(1) |
| **Delete from End** | O(n) | O(1) | O(n) | O(1) |
| **Search** | O(n) | O(n) | O(n) | O(n) |
| **Traversal** | O(n) | O(n) | O(n) | O(n) |

## Advanced Operations

### 1. **Merge Two Sorted Lists**
```java
public Node mergeSortedLists(Node l1, Node l2) {
    Node dummy = new Node(0);
    Node current = dummy;
    
    while (l1 != null && l2 != null) {
        if (l1.data <= l2.data) {
            current.next = l1;
            l1 = l1.next;
        } else {
            current.next = l2;
            l2 = l2.next;
        }
        current = current.next;
    }
    
    current.next = (l1 != null) ? l1 : l2;
    return dummy.next;
}
```

### 2. **Detect Cycle in Linked List (Floyd's Algorithm)**
```java
public boolean hasCycle(Node head) {
    if (head == null || head.next == null) return false;
    
    Node

Day -9
Binary Tree
    Definition of Binary Tree:
        - A Binary Tree is 
          a hierarchical data structure 
          in which each node has at most two children, 
          referred to as the left child and the right child.

    Binary Tree Terminologies:
        | Term                     | Description                                                                                           |
        | ------------------------ | ----------------------------------------------------------------------------------------------------- |
        | Node                     | Basic unit containing a data element and links to left and right child nodes.                         |
        | Root                     | The topmost node in the tree. It has no parent.                                                       |
        | Parent                   | A node that has a link to one or more child nodes.                                                    |
        | Child                    | A node that descends from another node (its parent).                                                  |
        | Left Child               | The node connected via the left link of its parent.                                                   |
        | Right Child              | The node connected via the right link of its parent.                                                  |
        | Leaf Node                | A node with no children (i.e., both left and right links are null).                                   |
        | Subtree                  | A tree formed by any node and its descendants.                                                        |
        | Siblings                 | Nodes that have the same parent.                                                                      |
        | Level                    | Distance from the root node (root is level 0 or 1, depending on convention).                          |
        | Height                   | The length of the longest path from a node to a leaf.                                                 |
        | Depth                    | The number of edges from the root to the node.                                                        |
        | Degree                   | Number of children a node has (0, 1, or 2 in binary tree).                                            |
        | Binary Search Tree (BST) | A binary tree where left child < parent < right child (for all nodes).                                |
        | Complete Binary Tree     | All levels are completely filled except possibly the last, and all nodes are as far left as possible. |
        | Full Binary Tree         | Every node has 0 or 2 children.                                                                       |
        | Perfect Binary Tree      | All internal nodes have two children, and all leaves are at the same level.                           |

    Example

        1. Simple Binary Tree
            ```
                 A
                / \
               B   C
              /     \
             D       E
            ```
            * Root: A
            * Internal Nodes: A, B, C
            * Leaf Nodes: D, E
            * Not a full or complete binary tree.

        2. Complete Binary Tree
            ```
                 1
                / \
               2   3
              / \  /
             4  5 6
            ```

            * All levels are filled except the last, 
              and nodes are as left as possible.
            * Good for array representation (e.g., heaps).

        3. Perfect Binary Tree
            ```
                  1
                /   \
               2     3
              / \   / \
             4   5 6   7
            ```

            * Every internal node has two children.
            * All leaf nodes are at the same level.
            * Height = 2, Number of nodes = 2^(h+1) - 1 = 7

        - Each of these binary trees serves 
          different structural and algorithmic purposes.

    Common Operations on a Binary Tree
        Binary trees support several fundamental operations 
        that are essential in various algorithms and data structures. 
        Here's a breakdown:
        | Operation               | Time Complexity     | Purpose                                                                              |
        | ----------------------- | ------------------- | ------------------------------------------------------------------------------------ |
        | Traversal               | O(n)                | Visit all nodes in a specific order (Inorder, Preorder, Postorder, Level-order).     |
        | Insertion               | O(n)                | Add a node at the next available position (in level-order-style).                    |
        | Deletion                | O(n)                | Remove a node while preserving tree structure.                                       |
        | Search                  | O(n)                | Find a node with a specific value.                                                   |
        | Find Height             | O(n)                | Compute the longest path from root to leaf.                                          |
        | Count Nodes / Leaves    | O(n)                | Count total nodes, or count only leaf nodes (with no children).                      |
        | Mirror / Invert Tree    | O(n)                | Swap left and right children recursively.                                            |
        | Check for Balanced Tree | O(n)                | Check whether height difference between left and right subtree is ≤ 1 for all nodes. |
        | Clone / Copy Tree       | O(n)                | Create an exact copy of the tree.                                                    |

        ---

        > Note: `n` = number of nodes in the tree.
        > Time complexity is linear for most operations because each node may need to be visited once.

Binary Search Tree (BST)
    Definition of Binary Search Tree (BST):
        A Binary Search Tree (BST) is 
        a special type of binary tree where:
            * The left subtree of a node contains 
              only nodes with values less than the node’s value.
            * The right subtree of a node contains only nodes 
              with values greater than the node’s value.
            * Both left and right subtrees must also be Binary Search Trees.

        This property makes BSTs efficient for searching, inserting, and deleting data.

    Terminologies in Binary Search Tree (BST):
        | Term              | Description                                                                   |
        | ----------------- | ----------------------------------------------------------------------------- |
        | Node              | Basic element holding data and links to left and right children.              |
        | Root              | The topmost node in the BST.                                                  |
        | Parent            | A node that has one or more child nodes.                                      |
        | Child             | A node descending from a parent.                                              |
        | Left Child        | The node connected to the left link; holds a smaller value than its parent.   |
        | Right Child       | The node connected to the right link; holds a greater value than its parent.  |
        | Leaf Node         | A node with no children.                                                      |
        | Subtree           | A tree formed from a node and its descendants.                                |
        | Inorder Traversal | Left → Root → Right traversal; returns nodes in sorted (ascending) order.     |
        | Height            | The length of the longest path from a node to a leaf.                         |
        | Depth             | The number of edges from the root to the node.                                |
        | Balanced BST      | A BST where the height difference between left and right subtrees is minimal. |
        | Unbalanced BST    | A BST skewed to one side (left or right), degrading performance to O(n).      |
        | Successor         | The smallest node in the right subtree (used in deletion).                    |
        | Predecessor       | The largest node in the left subtree.                                         |

    Example of a Binary Search Tree:
            ```
                50
                /  \
              30    70
             / \    / \
           20  40  60  80
            ```
        * Node `30` is left of `50`, and all of its descendants (`20`, `40`) are < `50`.
        * Node `70` is right of `50`, and its descendants (`60`, `80`) are > `50`.

    Example 
        Example 1: Simple BST
        ```
              40
             /  \
           20    60
          / \    / \
        10  30  50  70
        ```
        * Left subtree: All values < 40
        * Right subtree: All values > 40
        * Inorder Traversal: `10 20 30 40 50 60 70` (sorted order)

        Example 2: BST Built from [50, 30, 70, 20, 40, 60, 80]
        ```
             50
            /  \
         30    70
        / \    / \
      20  40  60  80
        ```
        * Inserted in the order: 
          root = 50, then 30 (left), 70 (right), and so on 
          maintaining BST property.

        Example 3: Skewed BST (Right-Skewed)
        ```
            10
             \
              20
                \
                30
                  \
                  40
        ```
        * Happens when elements are inserted in increasing order.
        * Worst-case BST (like a linked list), time complexities degrade to O(n).

        Example 4: Skewed BST (Left-Skewed)
        ```
                40
               /
              30
             /
            20
           /
          10
        ```

        * Happens with decreasing order inserts.

    Common Operations on BST:
        | Operation     | Time Complexity (Average Case)     | Purpose                               |
        | ------------- | ---------------------------------- | ------------------------------------- |
        | Search        | O(log n)                           | Find if a value exists                |
        | Insertion     | O(log n)                           | Add a new node                        |
        | Deletion      | O(log n)                           | Remove a node and rearrange           |
        | Traversals    | O(n)                               | Inorder (sorted), Preorder, Postorder |

        > Note: If the BST becomes unbalanced 
          (e.g., like a linked list), 
          time complexity can degrade to O(n).


    Common Operations on a Binary Search Tree (BST)
        A Binary Search Tree allows 
        for efficient searching, insertion, and deletion 
        by maintaining the BST property:
        Left subtree < Node < Right subtree.

        | Operation          | Average Time Complexity     | Worst Case     | Purpose                                             |
        | ------------------ | --------------------------- | -------------- | --------------------------------------------------- |
        | Search             | O(log n)                    | O(n)           | Find whether a value exists in the tree.            |
        | Insertion          | O(log n)                    | O(n)           | Add a new node while maintaining BST property.      |
        | Deletion           | O(log n)                    | O(n)           | Remove a node and rearrange the tree.               |
        | Inorder Traversal  | O(n)                        | O(n)           | Visit nodes in sorted (ascending) order.            |
        | Preorder/Postorder | O(n)                        | O(n)           | Used for copying or deleting the tree.              |
        | Find Min/Max       | O(log n)                    | O(n)           | Go left-most (min) or right-most (max) in the tree. |
        | Find Successor     | O(log n)                    | O(n)           | Find the next higher value (used in deletion).      |
        | Height of Tree     | O(n)                        | O(n)           | Longest path from root to leaf.                     |
        | Is Balanced        | O(n)                        | O(n)           | Check if the tree is height-balanced.               |

        > ⚠ Worst-case happens when the BST becomes skewed (like a linked list), 
             especially with sorted input.

General Tree
    Definition of General Tree:
        - A General Tree is 
        a hierarchical data structure 
        where each node can have any number of child nodes. 
        - Unlike binary trees, 
        which limit children to at most two, 
        general trees are more flexible and 
        are used to represent hierarchies 
        like file systems, company structures, etc.

    Terminologies in a General Tree:
        | Term           | Description                                                                     |
        | -------------- | ------------------------------------------------------------------------------- |
        | Node           | Basic element containing data and links to child nodes.                         |
        | Root           | The topmost node in the tree, without a parent.                                 |
        | Parent         | A node that has one or more child nodes.                                        |
        | Child          | A node that descends from another node (its parent).                            |
        | Siblings       | Nodes that share the same parent.                                               |
        | Leaf Node      | A node with no children.                                                        |
        | Internal Node  | A node that has at least one child.                                             |
        | Subtree        | A tree formed by any node and its descendants.                                  |
        | Level          | The number of edges from the root to the node (root is at level 0 or 1).        |
        | Height         | The maximum level (or depth) of any node in the tree.                           |
        | Depth          | The number of edges from the root to the current node.                          |
        | Degree of Node | Number of children a node has.                                                  |
        | Degree of Tree | The maximum degree of all nodes in the tree.                                    |
        | Ancestor       | Any node in the path from the root to a given node (excluding the node itself). |
        | Descendant     | Any node that comes after a given node in the tree structure.                   |

        - General trees are more abstract and useful 
        when node relationships are not limited to pairs.

    Example 
        1. Company Hierarchy Tree

        ```
                      CEO
                    /  |   \
                CTO   CFO   COO
                / \         |
             Dev1 Dev2     OpsHead
        ```
        * Root: CEO
        * Internal Nodes: CEO, CTO, COO
        * Leaf Nodes: Dev1, Dev2, CFO, OpsHead
        * This shows how an organization may structure its reporting hierarchy.

        2. File System Tree

        ```
                Root
              /   |   \
            etc   usr   var
                  |
                 bin
        ```
        * Root: Root
        * Subtrees: etc, usr (with child bin), var
        * Useful to represent directories and subdirectories.

        3. Family Tree

        ```
                 Grandparent
                /     |     \
             Uncle  Parent Aunt
                      |
                    Child
        ```

        * Shows multi-level relationships.
        * Degree of `Grandparent` is 3, Height of tree is 3.


        Each of these trees highlights 
        a common use case for general trees, 
        where nodes may have more than two children.

    Common Operations on a General Tree
        A General Tree allows each node 
        to have any number of children. 
        Here's a list of common operations performed on it:

        | Operation            | Time Complexity     | Purpose                                                                     |
        | -------------------- | ------------------- | --------------------------------------------------------------------------- |
        | Traversal            | O(n)                | Visit all nodes (DFS or BFS) to process or display tree contents.           |
        | Search               | O(n)                | Find a node with a specific value.                                          |
        | Insertion            | O(1) to O(n)        | Add a new child to a given parent node. (O(1) if parent reference is known) |
        | Deletion             | O(n)                | Remove a node and all its descendants (subtree deletion).                   |
        | Height Calculation   | O(n)                | Find the height (max depth) of the tree.                                    |
        | Find Parent          | O(n)                | Identify the parent of a given node (if no parent pointer exists).          |
        | Count Nodes / Leaves | O(n)                | Count total or leaf nodes for metrics or validation.                        |

        > Note: `n` is the number of nodes in the tree.
        > Most operations require traversal 
          since general trees don’t enforce structure like binary trees do.

// 16/7/25 (day 22 - revision) //

Recursion and Backtracking are 
    fundamental concepts in problem solving, 
    especially in algorithms and coding interviews. 
    Here’s a concise breakdown:

    🧠 Recursion:
        A function calling itself to solve smaller subproblems.
    Examples:
        * Factorial
        * Fibonacci
        * Sum of array
        * Binary Search (recursive)
        * Tree traversals

    The Tower of Hanoi 
        is a classic recursion problem 
        that teaches how recursive thinking 
        can break down a problem into 
        simpler subproblems.
    
        Recursive Nature
        To move `n` disks:
            * Move `n−1` disks from source to helper.
            * Move the largest disk to destination.
            * Move `n−1` disks from helper to destination.

Backtracking:
    A form of recursion 
    where you explore all possibilities, 
    but undo ("backtrack") 
    when a solution path fails.
    
    ie "Backtracking is recursion + undo. 
    You explore choices, and 
    backtrack when a path doesn’t lead to a solution."

    Steps:
        1. Choose a solution
        2. Explore recursively
        3. Undo the choice (backtrack)

    ✅ Where It Is Used
        * Problems where we must find:
            * All possible solutions (e.g., permutations, combinations).
            * A single valid solution under constraints (e.g., N-Queens).
            * Optimal solution by pruning (e.g., Sudoku).
    Classic Problems:
        * N-Queens
        * Sudoku Solver
        * Word Search
        * Subsets / Combinations / Permutations
        * Rat in a Maze
        * Permutations/Combinations
    


        Classification of Backtracking 
            based on problem types and 
            the technique used to prune the search space. Here’s a breakdown:

            🧩 1. Combinatorial Backtracking
                * Involves generating all combinations, permutations, or subsets.
                * No constraints or only soft rules (like avoiding duplicates).

                ✅ Examples:
                    * Generating subsets of a set
                    * All permutations of a string
                    * Combinations that sum to a target

            🧠 2. Constraint Satisfaction Backtracking
                * Deals with strong rules (constraints) that must be followed.
                * Applies pruning and early termination.

                ✅ Examples:
                    * N-Queens problem (no two queens attack each other)
                    * Sudoku solver
                    * Map coloring
                    * Cryptarithmetic puzzles

            ⚙️ 3. Optimization Backtracking
                * Tries all valid paths, but among them, returns the optimal (e.g., min/max) result.

                ✅ Examples:
                    * 0/1 Knapsack problem (when solved via backtracking)
                    * Longest valid path in a maze
                    * Maximum number of valid configurations

            🔍 Based on Pruning Techniques:
                | Type                   | Description                                                               |
                | ---------------------- | ------------------------------------------------------------------------- |
                | Naive Backtracking     | Tries all paths, no pruning                                               |
                | Forward Checking       | Eliminates invalid choices early based on current partial state           |
                | Constraint Propagation | Applies domain rules (like Sudoku rules) ahead to reduce the search space |
                | Memoized Backtracking  | Combines backtracking with memoization to avoid re-exploring same states  |


    Example: Subsets
        ```java
        import java.util.*;

        public class SubsetsGenerator {

            public static List<List<Integer>> subsets(int[] nums) {
                List<List<Integer>> res = new ArrayList<>();
                backtrack(nums, 0, new ArrayList<>(), res);
                return res;
            }

            private static void backtrack(int[] nums, int index, List<Integer> path, List<List<Integer>> res) {
                res.add(new ArrayList<>(path)); // Add a copy of current path

                for (int i = index; i < nums.length; i++) {
                    path.add(nums[i]);                  // Choose
                    backtrack(nums, i + 1, path, res);  // Explore
                    path.remove(path.size() - 1);       // Unchoose (backtrack)
                }
            }

            public static void main(String[] args) {
                int[] nums = {1, 2, 3};
                List<List<Integer>> result = subsets(nums);

                for (List<Integer> subset : result) {
                    System.out.println(subset);
                }
            }
        }
        ```
        State Space Tree for Subsets Example (nums = [1, 2, 3]):

        Each node shows the current subset (path) and the index in the array.

        Level 0: Start
        []
        |
        |-- Include 1
        |   [1]
        |   |
        |   |-- Include 2
        |   |   [1, 2]
        |   |   |
        |   |   |-- Include 3
        |   |   |   [1, 2, 3]
        |   |   |
        |   |   |-- Exclude 3
        |   |
        |   |-- Exclude 2
        |       [1]
        |       |
        |       |-- Include 3
        |           [1, 3]
        |       |
        |       |-- Exclude 3
        |
        |-- Exclude 1
            []
            |
            |-- Include 2
            |   [2]
            |   |
            |   |-- Include 3
            |   |   [2, 3]
            |   |
            |   |-- Exclude 3
            |
            |-- Exclude 2
                []
                |
                |-- Include 3
                |   [3]
                |
                |-- Exclude 3

        All paths (leaf nodes) represent a subset:
        [[], [1], [1,2], [1,2,3], [1,3], [2], [2,3], [3]]
------------------------------------------------------------------
-  A graph is a non-linear data structure 
   made up of vertices (nodes) and edges (connections). 
   It is used to represent networks 
   like social connections, maps, the internet, 
   dependency structures, and more.
    🔹 Types of Graphs:
        * Directed vs Undirected: Edges have direction or not.
        * Weighted vs Unweighted: Edges may carry weights (e.g., distances, costs).
        * Cyclic vs Acyclic: May or may not contain cycles.
        * Connected vs Disconnected: Whether all nodes are reachable from each other.
        * Simple vs Multigraph: No multiple edges between the same pair vs allowing them.

    🔹 Representation:
        * Adjacency Matrix: 2D array showing connections.
        * Adjacency List: List of nodes with their neighbors (space-efficient).

    🔹 Applications:
        * Shortest path finding (Dijkstra, Bellman-Ford)
        * Topological sort (DAG)
        * Network routing
        * Cycle detection
        * Social network analysis
        * Game AI and puzzles


----------------------------------

"Find Path in Undirected Graph", 
"Given an undirected graph and two nodes, determine if there is a path between them using DFS or BFS.", 
"Easy", 
"LeetCode, HackerRank, Google"

----------------------------------

Problem Title:
Find Path in Undirected Graph

Problem Description:
Given an undirected graph represented as a list of edges and two distinct nodes, determine if there exists a path between the two nodes. You may use either Depth-First Search (DFS) or Breadth-First Search (BFS) to solve this problem.

Input Format:
- The first line contains two integers n and m, where n is the number of nodes (numbered from 0 to n-1), and m is the number of edges.
- The next m lines each contain two integers u and v, representing an undirected edge between nodes u and v.
- The last line contains two integers start and end, representing the nodes between which you need to determine if a path exists.

Output Format:
- Print "Yes" if there is a path between start and end, otherwise print "No".

Constraints:
- 2 ≤ n ≤ 10^5
- 1 ≤ m ≤ 2×10^5
- 0 ≤ u, v, start, end < n
- u ≠ v
- The graph may contain self-loops and multiple edges.

Sample Input:
5 4
0 1
1 2
2 3
3 4
0 4

Sample Output:
Yes

Explanation:
There is a path from node 0 to node 4: 0-1-2-3-4.

Difficulty:
Easy

Five Test Cases:
Test Case 1:
Input:
4 2
0 1
2 3
0 3
Output:
No

Test Case 2:
Input:
3 3
0 1
1 2
2 0
0 2
Output:
Yes

Test Case 3:
Input:
6 5
0 1
1 2
2 3
3 4
4 5
0 5
Output:
Yes

Test Case 4:
Input:
5 2
0 1
3 4
0 4
Output:
No

Test Case 5:
Input:
2 1
0 1
0 1
Output:
Yes

----------------------------------

Java Solution: DFS using recursion

import java.util.*;

public class FindPathInUndirectedGraph {
    public static void main(String[] args) {
        Scanner sc = new Scanner(System.in);
        int n = sc.nextInt();
        int m = sc.nextInt();
        List<List<Integer>> adj = new ArrayList<>();
        for (int i = 0; i < n; i++) adj.add(new ArrayList<>());
        for (int i = 0; i < m; i++) {
            int u = sc.nextInt();
            int v = sc.nextInt();
            adj.get(u).add(v);
            adj.get(v).add(u);
        }
        int start = sc.nextInt();
        int end = sc.nextInt();
        boolean[] visited = new boolean[n];
        System.out.println(dfs(adj, visited, start, end) ? "Yes" : "No");
    }

    private static boolean dfs(List<List<Integer>> adj, boolean[] visited, int curr, int target) {
        if (curr == target) return true;
        visited[curr] = true;
        for (int neighbor : adj.get(curr)) {
            if (!visited[neighbor]) {
                if (dfs(adj, visited, neighbor, target)) return true;
            }
        }
        return false;
    }
}

---

Java Solution : BFS using queue

import java.util.*;

public class FindPathInUndirectedGraphBFS {
    public static void main(String[] args) {
        Scanner sc = new Scanner(System.in);
        int n = sc.nextInt();
        int m = sc.nextInt();
        List<List<Integer>> adj = new ArrayList<>();
        for (int i = 0; i < n; i++) adj.add(new ArrayList<>());
        for (int i = 0; i < m; i++) {
            int u = sc.nextInt();
            int v = sc.nextInt();
            adj.get(u).add(v);
            adj.get(v).add(u);
        }
        int start = sc.nextInt();
        int end = sc.nextInt();
        System.out.println(bfs(adj, n, start, end) ? "Yes" : "No");
    }

    private static boolean bfs(List<List<Integer>> adj, int n, int start, int end) {
        boolean[] visited = new boolean[n];
        Queue<Integer> queue = new LinkedList<>();
        queue.offer(start);
        visited[start] = true;
        while (!queue.isEmpty()) {
            int node = queue.poll(); 
            if (node == end) return true;
            for (int neighbor : adj.get(node)) {
                if (!visited[neighbor]) {
                    visited[neighbor] = true; //in the queue after visiting means it is visited
                    queue.offer(neighbor);
                }
            }
        }
        return false;
    }
}

---

Java Solution: DFS using stack (no recursion)

import java.util.*;

public class FindPathInUndirectedGraphDFSIterative {
    public static void main(String[] args) {
        Scanner sc = new Scanner(System.in);
        int n = sc.nextInt();
        int m = sc.nextInt();
        List<List<Integer>> adj = new ArrayList<>();
        for (int i = 0; i < n; i++) adj.add(new ArrayList<>());
        for (int i = 0; i < m; i++) {
            int u = sc.nextInt();
            int v = sc.nextInt();
            adj.get(u).add(v);
            adj.get(v).add(u);
        }
        int start = sc.nextInt();
        int end = sc.nextInt();
        System.out.println(dfsIterative(adj, n, start, end) ? "Yes" : "No");
    }

    private static boolean dfsIterative(List<List<Integer>> adj, int n, int start, int end) {
        boolean[] visited = new boolean[n];
        Stack<Integer> stack = new Stack<>();
        stack.push(start);
        while (!stack.isEmpty()) {
            int node = stack.pop(); 
            if (node == end) return true;
            if (!visited[node]) {
                visited[node] = true;
                for (int neighbor : adj.get(node)) {
                    if (!visited[neighbor]) {
                        stack.push(neighbor);  //when we delete the node means it is visited
                    }
                }
            }
        }
        return false;
    }
}
----------------------------------------
Heap Data Structure Notes
=========================
    1. Definition:
        - A heap is a specialized tree-based data structure that satisfies the heap property.
        - Types: Max Heap (parent >= children), Min Heap (parent <= children).

    2. Properties:
        - Complete binary tree: All levels are filled except possibly the last, which is filled from left to right.
        - Heap property: For max heap, every parent node is greater than or equal to its children; for min heap, every parent is less than or equal to its children.

    3. Representation:
        - Usually implemented as an array.
        - For node at index i:
        - Left child: 2*i + 1
        - Right child: 2*i + 2
        - Parent: (i - 1) // 2

    4. Operations:
        - Insertion: Add at the end, then "heapify up" to restore heap property.
        - Deletion (usually root): Replace root with last element, remove last, then "heapify down".
        - Peek: Return root element (max or min).
        - Heapify: Convert an array into a heap.

    5. Time Complexities:
        - Insertion: O(log n)
        - Deletion: O(log n)
        - Peek: O(1)
        - Heapify: O(n)

    6. Applications:
        - Priority queues
        - Heap sort
        - Graph algorithms (Dijkstra's, Prim's)
        - Scheduling

    7. Example (Min Heap):
        Array: [1, 3, 5, 7, 9, 8]
                 1
                / \
               3   5
              / \  /
             7  9 8

    8. Heap Sort:
        - Build a heap from the array.
        - Repeatedly remove the root and rebuild the heap.

    References:
    - CLRS: Introduction to Algorithms
    - https://en.wikipedia.org/wiki/Heap_(data_structure)


Priority Queue Notes
====================
    1. Definition:
        - A priority queue is an abstract data type where each element has a priority.
        - Elements are served based on priority (highest or lowest), not just insertion order.

    2. Implementation:
        - Commonly implemented using heaps (binary heap, Fibonacci heap).
        - Can also be implemented with arrays or linked lists (less efficient).

    3. Operations:
        - Insert: Add an element with a given priority.
        - Extract (Pop): Remove and return the element with the highest (or lowest) priority.
        - Peek: Return the element with the highest (or lowest) priority without removing it.
        - Change Priority: Update the priority of an element (optional, depends on implementation).

    4. Time Complexities (using binary heap):
        - Insert: O(log n)
        - Extract: O(log n)
        - Peek: O(1)

    5. Applications:
        - Task scheduling
        - Dijkstra's shortest path algorithm
        - Huffman coding
        - Event-driven simulation

    6. Example:
        - Elements: [(A, 2), (B, 1), (C, 3)]
        - After inserting: [(C, 3), (A, 2), (B, 1)] (if max-priority queue)

----------------------------------

"Kth Largest Element in an Array", 
"Find the kth largest element in an unsorted array using a heap.", 
"Medium", 
"LeetCode, Facebook, Amazon"

----------------------------------

Problem Title:
Kth Largest Element in an Array

Problem Description:
Given an unsorted array of integers and an integer k, find the kth largest element in the array. The kth largest element is the element that would be in the kth position if the array was sorted in descending order. Note that it is the kth largest element in sorted order, not the kth distinct element.

Input Format:
- The first line contains two integers n and k, where n is the number of elements in the array.
- The second line contains n space-separated integers representing the elements of the array.

Output Format:
- Output a single integer, the kth largest element in the array.

Constraints:
- 1 ≤ n ≤ 10^5
- 1 ≤ k ≤ n
- -10^9 ≤ array[i] ≤ 10^9

Sample Input:
6 2
3 2 1 5 6 4

Sample Output:
5

Explanation:
The sorted array in descending order is [6, 5, 4, 3, 2, 1]. The 2nd largest element is 5.

Difficulty:
Medium

Test Cases:
Test Case 1:
Input:
5 1
7 10 4 3 20
Output:
20

Test Case 2:
Input:
5 3
7 10 4 3 20
Output:
7

Test Case 3:
Input:
8 4
12 35 1 10 34 1 7 8
Output:
10

Test Case 4:
Input:
3 2
-1 -2 -3
Output:
-2

Test Case 5:
Input:
10 5
5 3 8 6 2 9 1 7 4 10
Output:
6

----------------------------------

import java.util.PriorityQueue;
import java.util.Scanner;

public class KthLargestElement {
    public static void main(String[] args) {
        Scanner sc = new Scanner(System.in);
        int n = sc.nextInt();
        int k = sc.nextInt();
        int[] nums = new int[n];
        for (int i = 0; i < n; i++) {
            nums[i] = sc.nextInt();
        }
        System.out.println(findKthLargest(nums, k));
    }

    public static int findKthLargest(int[] nums, int k) {
        PriorityQueue<Integer> minHeap = new PriorityQueue<>(k);
        for (int num : nums) {
            if (minHeap.size() < k) {
                minHeap.offer(num);
            } else if (num > minHeap.peek()) {
                minHeap.poll();
                minHeap.offer(num);
            }
        }
        return minHeap.peek();
    } 
}

----------------------------------
Hashing, Hashtable, and HashSet - Notes

1. Hashing
-----------
- Hashing is a technique to convert a range of key values into a range of indexes of an array.
- A hash function takes an input (key) and returns an integer (hash code).
- The hash code is used as an index in a hash table.
- Good hash functions distribute keys uniformly and minimize collisions.

2. Hashtable
-------------
- A Hashtable is a data structure that implements an associative array, mapping keys to values.
- Uses hashing to compute an index into an array of buckets or slots.
- On collision (when two keys hash to the same index), uses collision resolution techniques:
    - Chaining (linked lists at each bucket)
    - Open addressing (probing for next available slot)
- Average time complexity for search, insert, delete: O(1)
- Examples: Java's `Hashtable`, Python's `dict`, C++'s `unordered_map`

3. HashSet
-----------
- A HashSet is a collection that contains no duplicate elements.
- Internally uses a hash table to store elements.
- Only stores keys (no associated values).
- Operations: add, remove, contains (all O(1) average time)
- Examples: Java's `HashSet`, Python's `set`, C++'s `unordered_set`

Summary Table:
---------------
| Feature      | Hashtable         | HashSet           |
|--------------|------------------|-------------------|
| Stores       | Key-Value pairs  | Unique values     |
| Duplicates   | Keys unique      | No duplicates     |
| Lookup time  | O(1) avg         | O(1) avg          |
| Use case     | Map/dictionary   | Set operations    |


--------------------------------------

"Find All Duplicates in an Array", 
"Given an array of integers, find all elements that appear twice.", 
"Medium", 
"LeetCode, Amazon"

----------------------------------

Problem Title:
Find All Duplicates in an Array

Problem Description:
Given an array of integers where 1 ≤ a[i] ≤ n (n = size of array), some elements appear twice and others appear once. Find all the elements that appear twice in this array. Return the answer in any order. You must solve the problem without using extra space and in O(n) runtime.

Input Format:
- The first line contains an integer n, the size of the array.
- The second line contains n space-separated integers representing the elements of the array.

Output Format:
- Print the list of integers that appear twice in the array, separated by spaces. If no duplicates are found, print an empty line.

Constraints:
- 1 ≤ n ≤ 10^5
- 1 ≤ a[i] ≤ n

Sample Input:
8
4 3 2 7 8 2 3 1

Sample Output:
2 3

Explanation:
Both 2 and 3 appear twice in the array.

Difficulty:
Medium

Test Cases:
Test Case 1:
Input:
6
1 2 3 4 5 6
Output:

Test Case 2:
Input:
5
1 1 2 2 3
Output:
1 2

Test Case 3:
Input:
7
7 3 5 3 7 1 2
Output:
7 3

Test Case 4:
Input:
4
4 4 4 4
Output:
4

Test Case 5:
Input:
10
10 9 8 7 6 5 4 3 2 1
Output:(empty line)

--------------------------------------

import java.util.*;

public class FindDuplicates {
    public static void main(String[] args) {
        Scanner sc = new Scanner(System.in);
        int n = sc.nextInt();
        int[] nums = new int[n];
        for(int i = 0; i < n; i++) {
            nums[i] = sc.nextInt();
        }
        List<Integer> res = findDuplicates(nums);
        for(int i = 0; i < res.size(); i++) {
            System.out.print(res.get(i));
            if(i != res.size() - 1) System.out.print(" ");
        }
        System.out.println();
    }

    public static List<Integer> findDuplicates(int[] nums) {
        List<Integer> result = new ArrayList<>();
        for(int i = 0; i < nums.length; i++) {
            int idx = Math.abs(nums[i]) - 1;
            if(nums[idx] < 0) {
                result.add(Math.abs(nums[i]));
            } else {
                nums[idx] = -nums[idx];
            }
        }
        return result;
    }
}

--------------------------------------
Dynamic Programming (DP) Notes
==============================

Definition:
-----------
Dynamic Programming is a method for solving complex problems by breaking them down into simpler subproblems. It is applicable when the problem has overlapping subproblems and optimal substructure.

Key Concepts:
-------------
1. **Optimal Substructure**: The optimal solution to the problem can be constructed from optimal solutions of its subproblems.
2. **Overlapping Subproblems**: The problem can be broken down into subproblems which are reused several times.

Approaches:
-----------
- **Top-Down (Memorization)**: Solve the problem recursively and store the results of subproblems to avoid redundant computations.
- **Bottom-Up (Tabulation)**: Solve all possible subproblems starting from the smallest, and use their solutions to build up solutions to larger problems.

Steps to Solve DP Problems:
---------------------------
1. Define the subproblem.
2. Write the recurrence relation.
3. Identify base cases.
4. Choose memoization or tabulation.
5. Implement and optimize.

Examples:
---------
- Fibonacci Sequence
- Knapsack Problem
- Longest Common Subsequence
- Coin Change Problem
- Edit Distance

Common Pitfalls:
----------------
- Not identifying overlapping subproblems.
- Incorrect base cases.
- Inefficient state representation.

Tips:
-----
- Draw recursion trees to find overlapping subproblems.
- Use arrays or hash maps for memoization.
- Optimize space if possible (e.g., reduce 2D DP to 1D).

References:
-----------
- "Introduction to Algorithms" by Cormen et al.
- LeetCode DP problems
- GeeksforGeeks DP tutorials


Greedy Technique Notes
======================

Definition:
-----------
Greedy algorithms build up a solution piece by piece, 
always choosing the next piece that offers the most immediate benefit. 
They do not reconsider their choices, 
aiming for a locally optimal solution at each step.

Key Concepts:
-------------
1. **Greedy Choice Property**: A global optimum can be arrived at by selecting a local optimum.
2. **Optimal Substructure**: An optimal solution to the problem contains optimal solutions to subproblems.

Approach:
---------
- At each step, make the choice that seems best at the moment.
- Do not revisit previous choices.

Steps to Solve Greedy Problems:
-------------------------------
1. Identify the greedy choice property.
2. Prove that making greedy choices leads to an optimal solution.
3. Design an algorithm based on the greedy strategy.
4. Implement and test.

Examples:
---------
- Activity Selection Problem
- Fractional Knapsack Problem
- Huffman Coding
- Prim’s and Kruskal’s Algorithms for Minimum Spanning Tree
- Dijkstra’s Algorithm for Shortest Path

Common Pitfalls:
----------------
- Assuming greedy always works (it does not for all problems).
- Not proving the greedy choice property.
- Overlooking counterexamples.

Tips:
-----
- Try to prove correctness with an exchange argument or induction.
- Compare with dynamic programming to check if greedy is sufficient.
- Test with edge cases and small examples.

References:
-----------
- "Introduction to Algorithms" by Cormen et al.
- LeetCode Greedy problems
- GeeksforGeeks Greedy tutorials

----------------------------------

"Longest Increasing Subsequence", 
"Given an array of integers, 
find the length of the longest strictly increasing subsequence.", 
"Medium", 
"LeetCode, Amazon, Microsoft"

----------------------------------

Problem Title: Longest Increasing Subsequence

Problem Description:
Given an array of integers, find the length of the longest strictly increasing subsequence. A subsequence is a sequence that can be derived from the array by deleting some or no elements without changing the order of the remaining elements. The subsequence must be strictly increasing, meaning each element is greater than the previous one.

Input Format:
- The first line contains an integer n, the number of elements in the array.
- The second line contains n space-separated integers representing the elements of the array.

Output Format:
- Print a single integer, the length of the longest strictly increasing subsequence.

Constraints:
- 1 ≤ n ≤ 2500
- -10^4 ≤ array[i] ≤ 10^4

Sample Input:
6
10 9 2 5 3 7

Sample Output:
3

Sample Explanation:
The longest strictly increasing subsequence is [2, 5, 7], so the answer is 3.

Difficulty:
Medium

Five Test Cases:
Test Case 1:
Input:
8
1 3 2 4 3 5 4 6
Output:
5

Test Case 2:
Input:
5
5 4 3 2 1
Output:
1

Test Case 3:
Input:
7
1 2 3 4 5 6 7
Output:
7

Test Case 4:
Input:
10
10 22 9 33 21 50 41 60 80 1
Output:
6

Test Case 5:
Input:
1
100
Output:
1

----------------------------------

import java.util.Scanner;

public class LongestIncreasingSubsequence {
    public static void main(String[] args) {
        Scanner sc = new Scanner(System.in);
        int n = sc.nextInt();
        int[] nums = new int[n];
        for(int i = 0; i < n; i++) {
            nums[i] = sc.nextInt();
        }
        System.out.println(lengthOfLIS(nums));
    }

    public static int lengthOfLIS(int[] nums) {
        int n = nums.length;
        if (n == 0) return 0;
        int[] dp = new int[n];
        int maxLen = 1;
        for(int i = 0; i < n; i++) {
            dp[i] = 1;
            for(int j = 0; j < i; j++) {
                if(nums[i] > nums[j]) {
                    dp[i] = Math.max(dp[i], dp[j] + 1);
                }
            }
            maxLen = Math.max(maxLen, dp[i]);
        }
        return maxLen;
    }
}

----------------------------------
input `[10, 9, 2, 5, 3, 7]` 

| i | nums[i] | dp before     | j loop comparisons (nums[j] < nums[i])                              | dp after      | maxLen |
|---|---------|---------------|---------------------------------------------------------------------|---------------|--------|
| 0 |   10    | [0,0,0,0,0,0] | none                                                                | [1,0,0,0,0,0] |   1    |
| 1 |    9    | [1,0,0,0,0,0] | 10 < 9? No                                                          | [1,1,0,0,0,0] |   1    |
| 2 |    2    | [1,1,0,0,0,0] | 10 < 2? No; 9 < 2? No                                               | [1,1,1,0,0,0] |   1    |
| 3 |    5    | [1,1,1,0,0,0] | 10 < 5? No; 9 < 5? No; 2 < 5? Yes → dp[3]=dp[2]+1=2                 | [1,1,1,2,0,0] |   2    |
| 4 |    3    | [1,1,1,2,0,0] | 10 < 3? No; 9 < 3? No; 2 < 3? Yes → dp[4]=dp[2]+1=2; 5 < 3? No      | [1,1,1,2,2,0] |   2    |
| 5 |    7    | [1,1,1,2,2,0] | 10 < 7? No; 9 < 7? No; 2 < 7? Yes → dp[5]=dp[2]+1=2; 5 < 7? Yes → dp[5]=dp[3]+1=3; 3 < 7? Yes → dp[5]=dp[4]+1=3 
                                                                                                    | [1,1,1,2,2,3] |   3    |
----------------------------------

"Activity Selection", 
"Given start and end times of activities, select the maximum number of non-overlapping activities.", 
"Easy", 
"GeeksforGeeks, Google"

----------------------------------

Problem Title: Activity Selection Problem

Problem Description:
Given N activities with their start and end times, select the maximum number of activities that can be performed by a single person, assuming that a person can only work on a single activity at a time. Two activities are said to be non-overlapping if the start time of one activity is not less than the end time of the other.

Input Format:
- The first line contains a single integer N, the number of activities.
- The next N lines each contain two integers, S and E, representing the start and end times of each activity.

Output Format:
- Print a single integer, the maximum number of non-overlapping activities that can be performed.

Constraints:
- 1 ≤ N ≤ 10^5
- 0 ≤ S < E ≤ 10^9

Sample Input:
6
1 3
2 4
3 5
0 6
5 7
8 9

Sample Output:
4

Sample Explanation:
The selected activities can be (1,3), (3,5), (5,7), and (8,9). These activities do not overlap and their count is 4.

Difficulty:
Easy

Test Cases:
Test Case 1:
Input:
3
1 2
2 3
3 4
Output:
3

Test Case 2:
Input:
4
1 4
2 3
3 5
4 6
Output:
2

Test Case 3:
Input:
5
1 10
2 3
3 4
4 5
5 6
Output:
4

Test Case 4:
Input:
2
1 5
2 6
Output:
1

Test Case 5:
Input:
7
1 2
2 3
3 4
4 5
5 6
6 7
7 8
Output:
7

----------------------------------

import java.util.*;

public class ActivitySelection {
    public static void main(String[] args) {
        Scanner sc = new Scanner(System.in);
        int n = sc.nextInt();
        int[][] activities = new int[n][2];
        for (int i = 0; i < n; i++) {
            activities[i][0] = sc.nextInt();
            activities[i][1] = sc.nextInt();
        }
        Arrays.sort(activities, Comparator.comparingInt(a -> a[1]));
        int count = 0, lastEnd = -1;
        for (int[] act : activities) {
            if (act[0] >= lastEnd) {
                count++;
                lastEnd = act[1];
            }
        }
        System.out.println(count);
    }
}

----------------------------------
- Sorts the `activities` array in ascending order based on the end time (`a[1]`) of each activity.
- This is crucial for the greedy algorithm to select the maximum number of non-overlapping activities.

- Initializes `count` to 0 (number of selected activities) and 
`lastEnd` to -1 (end time of the last selected activity).
- Iterates through each activity:
    - If the activity’s start time (`act[0]`) is greater than or equal to `lastEnd`, 
    it means the activity does not overlap with the previously selected one.
    - Increments `count` and updates `lastEnd` to the current activity’s end time.

Test Case 2:
Input:
4
1 4
2 3
3 5
4 6
Output:
2

Test Case 3:
Input:
5
1 10
2 3
3 4
4 5
5 6
Output:
4

Dijkstra's Algorithm - Notes
----------------------------
    Purpose:
    - Finds the shortest path from a source node to all other nodes in a weighted graph with non-negative edge weights.

    Key Concepts:
    - Greedy algorithm.
    - Uses a priority queue (min-heap) to select the next node with the smallest tentative distance.

    Algorithm Steps:
    1. Initialize distances from the source to all nodes as infinity, except the source itself (distance 0).
    2. Mark all nodes as unvisited. Set the source node as current.
    3. For the current node, consider all its unvisited neighbors and calculate their tentative distances.
    4. If the calculated distance of a node is less than the known distance, update it.
    5. Once all neighbors are considered, mark the current node as visited.
    6. Select the unvisited node with the smallest tentative distance as the new current node.
    7. Repeat steps 3-6 until all nodes are visited or the smallest tentative distance among the unvisited nodes is infinity.

    Data Structures:
    - Distance array (or map)
    - Priority queue (min-heap)
    - Visited set

    Time Complexity:
    - O((V + E) log V) with a min-heap (V = vertices, E = edges)

    Limitations:
    - Does not work with negative edge weights.

    Applications:
    - GPS navigation
    - Network routing protocols
    - Shortest path in maps and games

    Pseudocode:
    ----------------
    function Dijkstra(Graph, source):
        dist[source] = 0
        for each vertex v in Graph:
            if v ≠ source:
                dist[v] = infinity
            add v to priority queue Q

        while Q is not empty:
            u = node in Q with smallest dist[u]
            remove u from Q

            for each neighbor v of u:
                alt = dist[u] + length(u, v)
                if alt < dist[v]:
                    dist[v] = alt

        return dist

Bellman-Ford Algorithm - Notes
------------------------------
    Purpose:
    - Computes shortest paths from a single source node to all other nodes in a weighted graph (can handle negative edge weights).

    Key Concepts:
    - Dynamic programming approach.
    - Relaxes all edges up to (V-1) times (V = number of vertices).
    - Can detect negative weight cycles.

    Algorithm Steps:
    1. Initialize distances from the source to all nodes as infinity, except the source itself (distance 0).
    2. Repeat (V-1) times:
        a. For each edge (u, v) with weight w:
            - If dist[u] + w < dist[v], update dist[v] = dist[u] + w.
    3. Check for negative weight cycles:
        a. For each edge (u, v) with weight w:
            - If dist[u] + w < dist[v], a negative cycle exists.

    Data Structures:
    - Distance array (or map)

    Time Complexity:
    - O(V * E) (V = vertices, E = edges)

    Limitations:
    - Slower than Dijkstra's algorithm for graphs without negative weights.

    Applications:
    - Detecting negative weight cycles
    - Currency arbitrage detection
    - Routing algorithms (e.g., RIP)

    Pseudocode:
    ----------------
    function BellmanFord(Graph, source):
        dist[source] = 0
        for each vertex v in Graph:
            if v ≠ source: 
                dist[v] = infinity

        for i from 1 to |V|-1:
            for each edge (u, v) with weight w in Graph:
                if dist[u] + w < dist[v]:
                    dist[v] = dist[u] + w

        for each edge (u, v) with weight w in Graph:
            if dist[u] + w < dist[v]:
                report "Negative weight cycle detected"

        return dist

Topological Sorting (Kahn's Algorithm) - Notes
----------------------------------------------
        Purpose:
        - Produces a linear ordering of vertices in a Directed Acyclic Graph (DAG) such that for every directed edge u → v, vertex u comes before v in the ordering.

        Key Concepts:
        - Only works for DAGs (Directed Acyclic Graphs).
        - Uses in-degree (number of incoming edges) for each node.
        - Nodes with in-degree 0 can be placed in the ordering.

        Algorithm Steps:
        1. Compute in-degree for each vertex.
        2. Initialize a queue and enqueue all vertices with in-degree 0.
        3. While the queue is not empty:
            a. Remove a vertex u from the queue and add it to the topological order.
            b. For each neighbor v of u:
                - Decrement in-degree of v by 1.
                - If in-degree of v becomes 0, enqueue v.
        4. If all vertices are processed, a valid topological order exists.
        5. If not all vertices are processed, the graph has a cycle (not a DAG).

        Data Structures:
        - In-degree array (or map)
        - Queue for vertices with in-degree 0
        - List for topological order

        Time Complexity:
        - O(V + E) (V = vertices, E = edges)

        Limitations:
        - Only applicable to DAGs.

        Applications:
        - Task scheduling
        - Course prerequisite ordering
        - Build systems (dependency resolution)

        Pseudocode:
        ----------------
        function KahnTopologicalSort(Graph):
            in_degree = [0 for each vertex]
            for each edge (u, v) in Graph:
                in_degree[v] += 1

            queue = all vertices with in_degree 0
            topo_order = []

            while queue is not empty:
                u = queue.pop()
                topo_order.append(u)
                for each neighbor v of u:
                    in_degree[v] -= 1
                    if in_degree[v] == 0:
                        queue.append(v)

            if length of topo_order == number of vertices:
                return topo_order
            else:
                report "Graph has a cycle"

//17/7/25 Insights on Product based companies interviews //

What are product based companies?
...Companies that build and sell their own software products (ex: Google,Microsoft,Adobe,Amazon,Zoho)
Big tech : Google,Amazon , Microsoft, Meta 
Indian product companies : Zoho, FreshWorks,Razorpay,InMobi
Startups:CRED, Postman,Swiggy(Partially product-oriented)

Feature        Product-based                       service-based
Revenue       selling products                     client projects
Focus        Innovation ,Product quality          client satisfaction
hiring        selective , deep skills              volume hiring
work culture   ownership,r&d                        strict deadline